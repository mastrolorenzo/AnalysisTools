saving logfile to [34m results//FINAL_VHLegacy_2lep_WP_SR_low_Zll/Zll2017_SR_low_Zll_191022_V11finalVarsWP.h5/512-256-128-64-64-64/0.20-0.50-0.50-0.50-0.50-0.50/1.000e+00/rnd_1/output.txt [0m
INFO: numpy random state =  MT19937 ,ae328a00,381fa202,2c723236,53dc75b1,8fb11bb2,72695c10,df60edc7,4c787eef,f7629af2,3b7caacd,14f1f72c,552af402,450bb8b7,7a598589,18a0c038,688100ee,b611d3d4,bdf6bc19,569170cc,22e93cc4,40c4c482,66a47da4,93eadc8a,49054e3a,f8eb5c04,882ce666,6dba14b8,bc286d55,96fb67d7,8b97b317,2d39309f,792154b4,fd687137,46216e5d,7bb3467d,b2587d23,eaf286a9,654ea97a,567a4d37,ca90b308,d73e32a6,5dc1b31e,9fd8f5c1,954c520c,f5264876,5f37e585,9e925e5e,dd51ff66,ddd6c5eb,34916f37,e9f089f3,b8e211b6,7e780d81,76643bee,aed8fc26,ba5596c8,be0190d5,ce347839,ea7aab8,a19a93c4,3e0ebb0e,979e61b9,4fa69f07,9f8f0d6e,6235c4be,90f8e93f,3d8e7f59,f9d42785,1dabb4db,34fd90eb,57922b01,8079a6dc,e8be44a9,c46e10a6,be083c4d,e88e4618,e6c23d62,b24db69f,92532db4,7418f6dd,bd2a4777,f4a5ee09,7eed2c48,42f9e21f,c633a416,766a24b,b0fd3715,f714587e,143e8524,60513ee2,448ca11e,35fdcdf4,e41a4417,36351c16,fd467e66,bc6887cd,5b7de085,2ca86a8e,39766b66,5c22dd94,e0e8549b,87793211,7a8eebc,93259be8,2c85e91b,4f616154,7f1f6105,2b04d3b2,be320c4e,55e58cb8,a2719f9d,1c43feec,ce736a78,23effb14,cfb7e855,e32c4d3d,a4dc2dfa,b4a06fb8,51a70033,99f37b2f,7010c85a,d1ebade5,e9e3ac67,b36b607e,4ff5c520,17ae67cb,865c688c,23c7bda4,69e6d930,f71e53bb,855da0a7,45a67fd0,5e3cb990,8b57e334,7769d6a2,58193ab1,626afe07,619b93c9,53e03f1b,d60bb76b,81e0e3c0,1226483c,e86f10c6,a0d2433b,46d8f984,38438b10,b1cb0540,21b06adb,bd4c9e50,b0f5d0d1,4f597f02,f0ad4c73,8780a326,5a31a1cb,fe2383e,a62dd33,69cc100f,cdc6c4d5,922a395d,4a25fe57,422811c3,9fb9e357,fbff99c0,c455ff2f,410d36bb,200ac3ce,c7f96a73,641f7ff,408b9e1d,1ce1fd1,1a63e823,cbcbbf33,adc2895b,bda01940,a31418e5,ed434f6,1b8a18b6,69c1ed12,92cc0dbc,b5a60fd8,f4367eb6,e7d3f2eb,96ebd5c2,cb2dbab3,80e2ca05,4f5572b5,373c08de,10377a12,48cd4e31,79582e0c,719eaab8,4f008f7b,6e3f31e6,d41a1543,a2665207,fbf00887,155d4b09,6aa26cf2,3b66d52d,8ae106a4,6e905623,713c12f4,cb2904fb,6ee29ec3,cd19f22d,e5a3469f,9090e7c,5e2624c,37de3a7f,9a6b10a,54f7a66,8529f4c3,71870675,f42190ed,45931b4a,37c89f93,b033c7dd,6a390386,658ed8ab,cde9cb7b,9d0677cf,a6827d45,86217028,896b264b,1c406bf7,e7599356,5ad2027e,a41dbf47,6839e992,afdfc8f4,3cd2797,216d8ada,f5da844e,ef66dc20,222f66b5,dd1d1977,590b9308,8076e227,a5fe38f2,43d9301d,ba87495d,f1a21f8b,8d7901ad,bec62468,101c646f,895bbf0e,6671d069,c788ae98,b79c32f1,a5ccb09,37c33aec,8f22c036,ebd8d2b1,a6053810,9189e41f,93e980a3,846d4185,a8e9a4a7,b185fb44,d82c4f0,2724e06a,15014454,3152caf,4df2ad27,37854c5,536b06,e85a3b72,2af43923,b6e63ac3,42362e69,b63c62c3,fa3325b7,78c481c6,9c77444d,8f16743d,1b856dd,5c678cad,e44ce594,acfeb0cf,ebd074c5,d1d9c3cd,ca542cd,bdaff2ad,7086adb4,eafd512c,77fed639,f93286c2,3ecea061,e26c6246,9e7a7068,b291e906,4d3ca58f,81cfb824,25bf2cf,f2e3fa3e,2629d9d2,af9c82fe,1f32ce3a,66f959d0,8ad32453,d6f07dd0,1e885d33,c2ad472d,ff33f780,46ef6984,13d47745,bf0bc373,7652fb10,3156998d,8f757c9b,efe3a6af,9502864d,3e130aed,5455b742,f9c37a29,ffed0d26,4bc9b378,e5852725,142b8d98,e237417b,238e439,6b7a98ff,c35306f4,95842571,38e57c9f,7bfcea8e,5fe6f2dd,b210bc17,546ada38,a9046aa6,664fbcdf,d8392eb,8574347b,8aeb6e7b,7a1e6ab5,91529bcb,f5647c68,c2225030,c5f6667a,3f45ebd0,a6055395,6838b027,160967b8,f9882249,309cbe25,f6ba4f0d,21b61a7b,f8a1ef34,884d93ee,b5ac2091,ca6256b4,a67a60e2,46846acf,fb650b26,5adc5421,29c731d8,4afe6680,779e78d3,73a43020,7571c422,fa926f3b,2d3165a3,3bae7c6b,3b03dd95,c925296,22c74f27,f8ef1edf,327c379,22180dc8,9c2478c0,5ce0125c,787f9411,36f3723,2d8d486,c3e010d7,5a6ad412,55c84aa8,1ba3c624,89f55b03,8b8d7fbf,2961edbc,23006811,cb83c95f,ce992cda,29684333,381a0c73,f6d76bfd,1334ced,acbfaedc,643e5ab9,33a82d51,e5dd5e8d,ed77edf9,d136851b,31a502b6,6800c6f4,ff7ae9f7,c46f6779,46232e2d,cbdb4afb,b26c30b4,c3bc1c1a,5e86abd8,b11f242a,9f4c316b,4d26de74,cc1cb49d,377604b5,551ea4ef,633314a5,6ce2ec19,897b753f,62a8dd6b,a9768fdd,c4f4ec7a,b2c3f2e7,61028a80,13055024,9bd51cb2,f0a498e7,1fd547c1,273f2482,48481cb9,9f965299,6b0ab078,ea19bb0a,448ea11e,a8561b3,e0a4cf2e,6c04a980,6d39f56,730b2358,663a2a10,5c604831,5c679ddf,faa0c748,af367e80,283f510e,34ede74b,4b6ba58b,7f77931f,b9783839,44489a77,880adff4,cb4290a4,4ce1a00c,47f6f364,55ce645d,24c00027,1d248798,983ce14d,9369b53a,407e7855,5a79d4b2,c1993cac,18085c64,42f7b3e4,e7310e77,5bab2194,df293eb0,42e31a64,2f0a832c,228ce987,49595adb,8afc922a,9702398a,361d6a9,e41d7769,c631ef2e,2d8795fa,7312d1d4,17eca146,dfd9154e,69ae16ad,78fdba8f,80509b3d,969a954f,a32b8345,6cef3cef,889e4063,58ff2557,a1da536c,9ca56d19,1246cd7b,58c9cbc7,65afe082,b55092e1,ebec2b29,6da83860,1647ce50,e5742a08,be8a0bbe,726c19c3,c92b43cf,f087a6c5,15eb9cea,ad2c0ce7,5ae88872,df802543,40308c59,1c47eaa5,a4b72ac4,f5982f3b,580972a4,4d2a172,425742fc,6b1d585b,4d9abe1,f373c44f,2f5f80aa,97fdc9cb,fb812991,6338850d,c50c9e3d,89cb183c,375aedcd,e76baad2,ea775e34,24ecc359,43928998,12a51a16,7baefae,d174589c,49b9513,fc7a1be4,12ed44c6,d550d7b1,4e47445d,847ecde6,5504dc12,b05aa4b8,eed6be18,43297b68,7abc931b,f133ca1,eae15598,ffd20b2,90a95cee,99b02f10,88ddde6b,c762f6f7,71907d1f,94baf994,7aad5b25,eb90c1ef,7eab958,1d44406a,7b84bd36,3f0d8570,793dd7c5,2a8d9e41,28a92ed5,cd768cc0,e5805e8b,b37d270,ea2984e2,eaf5627,582b47b4,a0b4d458,33091ed6,9c029a4a,13e81c96,ae3409b1,c91db220,c3ac0642,230fb01a,b2673e3c,86ce2ec2,e8fea5a9,143e8020,dae52922,682f1a14,aeb74e76,1ddbe291,622095e7,917d7ec5,83bece6d,f924c117,103f4520,d6c1ab62,2d5752ac,7d60d03c,ae436d6b,b6f9ce27,9f8796ec,10b0d299,4f1fc76e,9cd9a9de,dcffd7b9,7b5724e2,fdf8a45,ff3ddb68,c4bce217,3edc3fd,1e8fcef6,b26e020f,973d9516,53060b4a,81ef9931,69793a1f,e671030c,8732a4b7,1d02a1d5,27de9aaa,4c01b31e,268f24f5,9e8770f4,d57c2c4c,ccd4b279,cacc136b,40a913d,7df360f6,4d1e6ac1,ff82a43f,d563d7f3,364923bb,51331274,1d910fa,122016,7adfcf58,62acfe20,dff061bc,554e5d2c,35df7442
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------
initialized TensorflowDNNClassifier, version v0.1 ( 12006 )
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------
INFO: command: /var/spool/slurmd/job148018/slurm_script -i /work/berger_p2/VHbb/CMSSW_10_1_0/src/Xbb/python/dumps/Zll2017_SR_low_Zll_191022_V11finalVarsWP.h5 -c config/default_momentum.cfg -p FINAL_VHLegacy_2lep_WP_SR_low_Zll
INFO: read inputs from disk, metadata is pesent:
INFO:  >   cut (1 && V_mass > 75 && V_mass < 105 && (H_mass > 90 && H_mass < 150) && Jet_btagDeepB[hJidx[0]] > 0.4941 && Jet_btagDeepB[hJidx[1]] > 0.1522) && (V_pt>=50.0&&V_pt<150.0) && (isZee||isZmm)
INFO:  >   cutName SR_low_Zll
INFO:  >   region SR_low_Zll
INFO:  >   samples {'SIG_ALL': ['ZllH_lep_PTV_0_75_hbb', 'ZllH_lep_PTV_75_150_hbb', 'ZllH_lep_PTV_150_250_0J_hbb', 'ZllH_lep_PTV_150_250_GE1J_hbb', 'ZllH_lep_PTV_GT250_hbb', 'ZnnH_lep_PTV_0_75_hbb', 'ZnnH_lep_PTV_75_150_hbb', 'ZnnH_lep_PTV_150_250_0J_hbb', 'ZnnH_lep_PTV_150_250_GE1J_hbb', 'ZnnH_lep_PTV_GT250_hbb', 'ggZllH_lep_PTV_0_75_hbb', 'ggZllH_lep_PTV_75_150_hbb', 'ggZllH_lep_PTV_150_250_0J_hbb', 'ggZllH_lep_PTV_150_250_GE1J_hbb', 'ggZllH_lep_PTV_GT250_hbb', 'ggZnnH_lep_PTV_0_75_hbb', 'ggZnnH_lep_PTV_75_150_hbb', 'ggZnnH_lep_PTV_150_250_0J_hbb', 'ggZnnH_lep_PTV_150_250_GE1J_hbb', 'ggZnnH_lep_PTV_GT250_hbb', 'WminusH_lep_PTV_0_75_hbb', 'WminusH_lep_PTV_75_150_hbb', 'WminusH_lep_PTV_150_250_0J_hbb', 'WminusH_lep_PTV_150_250_GE1J_hbb', 'WminusH_lep_PTV_GT250_hbb', 'WplusH_lep_PTV_0_75_hbb', 'WplusH_lep_PTV_75_150_hbb', 'WplusH_lep_PTV_150_250_0J_hbb', 'WplusH_lep_PTV_150_250_GE1J_hbb', 'WplusH_lep_PTV_GT250_hbb'], 'BKG_ALL': ['TT_2l2n', 'TT_h', 'TT_Sl', 'ST_tW_antitop', 'ST_tW_top', 'ST_s-channel_4f', 'ST_t-channel_top_4f', 'ST_t-channel_antitop_4f', 'WWTo1L1Nu2Qnlo_0b', 'WZTo1L1Nu2Qnlo_0b', 'ZZTo2L2Qnlo_0b', 'WWTo1L1Nu2Qnlo_1b', 'WWTo1L1Nu2Qnlo_2b', 'WZTo1L1Nu2Qnlo_1b', 'WZTo1L1Nu2Qnlo_2b', 'ZZTo2L2Qnlo_1b', 'ZZTo2L2Qnlo_2b', 'M4HT100to200_0b', 'M4HT100to200_1b', 'M4HT100to200_2b', 'M4HT200to400_0b', 'M4HT200to400_1b', 'M4HT200to400_2b', 'M4HT400to600_0b', 'M4HT400to600_1b', 'M4HT400to600_2b', 'M4HT600toInf_0b', 'M4HT600toInf_1b', 'M4HT600toInf_2b', 'HT0to100ZJets_0b', 'HT0to100ZJets_1b', 'HT0to100ZJets_2b', 'HT100to200ZJets_0b', 'HT100to200ZJets_1b', 'HT100to200ZJets_2b', 'HT200to400ZJets_0b', 'HT200to400ZJets_1b', 'HT200to400ZJets_2b', 'HT400to600ZJets_0b', 'HT400to600ZJets_1b', 'HT400to600ZJets_2b', 'HT600to800ZJets_0b', 'HT600to800ZJets_1b', 'HT600to800ZJets_2b', 'HT800to1200ZJets_0b', 'HT800to1200ZJets_1b', 'HT800to1200ZJets_2b', 'HT1200to2500ZJets_0b', 'HT1200to2500ZJets_1b', 'HT1200to2500ZJets_2b', 'HT2500toinfZJets_0b', 'HT2500toinfZJets_1b', 'HT2500toinfZJets_2b', 'DYBJets_100to200_0b', 'DYBJets_100to200_1b', 'DYBJets_100to200_2b', 'DYBJets_200toInf_0b', 'DYBJets_200toInf_1b', 'DYBJets_200toInf_2b', 'DYJetsBGenFilter_100to200_0b', 'DYJetsBGenFilter_100to200_1b', 'DYJetsBGenFilter_100to200_2b', 'DYJetsBGenFilter_200toInf_0b', 'DYJetsBGenFilter_200toInf_1b', 'DYJetsBGenFilter_200toInf_2b']}
INFO:  >   scaleFactors {'M4HT600toInf_0b': 1.0, 'DYJetsBGenFilter_100to200_1b': 1.0, 'ggZnnH_lep_PTV_GT250_hbb': 1.0, 'ZZTo2L2Qnlo_0b': 1.0, 'ggZllH_lep_PTV_150_250_0J_hbb': 1.0, 'WplusH_lep_PTV_150_250_0J_hbb': 1.0, 'HT2500toinfZJets_2b': 1.0, 'ZllH_lep_PTV_150_250_0J_hbb': 1.0, 'WZTo1L1Nu2Qnlo_0b': 1.0, 'WminusH_lep_PTV_GT250_hbb': 1.0, 'ST_tW_top': 1.0, 'HT400to600ZJets_1b': 1.0, 'ZnnH_lep_PTV_75_150_hbb': 1.0, 'HT100to200ZJets_0b': 1.0, 'DYJetsBGenFilter_200toInf_0b': 1.0, 'DYJetsBGenFilter_100to200_2b': 1.0, 'ggZllH_lep_PTV_GT250_hbb': 1.0, 'WplusH_lep_PTV_GT250_hbb': 1.0, 'HT2500toinfZJets_1b': 1.0, 'ZllH_lep_PTV_GT250_hbb': 1.0, 'WWTo1L1Nu2Qnlo_2b': 1.0, 'HT800to1200ZJets_0b': 1.0, 'ZllH_lep_PTV_75_150_hbb': 1.0, 'WplusH_lep_PTV_0_75_hbb': 1.0, 'DYBJets_200toInf_1b': 1.0, 'M4HT600toInf_2b': 1.0, 'WminusH_lep_PTV_150_250_0J_hbb': 1.0, 'ggZllH_lep_PTV_0_75_hbb': 1.0, 'ggZnnH_lep_PTV_150_250_0J_hbb': 1.0, 'M4HT200to400_1b': 1.0, 'TT_2l2n': 1.0, 'HT0to100ZJets_1b': 1.0, 'DYBJets_200toInf_2b': 1.0, 'DYJetsBGenFilter_200toInf_1b': 1.0, 'HT1200to2500ZJets_1b': 1.0, 'M4HT100to200_2b': 1.0, 'WZTo1L1Nu2Qnlo_2b': 1.0, 'M4HT400to600_0b': 1.0, 'HT600to800ZJets_2b': 1.0, 'M4HT600toInf_1b': 1.0, 'HT800to1200ZJets_2b': 1.0, 'ZllH_lep_PTV_0_75_hbb': 1.0, 'DYJetsBGenFilter_100to200_0b': 1.0, 'ggZnnH_lep_PTV_150_250_GE1J_hbb': 1.0, 'WminusH_lep_PTV_0_75_hbb': 1.0, 'HT0to100ZJets_2b': 1.0, 'WplusH_lep_PTV_75_150_hbb': 1.0, 'HT1200to2500ZJets_0b': 1.0, 'WplusH_lep_PTV_150_250_GE1J_hbb': 1.0, 'WZTo1L1Nu2Qnlo_1b': 1.0, 'ST_t-channel_antitop_4f': 1.0, 'M4HT400to600_1b': 1.0, 'ZZTo2L2Qnlo_1b': 1.0, 'ZnnH_lep_PTV_0_75_hbb': 1.0, 'M4HT100to200_0b': 1.0, 'HT400to600ZJets_2b': 1.0, 'ST_s-channel_4f': 1.0, 'DYBJets_100to200_0b': 1.0, 'DYBJets_200toInf_0b': 1.0, 'M4HT400to600_2b': 1.0, 'ZllH_lep_PTV_150_250_GE1J_hbb': 1.0, 'WWTo1L1Nu2Qnlo_1b': 1.0, 'ZnnH_lep_PTV_150_250_0J_hbb': 1.0, 'HT600to800ZJets_0b': 1.0, 'HT200to400ZJets_2b': 1.0, 'TT_Sl': 1.0, 'M4HT200to400_0b': 1.0, 'HT1200to2500ZJets_2b': 1.0, 'DYBJets_100to200_1b': 1.0, 'HT0to100ZJets_0b': 1.0, 'ZnnH_lep_PTV_150_250_GE1J_hbb': 1.0, 'HT200to400ZJets_1b': 1.0, 'HT400to600ZJets_0b': 1.0, 'WminusH_lep_PTV_75_150_hbb': 1.0, 'HT100to200ZJets_1b': 1.0, 'DYBJets_100to200_2b': 1.0, 'ZZTo2L2Qnlo_2b': 1.0, 'HT2500toinfZJets_0b': 1.0, 'ST_tW_antitop': 1.0, 'ggZnnH_lep_PTV_75_150_hbb': 1.0, 'ST_t-channel_top_4f': 1.0, 'ggZnnH_lep_PTV_0_75_hbb': 1.0, 'HT100to200ZJets_2b': 1.0, 'M4HT100to200_1b': 1.0, 'TT_h': 1.0, 'HT200to400ZJets_0b': 1.0, 'DYJetsBGenFilter_200toInf_2b': 1.0, 'WminusH_lep_PTV_150_250_GE1J_hbb': 1.0, 'ggZllH_lep_PTV_150_250_GE1J_hbb': 1.0, 'ggZllH_lep_PTV_75_150_hbb': 1.0, 'WWTo1L1Nu2Qnlo_0b': 1.0, 'HT600to800ZJets_1b': 1.0, 'HT800to1200ZJets_1b': 1.0, 'M4HT200to400_2b': 1.0, 'ZnnH_lep_PTV_GT250_hbb': 1.0}
INFO:  >   systematics []
INFO:  >   testCut ((event%2)==0||isData)
INFO:  >   trainCut !((event%2)==0||isData)
INFO:  >   variables kinFit_H_mass_fit H_mass kinFit_H_pt_fit H_pt kinFit_HVdPhi_fit abs(VHbb::deltaPhi(H_phi,V_phi)) (Jet_btagDeepB[hJidx[0]]>0.1522)+(Jet_btagDeepB[hJidx[0]]>0.4941)+(Jet_btagDeepB[hJidx[0]]>0.8001) (Jet_btagDeepB[hJidx[1]]>0.1522)+(Jet_btagDeepB[hJidx[1]]>0.4941)+(Jet_btagDeepB[hJidx[1]]>0.8001) kinFit_hJets_pt_0_fit Jet_PtReg[hJidx[0]] kinFit_hJets_pt_1_fit Jet_PtReg[hJidx[1]] kinFit_V_mass_fit V_mass Sum$(Jet_Pt>30&&abs(Jet_eta)<2.4&&(Jet_puId>6||Jet_Pt>50)&&Jet_jetId>0&&Jet_lepFilter>0&&Iteration$!=hJidx[0]&&Iteration$!=hJidx[1]) kinFit_V_pt_fit V_pt kinFit_jjVPtRatio_fit (H_pt/V_pt) abs(Jet_eta[hJidx[0]]-Jet_eta[hJidx[1]]) SA5 VHbb::deltaR(kinFit_H_eta_fit,kinFit_H_phi_fit,kinFit_V_eta_fit,kinFit_V_phi_fit) VHbb::deltaR(H_eta,H_phi,V_eta,V_phi) MET_Pt kinFit_H_mass_sigma_fit kinFit_n_recoil_jets_fit VHbb::deltaR(Jet_eta[hJidx[0]],Jet_phi[hJidx[0]],Jet_eta[hJidx[1]],Jet_phi[hJidx[1]])
INFO:  >   version 3
INFO:  >   weightF genWeight*puWeight*1.0*muonSF_Iso[0]*muonSF_Id[0]*electronSF_IdIso[0]*electronSF_trigger[0]*bTagWeightDeepCSV*EWKw[0]*weightLOtoNLO_2016*FitCorr[0]
INFO:  >   weightSYS []
INFO:  >   xSecs {'M4HT600toInf_0b': 2.2755, 'DYJetsBGenFilter_100to200_1b': 3.2853299999999996, 'ggZnnH_lep_PTV_GT250_hbb': 0.01437, 'ZZTo2L2Qnlo_0b': 3.688, 'ggZllH_lep_PTV_150_250_0J_hbb': 0.0072, 'WplusH_lep_PTV_150_250_0J_hbb': 0.17202, 'HT2500toinfZJets_2b': 0.0042680999999999995, 'ZllH_lep_PTV_150_250_0J_hbb': 0.04718, 'WZTo1L1Nu2Qnlo_0b': 10.87, 'WminusH_lep_PTV_GT250_hbb': 0.10899, 'ST_tW_top': 35.85, 'HT400to600ZJets_1b': 8.57064, 'ZnnH_lep_PTV_75_150_hbb': 0.09322, 'HT100to200ZJets_0b': 198.153, 'DYJetsBGenFilter_200toInf_0b': 0.48388200000000003, 'DYJetsBGenFilter_100to200_2b': 3.2853299999999996, 'ggZllH_lep_PTV_GT250_hbb': 0.0072, 'WplusH_lep_PTV_GT250_hbb': 0.17202, 'HT2500toinfZJets_1b': 0.0042680999999999995, 'ZllH_lep_PTV_GT250_hbb': 0.04718, 'WWTo1L1Nu2Qnlo_2b': 50.85883, 'HT800to1200ZJets_0b': 0.990396, 'ZllH_lep_PTV_75_150_hbb': 0.04718, 'WplusH_lep_PTV_0_75_hbb': 0.17202, 'DYBJets_200toInf_1b': 0.40565399999999996, 'M4HT600toInf_2b': 2.2755, 'WminusH_lep_PTV_150_250_0J_hbb': 0.10899, 'ggZllH_lep_PTV_0_75_hbb': 0.0072, 'ggZnnH_lep_PTV_150_250_0J_hbb': 0.01437, 'M4HT200to400_1b': 66.8997, 'TT_2l2n': 88.29, 'HT0to100ZJets_1b': 6571.89, 'DYBJets_200toInf_2b': 0.40565399999999996, 'DYJetsBGenFilter_200toInf_1b': 0.48388200000000003, 'HT1200to2500ZJets_1b': 0.237759, 'M4HT100to200_2b': 250.92, 'WZTo1L1Nu2Qnlo_2b': 10.87, 'M4HT400to600_0b': 7.00731, 'HT600to800ZJets_2b': 2.1438900000000003, 'M4HT600toInf_1b': 2.2755, 'HT800to1200ZJets_2b': 0.990396, 'ZllH_lep_PTV_0_75_hbb': 0.04718, 'DYJetsBGenFilter_100to200_0b': 3.2853299999999996, 'ggZnnH_lep_PTV_150_250_GE1J_hbb': 0.01437, 'WminusH_lep_PTV_0_75_hbb': 0.10899, 'HT0to100ZJets_2b': 6571.89, 'WplusH_lep_PTV_75_150_hbb': 0.17202, 'HT1200to2500ZJets_0b': 0.237759, 'WplusH_lep_PTV_150_250_GE1J_hbb': 0.17202, 'WZTo1L1Nu2Qnlo_1b': 10.87, 'ST_t-channel_antitop_4f': 80.95, 'M4HT400to600_1b': 7.00731, 'ZZTo2L2Qnlo_1b': 3.688, 'ZnnH_lep_PTV_0_75_hbb': 0.09322, 'M4HT100to200_0b': 250.92, 'HT400to600ZJets_2b': 8.57064, 'ST_s-channel_4f': 3.692, 'DYBJets_100to200_0b': 3.96552, 'DYBJets_200toInf_0b': 0.40565399999999996, 'M4HT400to600_2b': 7.00731, 'ZllH_lep_PTV_150_250_GE1J_hbb': 0.04718, 'WWTo1L1Nu2Qnlo_1b': 50.85883, 'ZnnH_lep_PTV_150_250_0J_hbb': 0.09322, 'HT600to800ZJets_0b': 2.1438900000000003, 'HT200to400ZJets_2b': 59.8518, 'TT_Sl': 365.34, 'M4HT200to400_0b': 66.8997, 'HT1200to2500ZJets_2b': 0.237759, 'DYBJets_100to200_1b': 3.96552, 'HT0to100ZJets_0b': 6571.89, 'ZnnH_lep_PTV_150_250_GE1J_hbb': 0.09322, 'HT200to400ZJets_1b': 59.8518, 'HT400to600ZJets_0b': 8.57064, 'WminusH_lep_PTV_75_150_hbb': 0.10899, 'HT100to200ZJets_1b': 198.153, 'DYBJets_100to200_2b': 3.96552, 'ZZTo2L2Qnlo_2b': 3.688, 'HT2500toinfZJets_0b': 0.0042680999999999995, 'ST_tW_antitop': 35.85, 'ggZnnH_lep_PTV_75_150_hbb': 0.01437, 'ST_t-channel_top_4f': 136.02, 'ggZnnH_lep_PTV_0_75_hbb': 0.01437, 'HT100to200ZJets_2b': 198.153, 'M4HT100to200_1b': 250.92, 'TT_h': 377.96, 'HT200to400ZJets_0b': 59.8518, 'DYJetsBGenFilter_200toInf_2b': 0.48388200000000003, 'WminusH_lep_PTV_150_250_GE1J_hbb': 0.10899, 'ggZllH_lep_PTV_150_250_GE1J_hbb': 0.0072, 'ggZllH_lep_PTV_75_150_hbb': 0.0072, 'WWTo1L1Nu2Qnlo_0b': 50.85883, 'HT600to800ZJets_1b': 2.1438900000000003, 'HT800to1200ZJets_1b': 0.990396, 'M4HT200to400_2b': 66.8997, 'ZnnH_lep_PTV_GT250_hbb': 0.09322}
INFO: random state: (3, (2147483648, 497441267, 4130391968, 487221549, 3927047085, 2829660708, 291609748, 1559810189, 2844109843, 285424168, 1487863720, 1804028062, 3890715095, 1638280331, 1885671135, 584650725, 3837763167, 407750594, 2683869025, 1355556876, 3686823845, 246504564, 3212089705, 3371233613, 1650148760, 3384119874, 733392442, 302340557, 552242674, 2979816229, 1737604039, 403646763, 4000242114, 2684387765, 548358190, 227278939, 1166060452, 4033255635, 207003949, 372638564, 1948415076, 3998687709, 1638728863, 387602106, 3381175914, 3308942646, 1493344761, 2789633255, 3700848806, 1972011961, 3644080071, 1606539071, 2240684297, 4239513217, 1463447187, 2062656245, 2884855863, 196426612, 1991573445, 3035765218, 2574795265, 4142183562, 1041643203, 3307361365, 154055541, 253730400, 136139114, 2026047349, 2867698673, 1570590766, 3415221901, 2672027326, 737042917, 3461738024, 2311375717, 2717328319, 2142097554, 667667020, 3947563908, 4236353780, 1304845735, 4173288231, 1528725303, 1659971712, 1250798722, 333193319, 2071217245, 2565578411, 606512037, 3529477199, 1721145798, 3372662117, 3826255382, 1180465784, 3021772141, 748894021, 3009781253, 887624191, 2121015649, 754021427, 2886080921, 1891556303, 3778980936, 333206586, 1055408943, 1196896275, 4052805470, 1431012570, 2283149431, 1433494722, 1030604358, 2101020688, 3377415471, 1954000879, 2202237370, 1585247071, 3388791678, 446893042, 903471575, 745232795, 4171081230, 1150897310, 1751603299, 2172993935, 3815416757, 2892653274, 2452804958, 134288027, 3299998481, 1288904254, 521151196, 398213021, 2405957591, 2367076792, 4111689336, 294107645, 2595508828, 1446125903, 2410268848, 2886426926, 3585905402, 2332899231, 1664261947, 2944838805, 3393117175, 1383201775, 3895909845, 1121352523, 651287732, 4208155205, 3877734155, 2776617225, 2447661916, 1699838301, 1387860639, 2861193033, 662074982, 166867580, 3810376019, 1390472037, 3782029218, 4224144024, 243924651, 2498297765, 3783632185, 3384474165, 1499909612, 2543037998, 2529165328, 3961364489, 3969275201, 2198824717, 2515227103, 1578491113, 3761867364, 1972941436, 2839421043, 1862344621, 3470940974, 4201345618, 3277357417, 687728641, 3595715104, 4114861649, 2168764824, 3939981595, 4266739436, 3745867644, 2631065298, 1230380042, 3665498236, 1967112516, 3267783896, 2259214887, 3005712511, 2783297841, 4151695821, 4113200002, 2161332601, 759379508, 1708379378, 3892884225, 114785743, 706837063, 3616168738, 411707037, 910008849, 4009424350, 2726912804, 3453369848, 846307853, 2290694628, 1664901821, 2459337106, 2927920189, 822577272, 3167968526, 178196580, 93611138, 1077503989, 2254812282, 303990637, 373066614, 3878098156, 2835770174, 3642776120, 3123639391, 124343555, 2715589305, 1138918041, 3663279520, 381836069, 505090290, 4291095284, 3832037160, 2113468251, 293167191, 813979880, 930006492, 2041143739, 3967050618, 4255825294, 1891730842, 1881527722, 3752275796, 1463272439, 1591302614, 1536887791, 3820018796, 3744116373, 3574866981, 2427112247, 817347748, 994451167, 3360858758, 3204272332, 2836336351, 2081126409, 491315527, 1209084714, 2228815068, 3490144831, 171640204, 2531187138, 379888533, 2183955959, 1252238105, 1287157446, 3701251975, 1195032990, 2535033467, 1031368155, 3507129224, 2873553927, 3437362416, 650555787, 321381169, 3020456222, 2773215269, 995389643, 603566, 2321162706, 837569742, 326004938, 861397871, 3207548992, 3203465920, 3447181425, 3557107714, 3333210568, 2134906174, 2519828203, 4282636473, 827009294, 772620015, 2001242207, 3608063484, 3765768483, 3414642174, 2354423888, 174115820, 3422689489, 4294499760, 1388839872, 3306596434, 453017848, 3115933699, 2042495805, 1936861266, 1326193146, 3508547713, 535915393, 2806928923, 1509141600, 1189586711, 2794359754, 1727351508, 3638523170, 3818963124, 2522630838, 4178993870, 3638954532, 1641620565, 3944410889, 631618012, 3671131709, 3383342956, 216325232, 3085330409, 3500515667, 3498497107, 375284701, 932296325, 1397575505, 2386521918, 3149682654, 1768981700, 3761367762, 2596020574, 1061568186, 3852536181, 1458821104, 945336325, 2959951187, 2709185817, 3441491939, 2054853598, 2370566654, 1462613061, 863783803, 645130746, 4249632214, 64906281, 3505428478, 3412082575, 99762573, 2682740736, 1544556949, 3541080115, 3498796766, 2278934179, 3942802479, 2911318806, 2771872508, 3218149907, 825400779, 4149908676, 454232300, 4281724513, 3409396359, 2545299109, 2960742585, 3669554107, 1126989063, 2262744347, 1891394138, 2757098191, 2054749943, 3578721332, 3934847903, 876765607, 683466296, 3197455095, 2828840388, 2242976497, 1722058805, 994013972, 2266737332, 1433040556, 2615117723, 1100132536, 1889267831, 3228646059, 1030541974, 1431409102, 1928267961, 3636179055, 973770168, 623818005, 679871645, 3979983112, 1141584936, 3898860499, 1504555525, 744424232, 180034831, 2677917277, 2064621064, 1472918680, 2064165675, 345638211, 214637535, 2469487668, 871662626, 4113601546, 1590412618, 141113357, 3276449347, 1406660930, 2443210207, 1939236396, 2336164056, 3729766811, 2004318385, 116137855, 2202721712, 3625677105, 3665515727, 3634247298, 1359449224, 4157490318, 4078749441, 580335950, 1086167984, 715443433, 2150819353, 860138311, 3587067333, 791227368, 46123443, 3235946285, 240485689, 2196876755, 803631984, 194674179, 1242667378, 2291811468, 3178346782, 2790367223, 3379906541, 1245606954, 136354437, 829033714, 1005354219, 1218886946, 3779712862, 1197178278, 2174368635, 4035777236, 1851599677, 1481180813, 2785434551, 1292429786, 3904001124, 2500479587, 1868920741, 1338310398, 1989807821, 842820502, 779411483, 1735193218, 47903054, 642731298, 519634689, 886854636, 274962319, 1102818177, 3858124294, 3612590443, 3685599798, 726621093, 158417790, 2660777122, 1373358635, 3980379360, 456584900, 95892446, 3761082064, 3561832840, 2777565090, 179998823, 2450236980, 1357396366, 3596033522, 3955950434, 3459035613, 1466460551, 1137508107, 4073755585, 2855101903, 994647204, 990170442, 2258709510, 14466791, 1325685838, 1475092167, 3256016509, 624280808, 815086448, 2999058741, 2952370503, 540203464, 312696342, 4138203848, 2611662416, 2540879525, 3956824481, 2026156404, 3681586183, 1224232183, 112196345, 569648271, 2529923330, 3996222638, 2623637985, 2339708515, 467509281, 3860203648, 3992266706, 511256112, 809691617, 1605970241, 287880117, 3868567171, 3349393071, 1856533472, 773557793, 1520514391, 2555096087, 90778089, 3915090582, 3644976953, 2485017533, 3864430667, 2043691474, 591298306, 1962467109, 2323373211, 662379483, 1247373219, 379928827, 2662953455, 821525501, 580622987, 611664722, 3897972061, 2183768118, 3340126453, 2623538196, 837454519, 2199157268, 2898996337, 3543679595, 3419965970, 2067957459, 3201204730, 1591692898, 3885686087, 3825919856, 1278924777, 1972616061, 3633142292, 331821236, 82624194, 2502408134, 3756046220, 1147081466, 709287968, 2359767966, 4176203746, 4244935113, 1362601447, 138811198, 2994072766, 2401740570, 158805273, 2118865270, 4111190259, 3181571019, 3684258825, 1581328921, 2173607487, 1922573463, 489958221, 2522876714, 2050892932, 288631033, 3013615610, 1966126663, 884609059, 276898853, 3534708358, 779218747, 3455257372, 3473608355, 4128948745, 2433323303, 4251722346, 1648533829, 3494040993, 2296690887, 1732831989, 4266508944, 2570370954, 3941003374, 3575331265, 872600574, 3696897998, 2936050384, 4193670585, 548038636, 866356313, 363055960, 1455893442, 624), None)
INFO: set 10524 events to 0 because of negative weight
nFeatures =  27
--------------------------------------------------------------------------------
statistics for dataset: train
--------------------------------------------------------------------------------
SIG_ALL (y= 0 ) : 221222  avg weight: 0.00043819292189937976
BKG_ALL (y= 1 ) : 117717  avg weight: 0.1912605074804773
--------------------------------------------------------------------------------
statistics for dataset: test
--------------------------------------------------------------------------------
SIG_ALL (y= 0 ) : 220310  avg weight: 0.00043238692805790694
BKG_ALL (y= 1 ) : 118568  avg weight: 0.18749900487599694
--------------------------------------------------------------------------------
classes and labels
--------------------------------------------------------------------------------
list of classes: (signals in [32mgreen[0m, backgrounds in [31mred[0m)
[32m class 0 => SIG_ALL [0m is defined as a SIGNAL
[31m class 1 => BKG_ALL [0m
--------------------------------------------------------------------------------
weights and weight uncertainty examples
--------------------------------------------------------------------------------
weights:
train 0.00069325353 0.0011857942 0.0006333884 0.00063078443 0.0 0.00064805907 0.00041071084 0.00046593318 0.0006098744 0.0011216528
test  0.0006912686 0.00077690417 0.0010121835 0.00058697694 0.00081025687 0.0007340349 0.00076488603 0.0006416359 0.0008975615 0.0006360226
weights errors:
train 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
test  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
--------------------------------------------------------------------------------
input data
--------------------------------------------------------------------------------
feature                                            set   mean       std        examples
kinFit_H_mass_fit                                  train 1.10e+02   3.57e+01   118.64322 126.33046 115.682396 109.7067
kinFit_H_mass_fit                                  test  1.10e+02   3.54e+01   120.9713 131.43394 120.74222 90.710625
H_mass                                             train 1.18e+02   1.72e+01   134.73918 124.5817 122.80882 111.58908
H_mass                                             test  1.18e+02   1.71e+01   118.48677 127.97328 116.56078 115.791565
kinFit_H_pt_fit                                    train 8.59e+01   4.56e+01   59.758026 55.65296 50.48005 60.048813
kinFit_H_pt_fit                                    test  8.57e+01   4.56e+01   10.022633 191.83826 75.10405 72.63176
H_pt                                               train 8.72e+01   4.73e+01   52.117123 53.716095 62.179432 60.75435
H_pt                                               test  8.73e+01   4.72e+01   6.3139772 181.59222 79.86435 98.2841
kinFit_HVdPhi_fit                                  train 2.90e+00   8.29e-01   3.318525 3.1119933 2.4176843 3.159983
kinFit_HVdPhi_fit                                  test  2.90e+00   8.23e-01   1.6264024 3.441664 3.0796628 2.9983983
abs(VHbb::deltaPhi(H_phi,V_phi))                   train 2.46e+00   7.32e-01   2.7114027 2.9798713 2.3047526 3.077141
abs(VHbb::deltaPhi(H_phi,V_phi))                   test  2.46e+00   7.30e-01   1.2239364 2.8486156 2.8233936 2.6416216
(Jet_btagDeepB[hJidx[0]]>0.1522)+(Jet_btagDeep...  train 2.73e+00   4.45e-01   3.0 3.0 3.0 3.0
(Jet_btagDeepB[hJidx[0]]>0.1522)+(Jet_btagDeep...  test  2.72e+00   4.47e-01   3.0 3.0 3.0 3.0
(Jet_btagDeepB[hJidx[1]]>0.1522)+(Jet_btagDeep...  train 1.72e+00   8.27e-01   3.0 1.0 3.0 3.0
(Jet_btagDeepB[hJidx[1]]>0.1522)+(Jet_btagDeep...  test  1.71e+00   8.23e-01   3.0 1.0 3.0 2.0
kinFit_hJets_pt_0_fit                              train 6.36e+01   3.64e+01   91.97695 63.139675 86.071075 37.19175
kinFit_hJets_pt_0_fit                              test  6.33e+01   3.62e+01   61.46679 50.955505 72.85097 65.237526
Jet_PtReg[hJidx[0]]                                train 6.35e+01   3.25e+01   89.77448 57.01301 95.29095 35.481083
Jet_PtReg[hJidx[0]]                                test  6.34e+01   3.22e+01   59.99497 47.040413 83.68291 64.07248
kinFit_hJets_pt_1_fit                              train 5.81e+01   3.67e+01   35.408745 72.82957 38.751648 39.291843
kinFit_hJets_pt_1_fit                              test  5.81e+01   3.66e+01   59.626694 181.36478 59.244385 49.360184
Jet_PtReg[hJidx[1]]                                train 5.71e+01   3.29e+01   46.337837 73.20374 38.93799 41.68368
Jet_PtReg[hJidx[1]]                                test  5.74e+01   3.29e+01   56.465023 173.01024 47.242805 83.94309
kinFit_V_mass_fit                                  train 9.02e+01   5.39e+00   88.464676 91.379944 92.928406 93.63553
kinFit_V_mass_fit                                  test  9.03e+01   5.39e+00   91.62942 89.53336 93.855194 90.54829
V_mass                                             train 9.02e+01   5.96e+00   87.39481 91.319916 93.43828 93.80231
V_mass                                             test  9.03e+01   5.98e+00   91.62446 89.317505 93.68311 90.653336
Sum$(Jet_Pt>30&&abs(Jet_eta)<2.4&&(Jet_puId>6|...  train 5.58e-01   8.23e-01   0.0 0.0 1.0 0.0
Sum$(Jet_Pt>30&&abs(Jet_eta)<2.4&&(Jet_puId>6|...  test  5.55e-01   8.18e-01   1.0 2.0 0.0 0.0
kinFit_V_pt_fit                                    train 8.23e+01   2.46e+01   59.225048 56.933716 64.793884 59.745117
kinFit_V_pt_fit                                    test  8.23e+01   2.47e+01   65.86154 57.467587 74.76742 65.55585
V_pt                                               train 8.24e+01   2.46e+01   58.562378 56.93903 64.841705 60.061398
V_pt                                               test  8.23e+01   2.47e+01   66.18542 57.76677 74.24989 65.98686
kinFit_jjVPtRatio_fit                              train 1.07e+00   5.52e-01   1.0089992 0.9775045 0.77908665 1.0050832
kinFit_jjVPtRatio_fit                              test  1.07e+00   5.49e-01   0.15217732 3.3381994 1.0045023 1.1079372
(H_pt/V_pt)                                        train 1.10e+00   6.00e-01   0.88994205 0.94339675 0.95894194 1.0115374
(H_pt/V_pt)                                        test  1.10e+00   5.99e-01   0.095398314 3.1435413 1.0756156 1.4894496
abs(Jet_eta[hJidx[0]]-Jet_eta[hJidx[1]])           train 1.21e+00   7.46e-01   0.64889526 0.3499756 0.32250977 2.1499023
abs(Jet_eta[hJidx[0]]-Jet_eta[hJidx[1]])           test  1.20e+00   7.43e-01   0.13895035 0.1060791 0.7811279 0.33789062
SA5                                                train 2.17e+00   1.93e+00   1.0 1.0 1.0 0.0
SA5                                                test  2.15e+00   1.92e+00   4.0 2.0 2.0 4.0
VHbb::deltaR(kinFit_H_eta_fit,kinFit_H_phi_fit...  train 3.00e+00   7.39e-01   2.972813 3.1329348 2.461992 3.4103754
VHbb::deltaR(kinFit_H_eta_fit,kinFit_H_phi_fit...  test  3.00e+00   7.34e-01   2.2124448 3.240026 3.3577135 3.1300325
VHbb::deltaR(H_eta,H_phi,V_eta,V_phi)              train 2.86e+00   7.81e-01   2.7129695 3.0040135 2.329946 3.4031377
VHbb::deltaR(H_eta,H_phi,V_eta,V_phi)              test  2.86e+00   7.76e-01   1.684972 3.244772 3.1789603 2.791785
MET_Pt                                             train 5.02e+01   3.54e+01   5.944703 55.614925 29.145449 17.666605
MET_Pt                                             test  5.04e+01   3.62e+01   18.595234 61.410618 25.294308 82.559
kinFit_H_mass_sigma_fit                            train 7.47e+00   1.07e+01   5.4201913 2.598486 4.559727 4.436768
kinFit_H_mass_sigma_fit                            test  7.39e+00   8.12e+00   0.21040472 12.230204 4.42472 5.572996
kinFit_n_recoil_jets_fit                           train 5.83e-01   9.65e-01   0.0 0.0 1.0 0.0
kinFit_n_recoil_jets_fit                           test  5.68e-01   9.61e-01   1.0 2.0 0.0 0.0
VHbb::deltaR(Jet_eta[hJidx[0]],Jet_phi[hJidx[0...  train 2.30e+00   6.27e-01   2.7683895 2.3525755 2.725892 2.5300086
VHbb::deltaR(Jet_eta[hJidx[0]],Jet_phi[hJidx[0...  test  2.30e+00   6.27e-01   3.0547798 1.5234553 2.0917358 1.7430077
--------------------------------------------------------------------------------
input scaling
--------------------------------------------------------------------------------
[31mINFO: scaling is done inside tensorflow graph and StandardScaler() should not be used om top of it => scaler.dmp file will not be written![0m
number of expected events (test): {0: 22231.382010137204, 1: 95.25916412043748}
number of expected events (train): {0: 22514.613159079345, 1: 96.93791456842459}
balancing signal/background, reweight class SIG_ALL  from group 1 (signals) by 233.25807218275963
balancing signal/background, reweight class BKG_ALL  from group 0 (backgrounds) by 1.004305555413433
shape train: (338939, 27)
shape test:  (338878, 27)
building tensorflow graph with parameters
 adam_epsilon                             1e-09
 adaptiveRate                             False
 additional_noise                         0.0
 balanceClasses                           False
 balanceSignalBackground                  True
 batchNormalization                       [1, 2, 3, 4, 5, 6, 7, 8]
 batchSize                                1024
 batchSizeTest                            65536
 binMethod                                'SB'
 binTarget                                [0.109, 0.1194, 0.1242, 0.1226, 0.1148, 0.1021, 0.0861, 0.069, 0.0524, 0.0378, 0.0259, 0.0168, 0.0104, 0.0061, 0.0034]
 crossValidation_splitSeed                123456
 dropoutDecay                             1.0
 ignoreLargeWeights                       False
 ignoreNegativeWeights                    True
 learningRate                             {0: 1.0, 50: 0.5, 100: 0.25, 200: 0.1, 300: 0.05, 400: 0.02, 500: 0.01, 600: 0.005, 700: 0.002, 800: 0.001}
 loss                                     'cross_entropy'
 massless_importance                      1.0
 massless_powers                          [1, 2]
 momentum                                 0.9
 mvaScoreRescalingPercentileHigh          0.999
 mvaScoreRescalingPercentileLow           0.01
 nEpochs                                  1000
 nNodes                                   [512, 256, 128, 64, 64, 64]
 optimizer                                'momentum'
 pDropout                                 [0.2, 0.5, 0.5, 0.5, 0.5, 0.5]
 plot-data                                False
 plot-inputs                              True
 plot-jacobian                            False
 plot-scores                              True
 power                                    1.0
 rateGamma                                1.0
 removeFeature                            []
 reweight                                 None
 reweightTraining                         None
 saveCheckpointInterval                   50
 scaleInputsInsideGraph                   True
 shuffle                                  True
 signif_loss_b_epsilon                    1e-08
 signif_loss_low_b_threshold              1.5
 signif_loss_low_b_threshold_width        1.5
 signif_loss_nbins                        15
 signif_loss_smoothness                   500.0
 signif_loss_sysApprox_constant           1.5
 signif_loss_sysApprox_linear             0.1
 signif_loss_sys_variance_offset          0.1
 signif_loss_xe_factor                    0.0
 skipConnections                          {2: [0], 4: [0, 2], 6: [0, 2, 4]}
 statisticsInterval                       20
 systematics_scaling_factor               1.0
 systematics_weight_scaling_factor        1.0
 weight_sys_ntoys                         -1
initialize session...
initialized session!
add layers...
layer  1 :  [27, 512]
> activation with drop-out...
> batch normalization...
layer  2 :  [512, 256]
> activation with drop-out...
> batch normalization...
layer  3 :  [256, 128]
> activation with drop-out...
> batch normalization...
layer  4 :  [128, 64]
> activation with drop-out...
> batch normalization...
layer  5 :  [64, 64]
> activation with drop-out...
> batch normalization...
layer  6 :  [64, 64]
> activation with drop-out...
> batch normalization...
INFO: use MomentumOptimizer
graph built.
trainable variables: 242498
initialized.
conditions:
  loss(train,training)    training dataset, loss as seen by gradient descend optimizer
  loss(train,testing)     training dataset, loss with 'testing' conditions, e.g. is_training: False, no dropout etc
  loss(test)              test dataset
start training with batch size 1024 and learning rate 1.0 
 epoch     loss(train,training) loss(train,testing) loss(test)
         1    0.07317    0.06829    0.06747 significance (train): 1.314 significance: 1.297 
         2    0.06912    0.06604    0.06529 
         3    0.06824    0.06519    0.06463 
         4    0.06796    0.06526    0.06470 
         5    0.06761    0.06458    0.06405 
         6    0.06741    0.06575    0.06522 
         7    0.06697    0.06434    0.06395 
         8    0.06667    0.06460    0.06418 
         9    0.06676    0.06467    0.06441 
        10    0.06631    0.06379    0.06348 
        11    0.06646    0.06407    0.06373 
        12    0.06648    0.06448    0.06430 
        13    0.06636    0.06380    0.06368 
        14    0.06633    0.06351    0.06340 
        15    0.06613    0.06346    0.06339 
        16    0.06589    0.06423    0.06398 
        17    0.06628    0.06414    0.06407 
        18    0.06603    0.06323    0.06325 
        19    0.06550    0.06342    0.06339 
        20    0.06607    0.06348    0.06339 
        21    0.06574    0.06416    0.06422 significance (train): 1.509 significance: 1.451 
        22    0.06565    0.06320    0.06324 
        23    0.06555    0.06379    0.06384 
        24    0.06544    0.06337    0.06326 
        25    0.06576    0.06307    0.06311 
        26    0.06553    0.06462    0.06498 
        27    0.06547    0.06373    0.06381 
        28    0.06532    0.06409    0.06430 
        29    0.06549    0.06287    0.06321 
        30    0.06520    0.06284    0.06308 
        31    0.06519    0.06446    0.06474 
        32    0.06529    0.06270    0.06310 
        33    0.06501    0.06324    0.06372 
        34    0.06515    0.06283    0.06308 
        35    0.06488    0.06284    0.06303 
        36    0.06495    0.06264    0.06315 
        37    0.06500    0.06376    0.06421 
        38    0.06489    0.06246    0.06283 
        39    0.06484    0.06260    0.06297 
        40    0.06494    0.06337    0.06367 
        41    0.06490    0.06269    0.06288 significance (train): 1.523 significance: 1.455 
        42    0.06480    0.06254    0.06306 
        43    0.06470    0.06286    0.06350 
        44    0.06488    0.06264    0.06324 
        45    0.06478    0.06257    0.06313 
        46    0.06474    0.06258    0.06285 
        47    0.06466    0.06353    0.06376 
        48    0.06466    0.06238    0.06280 
        49    0.06459    0.06233    0.06288 
        50    0.06470    0.06225    0.06290 
set learning rate to: 0.5
        51    0.06377    0.06202    0.06255 
        52    0.06415    0.06192    0.06259 
        53    0.06379    0.06189    0.06262 
        54    0.06382    0.06244    0.06292 
        55    0.06400    0.06184    0.06260 
        56    0.06372    0.06261    0.06311 
        57    0.06369    0.06193    0.06266 
        58    0.06386    0.06241    0.06293 
        59    0.06367    0.06185    0.06272 
        60    0.06369    0.06196    0.06269 
        61    0.06366    0.06176    0.06268 significance (train): 1.536 significance: 1.460 
        62    0.06373    0.06173    0.06251 
        63    0.06358    0.06210    0.06281 
        64    0.06375    0.06186    0.06266 
        65    0.06371    0.06166    0.06264 
        66    0.06360    0.06186    0.06253 
        67    0.06356    0.06165    0.06267 
        68    0.06353    0.06180    0.06289 
        69    0.06377    0.06188    0.06262 
        70    0.06345    0.06202    0.06274 
        71    0.06352    0.06189    0.06281 
        72    0.06350    0.06171    0.06255 
        73    0.06373    0.06176    0.06249 
        74    0.06361    0.06260    0.06335 
        75    0.06369    0.06158    0.06247 
        76    0.06362    0.06153    0.06259 
        77    0.06366    0.06152    0.06269 
        78    0.06345    0.06158    0.06255 
        79    0.06345    0.06155    0.06260 
        80    0.06341    0.06170    0.06250 
        81    0.06348    0.06168    0.06266 significance (train): 1.562 significance: 1.474 
        82    0.06360    0.06140    0.06252 
        83    0.06343    0.06142    0.06251 
        84    0.06350    0.06150    0.06246 
        85    0.06364    0.06138    0.06254 
        86    0.06359    0.06157    0.06256 
        87    0.06341    0.06189    0.06290 
        88    0.06338    0.06143    0.06268 
        89    0.06354    0.06159    0.06253 
        90    0.06339    0.06142    0.06267 
        91    0.06343    0.06149    0.06276 
        92    0.06357    0.06251    0.06334 
        93    0.06346    0.06154    0.06255 
        94    0.06341    0.06235    0.06313 
        95    0.06353    0.06174    0.06270 
        96    0.06345    0.06177    0.06269 
        97    0.06348    0.06187    0.06305 
        98    0.06338    0.06135    0.06262 
        99    0.06337    0.06175    0.06270 
       100    0.06338    0.06143    0.06267 
set learning rate to: 0.25
       101    0.06297    0.06126    0.06251 significance (train): 1.551 significance: 1.457 
       102    0.06299    0.06128    0.06238 
       103    0.06292    0.06119    0.06240 
       104    0.06299    0.06127    0.06245 
       105    0.06284    0.06109    0.06235 
       106    0.06287    0.06102    0.06241 
       107    0.06279    0.06116    0.06241 
       108    0.06281    0.06116    0.06245 
       109    0.06299    0.06118    0.06235 
       110    0.06282    0.06090    0.06239 
       111    0.06290    0.06099    0.06249 
       112    0.06270    0.06102    0.06247 
       113    0.06290    0.06106    0.06240 
       114    0.06289    0.06087    0.06250 
       115    0.06280    0.06108    0.06244 
       116    0.06291    0.06106    0.06250 
       117    0.06287    0.06096    0.06233 
       118    0.06275    0.06084    0.06249 
       119    0.06290    0.06097    0.06245 
       120    0.06294    0.06111    0.06263 
       121    0.06267    0.06105    0.06251 significance (train): 1.579 significance: 1.473 
       122    0.06277    0.06088    0.06237 
       123    0.06296    0.06085    0.06246 
       124    0.06283    0.06085    0.06252 
       125    0.06284    0.06093    0.06237 
       126    0.06285    0.06097    0.06238 
       127    0.06278    0.06077    0.06245 
       128    0.06290    0.06080    0.06258 
       129    0.06283    0.06104    0.06260 
       130    0.06283    0.06092    0.06244 
       131    0.06279    0.06114    0.06250 
       132    0.06275    0.06097    0.06241 
       133    0.06284    0.06085    0.06242 
       134    0.06269    0.06099    0.06257 
       135    0.06281    0.06095    0.06241 
       136    0.06278    0.06119    0.06279 
       137    0.06280    0.06140    0.06283 
       138    0.06278    0.06092    0.06253 
       139    0.06271    0.06085    0.06246 
       140    0.06275    0.06104    0.06257 
       141    0.06261    0.06076    0.06241 significance (train): 1.584 significance: 1.470 
       142    0.06276    0.06073    0.06243 
       143    0.06279    0.06097    0.06244 
       144    0.06264    0.06081    0.06241 
       145    0.06272    0.06117    0.06264 
       146    0.06275    0.06112    0.06262 
       147    0.06276    0.06087    0.06245 
       148    0.06269    0.06077    0.06247 
       149    0.06272    0.06066    0.06244 
       150    0.06261    0.06063    0.06255 
       151    0.06265    0.06082    0.06245 
       152    0.06262    0.06061    0.06235 
       153    0.06270    0.06070    0.06237 
       154    0.06275    0.06074    0.06230 
       155    0.06274    0.06100    0.06241 
       156    0.06261    0.06075    0.06258 
       157    0.06263    0.06057    0.06245 
       158    0.06263    0.06087    0.06256 
       159    0.06267    0.06111    0.06281 
       160    0.06263    0.06059    0.06260 
       161    0.06266    0.06066    0.06240 significance (train): 1.583 significance: 1.460 
       162    0.06267    0.06071    0.06237 
       163    0.06268    0.06079    0.06249 
       164    0.06268    0.06069    0.06249 
       165    0.06243    0.06063    0.06239 
       166    0.06270    0.06075    0.06249 
       167    0.06267    0.06077    0.06234 
       168    0.06248    0.06062    0.06236 
       169    0.06276    0.06086    0.06231 
       170    0.06264    0.06091    0.06247 
       171    0.06258    0.06066    0.06240 
       172    0.06255    0.06071    0.06247 
       173    0.06273    0.06070    0.06234 
       174    0.06265    0.06080    0.06268 
       175    0.06280    0.06048    0.06253 
       176    0.06259    0.06063    0.06242 
       177    0.06259    0.06051    0.06239 
       178    0.06259    0.06055    0.06238 
       179    0.06264    0.06057    0.06255 
       180    0.06271    0.06061    0.06241 
       181    0.06264    0.06064    0.06244 significance (train): 1.586 significance: 1.474 
       182    0.06261    0.06057    0.06260 
       183    0.06244    0.06127    0.06277 
       184    0.06262    0.06068    0.06237 
       185    0.06266    0.06055    0.06238 
       186    0.06266    0.06068    0.06263 
       187    0.06270    0.06082    0.06247 
       188    0.06276    0.06088    0.06259 
       189    0.06252    0.06059    0.06258 
       190    0.06258    0.06063    0.06236 
       191    0.06265    0.06050    0.06235 
       192    0.06242    0.06051    0.06241 
       193    0.06257    0.06055    0.06247 
       194    0.06265    0.06044    0.06244 
       195    0.06243    0.06077    0.06261 
       196    0.06255    0.06051    0.06240 
       197    0.06266    0.06089    0.06279 
       198    0.06251    0.06052    0.06256 
       199    0.06251    0.06060    0.06262 
       200    0.06255    0.06062    0.06258 
set learning rate to: 0.1
       201    0.06229    0.06040    0.06238 significance (train): 1.592 significance: 1.469 
       202    0.06226    0.06043    0.06242 
       203    0.06213    0.06060    0.06258 
       204    0.06227    0.06051    0.06235 
       205    0.06232    0.06034    0.06236 
       206    0.06229    0.06028    0.06240 
       207    0.06238    0.06042    0.06231 
       208    0.06229    0.06049    0.06243 
       209    0.06219    0.06026    0.06233 
       210    0.06224    0.06033    0.06236 
       211    0.06226    0.06025    0.06241 
       212    0.06227    0.06029    0.06239 
       213    0.06215    0.06053    0.06272 
       214    0.06225    0.06038    0.06235 
       215    0.06231    0.06032    0.06237 
       216    0.06216    0.06030    0.06237 
       217    0.06232    0.06036    0.06234 
       218    0.06223    0.06023    0.06244 
       219    0.06220    0.06042    0.06234 
       220    0.06215    0.06029    0.06248 
       221    0.06211    0.06024    0.06232 significance (train): 1.597 significance: 1.478 
       222    0.06222    0.06020    0.06238 
       223    0.06215    0.06051    0.06262 
       224    0.06228    0.06034    0.06245 
       225    0.06217    0.06027    0.06235 
       226    0.06206    0.06031    0.06236 
       227    0.06212    0.06028    0.06232 
       228    0.06209    0.06015    0.06242 
       229    0.06221    0.06037    0.06231 
       230    0.06220    0.06047    0.06236 
       231    0.06228    0.06030    0.06228 
       232    0.06226    0.06018    0.06242 
       233    0.06197    0.06018    0.06241 
       234    0.06216    0.06030    0.06248 
       235    0.06227    0.06021    0.06234 
       236    0.06219    0.06027    0.06236 
       237    0.06215    0.06016    0.06237 
       238    0.06216    0.06029    0.06237 
       239    0.06211    0.06013    0.06234 
       240    0.06221    0.06031    0.06246 
       241    0.06212    0.06018    0.06239 significance (train): 1.601 significance: 1.460 
       242    0.06209    0.06013    0.06235 
       243    0.06206    0.06015    0.06229 
       244    0.06218    0.06015    0.06230 
       245    0.06218    0.06031    0.06244 
       246    0.06214    0.06036    0.06246 
       247    0.06210    0.06031    0.06243 
       248    0.06220    0.06017    0.06236 
       249    0.06212    0.06015    0.06244 
       250    0.06209    0.06011    0.06240 
       251    0.06213    0.06016    0.06249 
       252    0.06210    0.06023    0.06236 
       253    0.06203    0.06018    0.06236 
       254    0.06209    0.06010    0.06233 
       255    0.06209    0.06008    0.06248 
       256    0.06202    0.06019    0.06236 
       257    0.06209    0.06029    0.06236 
       258    0.06208    0.06059    0.06257 
       259    0.06212    0.06017    0.06235 
       260    0.06205    0.06020    0.06234 
       261    0.06217    0.06025    0.06229 significance (train): 1.613 significance: 1.485 
       262    0.06198    0.06014    0.06241 
       263    0.06210    0.06009    0.06252 
       264    0.06210    0.06007    0.06259 
       265    0.06207    0.06024    0.06231 
       266    0.06208    0.06005    0.06239 
       267    0.06211    0.06004    0.06247 
       268    0.06205    0.06032    0.06246 
       269    0.06213    0.06019    0.06236 
       270    0.06207    0.06012    0.06243 
       271    0.06217    0.06055    0.06250 
       272    0.06208    0.06005    0.06251 
       273    0.06223    0.06017    0.06230 
       274    0.06218    0.06009    0.06233 
       275    0.06209    0.06021    0.06231 
       276    0.06207    0.06029    0.06241 
       277    0.06199    0.06008    0.06241 
       278    0.06205    0.06008    0.06236 
       279    0.06216    0.06000    0.06249 
       280    0.06200    0.06012    0.06243 
       281    0.06210    0.06014    0.06252 significance (train): 1.609 significance: 1.466 
       282    0.06229    0.06006    0.06243 
       283    0.06220    0.06020    0.06229 
       284    0.06208    0.06023    0.06253 
       285    0.06214    0.06007    0.06237 
       286    0.06210    0.06020    0.06241 
       287    0.06212    0.06003    0.06235 
       288    0.06205    0.06003    0.06242 
       289    0.06216    0.06015    0.06234 
       290    0.06208    0.06000    0.06254 
       291    0.06211    0.06015    0.06233 
       292    0.06211    0.06015    0.06239 
       293    0.06197    0.05996    0.06248 
       294    0.06196    0.05997    0.06253 
       295    0.06223    0.06008    0.06251 
       296    0.06206    0.06000    0.06240 
       297    0.06191    0.06009    0.06232 
       298    0.06194    0.05991    0.06244 
       299    0.06210    0.06034    0.06264 
       300    0.06199    0.06018    0.06243 
set learning rate to: 0.05
       301    0.06188    0.05996    0.06241 significance (train): 1.604 significance: 1.462 
       302    0.06183    0.05996    0.06233 
       303    0.06181    0.06010    0.06239 
       304    0.06181    0.05995    0.06240 
       305    0.06207    0.06002    0.06234 
       306    0.06196    0.06007    0.06230 
       307    0.06213    0.06007    0.06232 
       308    0.06193    0.05997    0.06234 
       309    0.06185    0.06001    0.06232 
       310    0.06185    0.06002    0.06239 
       311    0.06191    0.06002    0.06249 
       312    0.06202    0.05992    0.06235 
       313    0.06189    0.05995    0.06238 
       314    0.06203    0.06007    0.06236 
       315    0.06199    0.06001    0.06244 
       316    0.06200    0.06000    0.06234 
       317    0.06184    0.06000    0.06238 
       318    0.06190    0.05995    0.06247 
       319    0.06193    0.05995    0.06237 
       320    0.06196    0.05993    0.06233 
       321    0.06187    0.06003    0.06240 significance (train): 1.617 significance: 1.482 
       322    0.06192    0.05995    0.06241 
       323    0.06201    0.05994    0.06235 
       324    0.06193    0.05991    0.06233 
       325    0.06190    0.05994    0.06236 
       326    0.06205    0.05991    0.06233 
       327    0.06187    0.06004    0.06237 
       328    0.06203    0.05993    0.06234 
       329    0.06198    0.05992    0.06239 
       330    0.06188    0.05999    0.06238 
       331    0.06197    0.05987    0.06240 
       332    0.06191    0.05991    0.06237 
       333    0.06200    0.05991    0.06238 
       334    0.06187    0.05992    0.06240 
       335    0.06193    0.05987    0.06247 
       336    0.06180    0.06003    0.06244 
       337    0.06188    0.05987    0.06238 
       338    0.06193    0.05991    0.06236 
       339    0.06198    0.06000    0.06242 
       340    0.06187    0.05987    0.06248 
       341    0.06193    0.05990    0.06235 significance (train): 1.607 significance: 1.470 
       342    0.06187    0.05993    0.06242 
       343    0.06184    0.05997    0.06234 
       344    0.06201    0.05992    0.06237 
       345    0.06203    0.05991    0.06242 
       346    0.06197    0.05988    0.06234 
       347    0.06202    0.06005    0.06254 
       348    0.06192    0.05987    0.06238 
       349    0.06184    0.06030    0.06263 
       350    0.06186    0.05983    0.06245 
       351    0.06199    0.05985    0.06242 
       352    0.06186    0.05987    0.06244 
       353    0.06190    0.05998    0.06233 
       354    0.06190    0.05994    0.06237 
       355    0.06170    0.05984    0.06236 
       356    0.06180    0.05991    0.06242 
       357    0.06190    0.06011    0.06243 
       358    0.06187    0.05980    0.06264 
       359    0.06198    0.05991    0.06232 
       360    0.06191    0.05990    0.06243 
       361    0.06181    0.05991    0.06235 significance (train): 1.606 significance: 1.472 
       362    0.06180    0.05989    0.06237 
       363    0.06187    0.05996    0.06230 
       364    0.06188    0.06011    0.06250 
       365    0.06200    0.05995    0.06248 
       366    0.06196    0.05990    0.06237 
       367    0.06179    0.05993    0.06233 
       368    0.06190    0.05979    0.06249 
       369    0.06195    0.05986    0.06234 
       370    0.06194    0.05994    0.06236 
       371    0.06194    0.05982    0.06238 
       372    0.06181    0.05997    0.06240 
       373    0.06185    0.05984    0.06238 
       374    0.06183    0.05986    0.06235 
       375    0.06197    0.05985    0.06244 
       376    0.06199    0.05994    0.06236 
       377    0.06187    0.05982    0.06248 
       378    0.06191    0.05978    0.06242 
       379    0.06194    0.05981    0.06236 
       380    0.06186    0.05982    0.06243 
       381    0.06183    0.05984    0.06246 significance (train): 1.607 significance: 1.458 
       382    0.06190    0.05998    0.06239 
       383    0.06179    0.05988    0.06242 
       384    0.06185    0.06013    0.06244 
       385    0.06192    0.05984    0.06238 
       386    0.06184    0.05981    0.06243 
       387    0.06177    0.05982    0.06239 
       388    0.06189    0.05985    0.06239 
       389    0.06172    0.05990    0.06232 
       390    0.06189    0.05982    0.06235 
       391    0.06182    0.05981    0.06243 
       392    0.06170    0.05983    0.06235 
       393    0.06187    0.05995    0.06236 
       394    0.06195    0.05986    0.06233 
       395    0.06188    0.05977    0.06239 
       396    0.06183    0.05984    0.06244 
       397    0.06186    0.05985    0.06239 
       398    0.06187    0.05985    0.06238 
       399    0.06190    0.05984    0.06236 
       400    0.06196    0.06015    0.06243 
set learning rate to: 0.02
       401    0.06176    0.05991    0.06238 significance (train): 1.618 significance: 1.477 
       402    0.06178    0.05982    0.06234 
       403    0.06172    0.05982    0.06236 
       404    0.06183    0.05982    0.06240 
       405    0.06182    0.05981    0.06239 
       406    0.06182    0.05978    0.06238 
       407    0.06190    0.05985    0.06236 
       408    0.06188    0.05982    0.06237 
       409    0.06188    0.05981    0.06234 
       410    0.06178    0.05981    0.06239 
       411    0.06166    0.05983    0.06246 
       412    0.06179    0.05979    0.06239 
       413    0.06184    0.05981    0.06234 
       414    0.06173    0.05979    0.06243 
       415    0.06182    0.05985    0.06240 
       416    0.06178    0.05974    0.06240 
       417    0.06182    0.05975    0.06237 
       418    0.06179    0.05977    0.06243 
       419    0.06186    0.05975    0.06238 
       420    0.06179    0.05978    0.06236 
       421    0.06175    0.05981    0.06239 significance (train): 1.617 significance: 1.481 
       422    0.06171    0.05977    0.06238 
       423    0.06177    0.05980    0.06237 
       424    0.06168    0.05976    0.06239 
       425    0.06175    0.05975    0.06241 
       426    0.06186    0.05986    0.06239 
       427    0.06176    0.05974    0.06243 
       428    0.06183    0.05975    0.06240 
       429    0.06184    0.05979    0.06237 
       430    0.06170    0.05973    0.06241 
       431    0.06177    0.05977    0.06236 
       432    0.06184    0.05993    0.06241 
       433    0.06167    0.05974    0.06239 
       434    0.06183    0.05976    0.06238 
       435    0.06176    0.05977    0.06240 
       436    0.06169    0.05985    0.06235 
       437    0.06185    0.05977    0.06236 
       438    0.06192    0.05979    0.06237 
       439    0.06178    0.05978    0.06237 
       440    0.06171    0.05981    0.06238 
       441    0.06181    0.05972    0.06243 significance (train): 1.612 significance: 1.458 
       442    0.06174    0.05980    0.06238 
       443    0.06177    0.05974    0.06240 
       444    0.06165    0.05988    0.06245 
       445    0.06179    0.05975    0.06234 
       446    0.06171    0.05977    0.06233 
       447    0.06176    0.05975    0.06240 
       448    0.06171    0.05975    0.06238 
       449    0.06178    0.05978    0.06239 
       450    0.06188    0.05986    0.06237 
       451    0.06168    0.05972    0.06240 
       452    0.06171    0.05973    0.06244 
       453    0.06172    0.05979    0.06240 
       454    0.06172    0.05976    0.06234 
       455    0.06177    0.05973    0.06240 
       456    0.06178    0.05972    0.06238 
       457    0.06168    0.05972    0.06239 
       458    0.06181    0.05978    0.06235 
       459    0.06178    0.05984    0.06233 
       460    0.06176    0.05981    0.06238 
       461    0.06180    0.05979    0.06240 significance (train): 1.625 significance: 1.480 
       462    0.06175    0.05982    0.06241 
       463    0.06184    0.05973    0.06236 
       464    0.06185    0.05984    0.06237 
       465    0.06170    0.05977    0.06242 
       466    0.06181    0.05975    0.06233 
       467    0.06165    0.05975    0.06238 
       468    0.06160    0.05971    0.06237 
       469    0.06171    0.05973    0.06240 
       470    0.06170    0.05978    0.06241 
       471    0.06176    0.05980    0.06237 
       472    0.06179    0.05975    0.06237 
       473    0.06174    0.05970    0.06238 
       474    0.06183    0.05972    0.06237 
       475    0.06168    0.05975    0.06236 
       476    0.06175    0.05974    0.06246 
       477    0.06169    0.05975    0.06240 
       478    0.06169    0.05972    0.06240 
       479    0.06165    0.05974    0.06239 
       480    0.06152    0.05967    0.06242 
       481    0.06171    0.05978    0.06237 significance (train): 1.620 significance: 1.480 
       482    0.06180    0.05969    0.06241 
       483    0.06181    0.05972    0.06238 
       484    0.06180    0.05976    0.06236 
       485    0.06168    0.05974    0.06245 
       486    0.06168    0.05968    0.06241 
       487    0.06185    0.05979    0.06239 
       488    0.06174    0.05984    0.06241 
       489    0.06174    0.05974    0.06239 
       490    0.06172    0.05977    0.06238 
       491    0.06184    0.05982    0.06239 
       492    0.06169    0.05971    0.06237 
       493    0.06171    0.05970    0.06236 
       494    0.06181    0.05980    0.06234 
       495    0.06176    0.05978    0.06239 
       496    0.06160    0.05980    0.06238 
       497    0.06169    0.05972    0.06239 
       498    0.06176    0.05979    0.06240 
       499    0.06188    0.05974    0.06234 
       500    0.06173    0.05969    0.06235 
set learning rate to: 0.01
       501    0.06171    0.05971    0.06239 significance (train): 1.613 significance: 1.470 
       502    0.06181    0.05972    0.06237 
       503    0.06173    0.05969    0.06238 
       504    0.06180    0.05977    0.06241 
       505    0.06164    0.05970    0.06238 
       506    0.06167    0.05973    0.06239 
       507    0.06177    0.05972    0.06238 
       508    0.06174    0.05967    0.06239 
       509    0.06182    0.05975    0.06238 
       510    0.06162    0.05974    0.06238 
       511    0.06163    0.05967    0.06238 
       512    0.06172    0.05975    0.06239 
       513    0.06165    0.05968    0.06238 
       514    0.06157    0.05969    0.06241 
       515    0.06165    0.05976    0.06240 
       516    0.06161    0.05975    0.06240 
       517    0.06177    0.05979    0.06240 
       518    0.06162    0.05976    0.06242 
       519    0.06173    0.05971    0.06238 
       520    0.06155    0.05970    0.06238 
       521    0.06165    0.05970    0.06241 significance (train): 1.611 significance: 1.473 
       522    0.06180    0.05978    0.06237 
       523    0.06170    0.05973    0.06238 
       524    0.06180    0.05979    0.06239 
       525    0.06167    0.05971    0.06238 
       526    0.06176    0.05969    0.06239 
       527    0.06166    0.05970    0.06239 
       528    0.06184    0.05971    0.06238 
       529    0.06176    0.05973    0.06238 
       530    0.06183    0.05972    0.06235 
       531    0.06159    0.05970    0.06239 
       532    0.06155    0.05973    0.06240 
       533    0.06170    0.05972    0.06237 
       534    0.06162    0.05972    0.06239 
       535    0.06178    0.05971    0.06240 
       536    0.06164    0.05973    0.06241 
       537    0.06160    0.05969    0.06238 
       538    0.06168    0.05970    0.06239 
       539    0.06179    0.05969    0.06240 
       540    0.06168    0.05975    0.06240 
       541    0.06161    0.05969    0.06241 significance (train): 1.615 significance: 1.468 
       542    0.06175    0.05968    0.06239 
       543    0.06165    0.05965    0.06240 
       544    0.06178    0.05971    0.06241 
       545    0.06175    0.05969    0.06243 
       546    0.06167    0.05969    0.06242 
       547    0.06165    0.05967    0.06239 
       548    0.06164    0.05968    0.06240 
       549    0.06186    0.05974    0.06236 
       550    0.06170    0.05972    0.06237 
       551    0.06179    0.05972    0.06237 
       552    0.06178    0.05970    0.06239 
       553    0.06176    0.05973    0.06235 
       554    0.06164    0.05967    0.06239 
       555    0.06172    0.05973    0.06240 
       556    0.06165    0.05968    0.06239 
       557    0.06161    0.05972    0.06238 
       558    0.06174    0.05964    0.06241 
       559    0.06167    0.05968    0.06242 
       560    0.06171    0.05970    0.06242 
       561    0.06174    0.05969    0.06238 significance (train): 1.615 significance: 1.472 
       562    0.06156    0.05965    0.06241 
       563    0.06165    0.05970    0.06243 
       564    0.06172    0.05969    0.06240 
       565    0.06166    0.05967    0.06237 
       566    0.06170    0.05970    0.06239 
       567    0.06161    0.05967    0.06240 
       568    0.06164    0.05970    0.06238 
       569    0.06177    0.05970    0.06237 
       570    0.06175    0.05968    0.06242 
       571    0.06167    0.05965    0.06241 
       572    0.06176    0.05974    0.06240 
       573    0.06164    0.05966    0.06241 
       574    0.06173    0.05969    0.06240 
       575    0.06176    0.05969    0.06237 
       576    0.06176    0.05971    0.06236 
       577    0.06167    0.05970    0.06240 
       578    0.06178    0.05972    0.06236 
       579    0.06175    0.05971    0.06239 
       580    0.06171    0.05977    0.06238 
       581    0.06171    0.05966    0.06236 significance (train): 1.617 significance: 1.460 
       582    0.06177    0.05970    0.06237 
       583    0.06159    0.05969    0.06238 
       584    0.06168    0.05968    0.06237 
       585    0.06164    0.05970    0.06240 
       586    0.06172    0.05967    0.06239 
       587    0.06187    0.05972    0.06238 
       588    0.06175    0.05973    0.06238 
       589    0.06158    0.05968    0.06239 
       590    0.06163    0.05968    0.06245 
       591    0.06163    0.05973    0.06240 
       592    0.06178    0.05970    0.06238 
       593    0.06164    0.05972    0.06241 
       594    0.06175    0.05975    0.06240 
       595    0.06174    0.05966    0.06240 
       596    0.06173    0.05975    0.06239 
       597    0.06171    0.05973    0.06238 
       598    0.06165    0.05969    0.06241 
       599    0.06170    0.05971    0.06238 
       600    0.06160    0.05968    0.06240 
set learning rate to: 0.005
       601    0.06177    0.05969    0.06238 significance (train): 1.613 significance: 1.475 
       602    0.06176    0.05970    0.06237 
       603    0.06170    0.05969    0.06240 
       604    0.06161    0.05969    0.06241 
       605    0.06165    0.05970    0.06239 
       606    0.06159    0.05971    0.06239 
       607    0.06167    0.05969    0.06241 
       608    0.06173    0.05969    0.06239 
       609    0.06164    0.05966    0.06240 
       610    0.06159    0.05964    0.06241 
       611    0.06180    0.05970    0.06240 
       612    0.06175    0.05968    0.06241 
       613    0.06173    0.05966    0.06238 
       614    0.06160    0.05968    0.06239 
       615    0.06167    0.05969    0.06240 
       616    0.06171    0.05967    0.06240 
       617    0.06154    0.05967    0.06240 
       618    0.06157    0.05966    0.06239 
       619    0.06171    0.05966    0.06239 
       620    0.06171    0.05968    0.06238 
       621    0.06165    0.05968    0.06239 significance (train): 1.617 significance: 1.474 
       622    0.06169    0.05969    0.06238 
       623    0.06173    0.05969    0.06240 
       624    0.06162    0.05968    0.06239 
       625    0.06164    0.05967    0.06240 
       626    0.06159    0.05970    0.06241 
       627    0.06177    0.05970    0.06239 
       628    0.06164    0.05966    0.06239 
       629    0.06169    0.05970    0.06238 
       630    0.06172    0.05968    0.06239 
       631    0.06164    0.05967    0.06240 
       632    0.06171    0.05968    0.06240 
       633    0.06165    0.05965    0.06242 
       634    0.06168    0.05969    0.06240 
       635    0.06173    0.05967    0.06239 
       636    0.06168    0.05968    0.06239 
       637    0.06169    0.05973    0.06238 
       638    0.06180    0.05969    0.06240 
       639    0.06157    0.05967    0.06240 
       640    0.06182    0.05970    0.06237 
       641    0.06165    0.05970    0.06239 significance (train): 1.617 significance: 1.478 
       642    0.06153    0.05965    0.06241 
       643    0.06159    0.05968    0.06241 
       644    0.06161    0.05966    0.06240 
       645    0.06158    0.05968    0.06239 
       646    0.06172    0.05964    0.06241 
       647    0.06174    0.05964    0.06240 
       648    0.06173    0.05970    0.06239 
       649    0.06173    0.05969    0.06241 
       650    0.06157    0.05967    0.06240 
       651    0.06168    0.05968    0.06240 
       652    0.06161    0.05964    0.06243 
       653    0.06171    0.05967    0.06239 
       654    0.06158    0.05967    0.06238 
       655    0.06173    0.05966    0.06239 
       656    0.06160    0.05967    0.06239 
       657    0.06166    0.05965    0.06239 
       658    0.06161    0.05964    0.06240 
       659    0.06166    0.05967    0.06240 
       660    0.06171    0.05966    0.06239 
       661    0.06153    0.05966    0.06241 significance (train): 1.616 significance: 1.469 
       662    0.06153    0.05966    0.06240 
       663    0.06177    0.05966    0.06239 
       664    0.06175    0.05971    0.06241 
       665    0.06173    0.05965    0.06241 
       666    0.06175    0.05966    0.06239 
       667    0.06167    0.05966    0.06239 
       668    0.06174    0.05965    0.06239 
       669    0.06160    0.05965    0.06241 
       670    0.06168    0.05968    0.06240 
       671    0.06163    0.05966    0.06239 
       672    0.06155    0.05969    0.06239 
       673    0.06170    0.05967    0.06241 
       674    0.06169    0.05965    0.06241 
       675    0.06159    0.05964    0.06241 
       676    0.06174    0.05966    0.06238 
       677    0.06185    0.05969    0.06239 
       678    0.06166    0.05965    0.06239 
       679    0.06164    0.05967    0.06239 
       680    0.06160    0.05967    0.06239 
       681    0.06173    0.05970    0.06240 significance (train): 1.617 significance: 1.478 
       682    0.06164    0.05969    0.06239 
       683    0.06163    0.05967    0.06238 
       684    0.06166    0.05967    0.06242 
       685    0.06161    0.05965    0.06241 
       686    0.06170    0.05965    0.06239 
       687    0.06160    0.05965    0.06240 
       688    0.06154    0.05964    0.06241 
       689    0.06170    0.05968    0.06242 
       690    0.06174    0.05968    0.06240 
       691    0.06168    0.05966    0.06239 
       692    0.06165    0.05968    0.06238 
       693    0.06172    0.05971    0.06239 
       694    0.06164    0.05968    0.06239 
       695    0.06173    0.05968    0.06240 
       696    0.06169    0.05967    0.06238 
       697    0.06161    0.05966    0.06240 
       698    0.06149    0.05965    0.06242 
       699    0.06162    0.05964    0.06241 
       700    0.06170    0.05970    0.06240 
set learning rate to: 0.002
       701    0.06154    0.05967    0.06241 significance (train): 1.615 significance: 1.474 
       702    0.06172    0.05967    0.06240 
       703    0.06167    0.05966    0.06239 
       704    0.06168    0.05967    0.06240 
       705    0.06161    0.05967    0.06241 
       706    0.06161    0.05966    0.06239 
       707    0.06159    0.05967    0.06240 
       708    0.06163    0.05969    0.06241 
       709    0.06172    0.05967    0.06240 
       710    0.06171    0.05966    0.06240 
       711    0.06178    0.05967    0.06238 
       712    0.06172    0.05966    0.06239 
       713    0.06167    0.05968    0.06240 
       714    0.06158    0.05965    0.06241 
       715    0.06167    0.05966    0.06239 
       716    0.06165    0.05967    0.06241 
       717    0.06171    0.05969    0.06239 
       718    0.06167    0.05967    0.06238 
       719    0.06172    0.05968    0.06240 
       720    0.06169    0.05967    0.06239 
       721    0.06178    0.05967    0.06238 significance (train): 1.617 significance: 1.474 
       722    0.06166    0.05966    0.06238 
       723    0.06163    0.05967    0.06239 
       724    0.06167    0.05965    0.06239 
       725    0.06163    0.05965    0.06239 
       726    0.06159    0.05965    0.06240 
       727    0.06165    0.05967    0.06241 
       728    0.06163    0.05966    0.06241 
       729    0.06173    0.05968    0.06240 
       730    0.06160    0.05967    0.06239 
       731    0.06167    0.05966    0.06240 
       732    0.06169    0.05967    0.06239 
       733    0.06167    0.05966    0.06239 
       734    0.06165    0.05967    0.06240 
       735    0.06161    0.05967    0.06240 
       736    0.06161    0.05967    0.06241 
       737    0.06167    0.05965    0.06239 
       738    0.06169    0.05965    0.06240 
       739    0.06166    0.05967    0.06239 
       740    0.06173    0.05966    0.06240 
       741    0.06179    0.05967    0.06239 significance (train): 1.617 significance: 1.475 
       742    0.06176    0.05968    0.06239 
       743    0.06176    0.05967    0.06239 
       744    0.06168    0.05966    0.06238 
       745    0.06161    0.05966    0.06239 
       746    0.06172    0.05968    0.06239 
       747    0.06171    0.05965    0.06239 
       748    0.06175    0.05967    0.06239 
       749    0.06162    0.05965    0.06238 
       750    0.06169    0.05966    0.06238 
       751    0.06172    0.05967    0.06239 
       752    0.06168    0.05967    0.06238 
       753    0.06155    0.05965    0.06239 
       754    0.06168    0.05966    0.06239 
       755    0.06163    0.05966    0.06240 
       756    0.06175    0.05967    0.06239 
       757    0.06168    0.05968    0.06240 
       758    0.06161    0.05967    0.06240 
       759    0.06173    0.05966    0.06238 
       760    0.06162    0.05965    0.06240 
       761    0.06177    0.05967    0.06239 significance (train): 1.617 significance: 1.475 
       762    0.06156    0.05968    0.06239 
       763    0.06170    0.05966    0.06239 
       764    0.06162    0.05966    0.06240 
       765    0.06157    0.05965    0.06241 
       766    0.06177    0.05966    0.06241 
       767    0.06168    0.05967    0.06240 
       768    0.06160    0.05966    0.06240 
       769    0.06165    0.05966    0.06240 
       770    0.06175    0.05966    0.06240 
       771    0.06162    0.05966    0.06239 
       772    0.06165    0.05965    0.06239 
       773    0.06176    0.05967    0.06239 
       774    0.06155    0.05966    0.06241 
       775    0.06160    0.05965    0.06241 
       776    0.06162    0.05964    0.06240 
       777    0.06161    0.05967    0.06242 
       778    0.06163    0.05966    0.06241 
       779    0.06165    0.05966    0.06239 
       780    0.06167    0.05966    0.06240 
       781    0.06158    0.05964    0.06240 significance (train): 1.618 significance: 1.475 
       782    0.06165    0.05966    0.06240 
       783    0.06158    0.05964    0.06242 
       784    0.06162    0.05964    0.06240 
       785    0.06161    0.05964    0.06241 
       786    0.06177    0.05966    0.06241 
       787    0.06155    0.05964    0.06241 
       788    0.06160    0.05966    0.06241 
       789    0.06169    0.05967    0.06240 
       790    0.06163    0.05963    0.06239 
       791    0.06174    0.05964    0.06239 
       792    0.06174    0.05965    0.06239 
       793    0.06164    0.05965    0.06239 
       794    0.06176    0.05966    0.06240 
       795    0.06181    0.05966    0.06239 
       796    0.06174    0.05966    0.06239 
       797    0.06159    0.05965    0.06239 
       798    0.06171    0.05967    0.06238 
       799    0.06163    0.05965    0.06240 
       800    0.06169    0.05967    0.06239 
set learning rate to: 0.001
       801    0.06159    0.05968    0.06239 significance (train): 1.619 significance: 1.473 
       802    0.06162    0.05966    0.06239 
       803    0.06163    0.05965    0.06240 
       804    0.06169    0.05966    0.06239 
       805    0.06148    0.05966    0.06241 
       806    0.06166    0.05965    0.06239 
       807    0.06168    0.05966    0.06241 
       808    0.06175    0.05966    0.06240 
       809    0.06164    0.05966    0.06239 
       810    0.06178    0.05966    0.06239 
       811    0.06162    0.05966    0.06238 
       812    0.06160    0.05967    0.06240 
       813    0.06171    0.05967    0.06240 
       814    0.06159    0.05965    0.06240 
       815    0.06177    0.05966    0.06240 
       816    0.06168    0.05966    0.06241 
       817    0.06163    0.05965    0.06240 
       818    0.06161    0.05965    0.06240 
       819    0.06149    0.05966    0.06240 
       820    0.06177    0.05966    0.06240 
       821    0.06160    0.05966    0.06239 significance (train): 1.614 significance: 1.475 
       822    0.06169    0.05966    0.06239 
       823    0.06155    0.05966    0.06239 
       824    0.06157    0.05964    0.06240 
       825    0.06163    0.05966    0.06240 
       826    0.06167    0.05965    0.06241 
       827    0.06167    0.05965    0.06240 
       828    0.06159    0.05965    0.06240 
       829    0.06184    0.05965    0.06239 
       830    0.06167    0.05966    0.06239 
       831    0.06161    0.05965    0.06241 
       832    0.06175    0.05965    0.06239 
       833    0.06171    0.05966    0.06240 
       834    0.06178    0.05966    0.06238 
       835    0.06175    0.05966    0.06239 
       836    0.06177    0.05966    0.06240 
       837    0.06165    0.05965    0.06239 
       838    0.06169    0.05964    0.06239 
       839    0.06176    0.05964    0.06239 
       840    0.06152    0.05966    0.06240 
       841    0.06159    0.05965    0.06240 significance (train): 1.617 significance: 1.473 
       842    0.06153    0.05966    0.06240 
       843    0.06162    0.05965    0.06239 
       844    0.06163    0.05966    0.06240 
       845    0.06172    0.05966    0.06239 
       846    0.06169    0.05964    0.06239 
       847    0.06163    0.05965    0.06238 
       848    0.06164    0.05965    0.06239 
       849    0.06162    0.05966    0.06240 
       850    0.06175    0.05965    0.06238 
       851    0.06170    0.05966    0.06240 
       852    0.06159    0.05966    0.06239 
       853    0.06176    0.05967    0.06239 
       854    0.06161    0.05965    0.06240 
       855    0.06165    0.05966    0.06240 
       856    0.06162    0.05966    0.06238 
       857    0.06163    0.05966    0.06239 
       858    0.06166    0.05967    0.06239 
       859    0.06161    0.05964    0.06239 
       860    0.06162    0.05966    0.06240 
       861    0.06165    0.05965    0.06240 significance (train): 1.616 significance: 1.474 
       862    0.06165    0.05966    0.06240 
       863    0.06166    0.05966    0.06241 
       864    0.06165    0.05965    0.06239 
       865    0.06164    0.05965    0.06239 
       866    0.06156    0.05965    0.06240 
       867    0.06168    0.05966    0.06240 
       868    0.06165    0.05966    0.06239 
       869    0.06168    0.05965    0.06240 
       870    0.06175    0.05966    0.06239 
       871    0.06164    0.05964    0.06239 
       872    0.06157    0.05965    0.06239 
       873    0.06162    0.05965    0.06240 
       874    0.06161    0.05965    0.06240 
       875    0.06176    0.05964    0.06240 
       876    0.06177    0.05966    0.06240 
       877    0.06160    0.05965    0.06238 
       878    0.06162    0.05965    0.06240 
       879    0.06164    0.05966    0.06240 
       880    0.06157    0.05965    0.06240 
       881    0.06176    0.05966    0.06239 significance (train): 1.621 significance: 1.474 
       882    0.06159    0.05965    0.06239 
       883    0.06165    0.05967    0.06240 
       884    0.06169    0.05967    0.06241 
       885    0.06163    0.05964    0.06240 
       886    0.06173    0.05965    0.06240 
       887    0.06173    0.05967    0.06240 
       888    0.06168    0.05965    0.06239 
       889    0.06171    0.05966    0.06240 
       890    0.06176    0.05967    0.06240 
       891    0.06166    0.05966    0.06239 
       892    0.06162    0.05964    0.06239 
       893    0.06162    0.05965    0.06240 
       894    0.06165    0.05965    0.06239 
       895    0.06171    0.05965    0.06239 
       896    0.06168    0.05964    0.06239 
       897    0.06165    0.05967    0.06240 
       898    0.06154    0.05966    0.06239 
       899    0.06177    0.05966    0.06239 
       900    0.06155    0.05965    0.06239 
       901    0.06169    0.05965    0.06241 significance (train): 1.617 significance: 1.470 
       902    0.06159    0.05965    0.06240 
       903    0.06155    0.05965    0.06240 
       904    0.06166    0.05966    0.06240 
       905    0.06172    0.05966    0.06239 
       906    0.06159    0.05965    0.06240 
       907    0.06161    0.05965    0.06240 
       908    0.06156    0.05966    0.06241 
       909    0.06171    0.05964    0.06240 
       910    0.06155    0.05965    0.06240 
       911    0.06163    0.05965    0.06240 
       912    0.06164    0.05966    0.06240 
       913    0.06161    0.05965    0.06240 
       914    0.06176    0.05966    0.06240 
       915    0.06155    0.05964    0.06240 
       916    0.06164    0.05965    0.06240 
       917    0.06167    0.05965    0.06240 
       918    0.06171    0.05964    0.06240 
       919    0.06167    0.05966    0.06239 
       920    0.06175    0.05966    0.06239 
       921    0.06169    0.05965    0.06239 significance (train): 1.618 significance: 1.473 
       922    0.06168    0.05965    0.06239 
       923    0.06163    0.05965    0.06239 
       924    0.06180    0.05965    0.06239 
       925    0.06168    0.05965    0.06240 
       926    0.06171    0.05965    0.06239 
       927    0.06150    0.05965    0.06240 
       928    0.06153    0.05964    0.06240 
       929    0.06160    0.05965    0.06240 
       930    0.06151    0.05964    0.06240 
       931    0.06162    0.05964    0.06240 
       932    0.06162    0.05964    0.06241 
       933    0.06164    0.05964    0.06240 
       934    0.06160    0.05965    0.06240 
       935    0.06166    0.05965    0.06241 
       936    0.06173    0.05966    0.06239 
       937    0.06163    0.05965    0.06240 
       938    0.06169    0.05964    0.06239 
       939    0.06168    0.05966    0.06240 
       940    0.06164    0.05966    0.06241 
       941    0.06178    0.05965    0.06241 significance (train): 1.618 significance: 1.473 
       942    0.06172    0.05965    0.06239 
       943    0.06162    0.05965    0.06240 
       944    0.06173    0.05966    0.06241 
       945    0.06163    0.05965    0.06239 
       946    0.06156    0.05964    0.06239 
       947    0.06172    0.05966    0.06240 
       948    0.06167    0.05964    0.06240 
       949    0.06165    0.05966    0.06241 
       950    0.06166    0.05966    0.06239 
       951    0.06171    0.05966    0.06239 
       952    0.06172    0.05966    0.06240 
       953    0.06172    0.05965    0.06239 
       954    0.06165    0.05965    0.06239 
       955    0.06155    0.05965    0.06239 
       956    0.06167    0.05965    0.06240 
       957    0.06174    0.05966    0.06240 
       958    0.06174    0.05965    0.06240 
       959    0.06175    0.05967    0.06240 
       960    0.06173    0.05966    0.06238 
       961    0.06170    0.05965    0.06240 significance (train): 1.618 significance: 1.474 
       962    0.06168    0.05966    0.06240 
       963    0.06165    0.05965    0.06240 
       964    0.06173    0.05965    0.06239 
       965    0.06166    0.05965    0.06239 
       966    0.06172    0.05965    0.06239 
       967    0.06168    0.05965    0.06239 
       968    0.06164    0.05964    0.06238 
       969    0.06163    0.05964    0.06241 
       970    0.06168    0.05965    0.06240 
       971    0.06174    0.05964    0.06239 
       972    0.06159    0.05964    0.06238 
       973    0.06169    0.05964    0.06239 
       974    0.06177    0.05966    0.06239 
       975    0.06162    0.05965    0.06239 
       976    0.06180    0.05967    0.06239 
       977    0.06174    0.05966    0.06238 
       978    0.06156    0.05965    0.06240 
       979    0.06167    0.05964    0.06238 
       980    0.06164    0.05965    0.06240 
       981    0.06166    0.05964    0.06240 significance (train): 1.615 significance: 1.475 
       982    0.06159    0.05966    0.06240 
       983    0.06161    0.05965    0.06239 
       984    0.06158    0.05965    0.06241 
       985    0.06175    0.05965    0.06240 
       986    0.06180    0.05965    0.06240 
       987    0.06168    0.05965    0.06239 
       988    0.06167    0.05964    0.06239 
       989    0.06168    0.05966    0.06239 
       990    0.06172    0.05966    0.06240 
       991    0.06167    0.05965    0.06239 
       992    0.06155    0.05965    0.06239 
       993    0.06184    0.05967    0.06240 
       994    0.06165    0.05966    0.06240 
       995    0.06176    0.05966    0.06239 
       996    0.06164    0.05965    0.06239 
       997    0.06149    0.05964    0.06239 
       998    0.06164    0.05966    0.06240 
       999    0.06167    0.05963    0.06239 
      1000    0.06168    0.05964    0.06240 significance (train): 1.617 significance: 1.475 
FINAL RESULTS:       1000   0.061682   0.062397 significance (train): 1.617 significance: 1.475 
TRAINING TIME: 0:49:10.400676 (2950.4 seconds)
GRADIENT UPDATES: 330000
MIN TEST LOSS: 0.06228018524164628
training done.
> results//FINAL_VHLegacy_2lep_WP_SR_low_Zll/Zll2017_SR_low_Zll_191022_V11finalVarsWP.h5/512-256-128-64-64-64/0.20-0.50-0.50-0.50-0.50-0.50/1.000e+00/rnd_1/checkpoints/model.ckpt
saved checkpoint to [34m results//FINAL_VHLegacy_2lep_WP_SR_low_Zll/Zll2017_SR_low_Zll_191022_V11finalVarsWP.h5/512-256-128-64-64-64/0.20-0.50-0.50-0.50-0.50-0.50/1.000e+00/rnd_1/checkpoints/model.ckpt [0m
LOSS(train, unmodified):  0.05963856276663485
LOSS(test):               0.062396501817325804
---
S    B
---
 0.45 4540.41
 1.21 2610.65
 1.77 2210.21
 2.33 1946.28
 2.96 1629.36
 3.49 1410.70
 3.97 1279.96
 4.89 1253.35
 6.27 1174.82
 6.64 970.18
 8.30 958.82
11.05 916.75
14.37 746.75
17.03 470.87
10.54 112.41
---
significance: 1.475 
area under ROC: AUC_test =  85.16153075990897
area under ROC: AUC_train =  87.07472764177335
INFO: set range to: 0.7239665 817.80383
INFO: set range to: 90.00024 149.99919
INFO: set range to: 0.13183 1646.4913
INFO: set range to: 0.25703242 1638.5796
INFO: set range to: 0.00015607555 6.206897
INFO: set range to: 5.0920993e-05 3.141592
INFO: set range to: 2.0 3.0
INFO: set range to: 1.0 3.0
INFO: set range to: 0.0035906518 1380.5991
INFO: set range to: 20.000076 1192.3236
INFO: set range to: 0.0018067261 1617.5891
INFO: set range to: 20.000605 1609.3499
INFO: set range to: 71.49592 112.76267
INFO: set range to: 75.00003 104.99966
INFO: set range to: 0.0 9.0
INFO: set range to: 32.166363 201.42319
INFO: set range to: 50.000095 149.9997
INFO: set range to: 0.0021407274 18.46004
INFO: set range to: 0.0046184547 18.159197
INFO: set range to: 0.0 3.8371582
INFO: set range to: -1.0 15.0
INFO: set range to: 0.007676397 8.0666895
INFO: set range to: 0.0071152686 7.716344
INFO: set range to: 0.10550891 749.117
INFO: set range to: -1.0 581.7251
INFO: set range to: -1.0 9.0
INFO: set range to: 0.387904 4.7402053
-------------------------
with optimized binning:
 method: SB
 target: 0.1090, 0.1194, 0.1242, 0.1226, 0.1148, 0.1021, 0.0861, 0.0690, 0.0524, 0.0378, 0.0259, 0.0168, 0.0104, 0.0061, 0.0034
 bins:   0.0000, 0.0210, 0.0794, 0.1542, 0.2418, 0.3434, 0.4566, 0.5572, 0.6569, 0.7367, 0.7988, 0.8463, 0.8858, 0.9172, 0.9468, 1.0000
-------------------------
---
S    B
---
 0.09 2433.35
 0.52 2665.37
 1.58 2771.50
 2.66 2735.19
 4.37 2558.21
 6.32 2273.92
 7.78 1914.51
 9.59 1531.08
 9.88 1160.10
10.32 833.92
 9.74 568.59
 9.65 365.59
 8.12 224.15
 7.34 129.28
 7.31 66.73
---
significance: 1.490 (for optimized binning)
significance: 1.466 ( 1% background uncertainty, for optimized binning)
significance: 1.198 ( 5% background uncertainty, for optimized binning)
significance: 0.910 (10% background uncertainty, for optimized binning)
significance: 0.710 (15% background uncertainty, for optimized binning)
significance: 0.574 (20% background uncertainty, for optimized binning)
[32mPLOTS: use n=S+B Asimov data in the plots![0m
[32mPLOTS: use n=S+B Asimov data in the plots![0m
[32mPLOTS: use n=S+B Asimov data in the plots![0m
INFO: search optimal cut position for sensitivity
optimal position for analysis based on single cut on score > x:
AMS_cut_position = 0.818008542060852
AMC_cut_significance = 1.1569329406533209
INFO: convert to histogram
