saving logfile to [34m results//FINAL_VHLegacy_0lep_WP_SR_medhigh_Znn/Zvv2017_SR_medhigh_Znn_191022_V11finalVarsWP.h5/512-256-128-64-64-64/0.20-0.50-0.50-0.50-0.50-0.50/1.000e+00/rnd_1/output.txt [0m
INFO: numpy random state =  MT19937 ,aafd4cf5,f3861cba,e24700df,16f46d50,9939e9d5,6e66191d,f80aa69d,2f13d3ae,17328343,bfcff269,f5cea6b7,c4eb201,a44b968c,da532256,aa7a66ed,933bef60,ea9aea9a,bf48e46,b7a35150,9d90bf4,995944ef,12ba9411,6e061c06,511c78c2,47d4286a,a6407d2e,4ef14c54,ee2407f0,72a55778,60ca9602,d9dc4c5,68b91c21,9a72ef15,6a203bd1,97e295e0,b02f063d,7f9d2be,eb92e8bb,ec2203ca,11a234f4,74ca44db,5f4ae399,92308352,447fee17,26bafe94,cf75259f,3ffda03b,631ab9bc,ca6c0f05,1a35d867,9f40694a,7472d96b,9ce370fb,f350fbd5,8cc91b3b,d3b19547,e42840e5,47f2121,453eaaea,7b6c18dd,38f9e13d,904c052a,df6e5aea,c3684be7,3c8319c0,9d6d1093,d70c006,ac1d162f,d3c27341,1b8dda05,354a021d,b7d96d51,d4c8e72b,5d4b2f38,c1245c5,fdd54353,c0c425f5,63175dd2,df87074f,cad39a04,a44576c5,d3c9350,fe2d87ad,3b32b5e2,eb71afac,6037fcaa,a3a57b27,907d7c39,6b90499d,a1e601d2,94bf8ca,a4469631,94cb7528,b34063fd,59d34681,8c26071f,a82d81fd,c8d4e437,58305073,4c7e2855,c114b802,1a41e771,a12ee087,772b5807,9fcce3d9,3b34bdb9,ecaf0513,3789e886,4d520e3d,1d4b7b4d,d1747295,2982de8a,1355f11a,83a3cfb2,3f6c7fce,620b85e3,9b0b80aa,82da3d,3250310,d5c22360,b9cd025b,302e4df6,5d03e32b,e7eb8f4,33ae5a39,d2ff0905,b13fee0d,c13a10,826b29b8,39bb5018,8e2fc98f,37b9d81c,4c89ada6,9b4cf941,be4ac35d,3ab3ba42,7addcddc,c7988237,d10578cc,e9b6c38d,ca7a07c7,c18a598f,f59e5894,18429a88,5693e71a,d5e7822e,153ebfdf,f0e006fe,306f0d5e,5dd3dafc,6f9edb5c,2cebbf2d,b188f32,8fc2325c,3b29729a,2d1b3ec0,d010e322,add67076,8855f93,e7acf87,1d5545b,fdf1d6d9,80683d50,862538fe,7c182c64,35e5cdcb,d19d024d,d5a0ea0e,fdf0269f,6c7c6e69,3b50a025,b244d08f,96eb52c,f7657694,8d2bbbc0,d0667e14,80838ee9,b9477177,cfb2f3df,ba942d05,87feda6e,8736d46,5e23ac48,1845f347,405e9c05,227fbb42,45150ebd,dda4ee17,534f7c58,36779bda,cd0b5f24,81f32ce7,939266eb,158dd4e8,8720066d,27d2d07e,43828fe1,1f4b45da,d81eca96,6d17a550,c140b5ef,a3617d46,36e240f1,d1d41aea,5e95fa4d,527df4b6,3a684d05,3a8b81ed,22f70292,e8fe20f6,b7d6a5d4,ce0ef49d,9cd31ab5,2127e622,96ba0994,ad6e0141,ba73f1d0,cf5b7d10,6a3dc2c1,a744e416,b55554b4,c1feb29e,75f40d89,4a0b5321,9f630a85,386ef302,a8ef271e,159251d7,c4188362,506d6397,5bb153d9,d2b9ffc4,2809a3f4,1644e5bb,e97a74b1,89f461fe,96de3d05,51ceef7c,9dae336d,36fc1982,b537b35e,76e41095,610ab83c,c2680b1c,d224043e,d26e756a,48d74781,b866c0be,beed9c16,4c77d4c7,d41884f6,1423ad2d,b0f76c92,b7579db4,a124440c,325b6303,357574d8,a9d2065,a94ff800,a48451ad,c2d8397a,7fca555c,1aac90a5,2fc21f6e,1f01667a,23ffd935,4a95c915,3444cd3f,a0920dba,bc76d8ca,6d80b285,6f71dfa3,816dc616,a940604c,d55274ec,35029f7b,29ccce2e,e6cf648d,9fa596db,70e68cbe,1bce08dc,c8f63155,a39335ea,da5d881a,82d1ef8a,4e490562,3dfcdbf9,8d825e70,950839f4,a5d1c3e8,9285ffc8,3b26cf1b,70fcb641,6fc453d3,62517a5a,4c269a3c,f9b00cc6,7e80819f,bea24e5a,197a9175,89209b79,cf15c1bc,33d7c30c,cfe15a8,38d22848,f052665c,e31740f9,d729c53,63fb84c8,bdf604a0,357dd385,693b1f19,6758ecf6,fff156c0,6c0207a0,396ee782,f0d4d72,2f55a99d,96dd38b4,77d17378,b1e968a5,3ae0e9df,c68e02,ec9ab48f,7c504a91,87a85822,fabdb245,8247ec12,8e2b2a0d,a70462ef,703fff75,c8423d08,63dafec3,e996942b,8bc5a139,15fcccca,bfd787d5,b11389e6,4e7304c3,2cabb8f3,61772617,e926fc93,a852130b,c71bb6fc,4f4bab69,828d7546,72d80aac,b3a983f9,6f120205,63faf86f,f4b5517,3083a13f,6d3de53e,f35111c0,93ac558d,9d674e8,4e637e5e,3de793f2,1cc8d2b2,41ea4f09,57f3536f,3621b021,4a67e6b5,941e54cd,2f503894,d4a35a6e,4901aa78,bd9066d6,248eb33,51861d2f,29f7a093,dd11777a,e6b9cebe,b3f2c156,17052d69,a707e9e6,5ce3357e,7289995d,b9a2c2c3,9f40d0c,f6a3fd7d,d9ea509f,87e9f84b,343f41e9,5b0e2c47,7c66f3cd,d435ea39,3183df63,7d3b5c2,76ad7ad4,d874c991,b8377992,9b8cbca9,74e0cfbe,bf5e8781,aeff6938,55441ba7,8a864535,e9e5e31a,17faa92a,e14fa34,20e0eaeb,c5beee42,38abd785,12f96db2,4797d6bd,b1bf9a49,69af2a25,c57944d,fbd1d21f,86c782df,2d7dbd0d,a909534,25d2a665,60635585,475e237,8fbe79d,37babcaf,2c45c74d,47809ddd,e6954210,6ffc4382,956729fb,f80bf82f,e0470a57,adfd517a,11e92151,52eada8a,f7b7b4c9,f9329d60,a037f6fe,59163eaf,d73dc1fb,4d8a6185,f8d8fb,83c9d2c0,f9eeb3cb,3b65c26,e8dd99f7,e8872928,79e8c129,6013b6d,c9e9b2f8,f7663377,cf9762bf,50e91a63,fb6b571b,9f577073,1e7a42f0,fcf71d20,5606571a,12b71b9c,c6561d12,30240782,9f881fa,84788bc8,3e319533,3541d946,1775940f,653a0b30,3c99048e,62344e63,98bfbc62,c8edcd20,86370f14,e4cda39c,2f362053,b6b304d4,61c7d7e8,52f0acdf,8ed5b135,f39dd75b,529283d7,a53b67b7,a294f2e9,eff7f6f4,f89e548b,bfc2cb4f,1d704605,7871b3d1,e21f6324,aa312d62,1572642e,3bbf20ff,1accd8d8,e7e75f6,e453c946,ba76fd2c,99020fb6,33afe2ac,ee445199,7b103ecd,aae31f8c,683adc39,6f8e6e0e,6b8da7c7,e35b5445,23ccd1fc,7139693,cf670c95,d49168d3,863fa415,2467fbd3,1e28b2cd,9e03500e,3f0bf5bb,60608311,1a27e971,ab5db62f,ab95f1a4,5dca40b8,e1df8c64,df5232a1,47facc95,c3b97fba,339e8eb7,96f21060,b42881de,f064bfc8,2cef51a2,2a873712,a2d6d1fb,5731aed3,94234faf,91260337,1f72645e,5ab85f92,a5dae229,5253ee9c,2db2ea6a,b149bfa,ec8ba7a9,b7c6ee5a,d742819f,9f39d6a6,c7dd391a,cab32cd4,b2df142f,57f74488,663ebad5,6257808c,785b55b9,ea36831f,b9ccff2d,f62443e4,cd85cee2,974e5969,45ffb621,d8636848,a34039eb,d1070fd4,487bc7c,e0db732d,7131fe76,9294a18c,8cafbef8,b8cacefb,9c15e8e3,dc65c826,59eab83b,1bd30457,d4c27e25,7878a850,7226f9,f352bc6c,9da961a7,a6040e84,48da3e19,a3c3a8d6,69b517a9,ae997ea8,130740d6,e7b65f4b,91308561,46d41630,a563ee3c,f891ba91,c3872654,9e1972fb,70c58a29,13753b4c,e230c1a2,d982b864,92a24c82,501d6f13,5710511c,837a7c9c,7f8b3045,37d448ac,5142c4e1,da7b94e9,e9371314,5f8f80fb,529c073b,fca1701c,78b8492e,28f3d584,f77f286,8fe748c4,c5d333e,2ed0ffec,355fe096,8ab1ed09,83ddf800,400dec50,ab101653,769b4654,fcc8926b,d3cbb523,bc3e7b8d,8f0d58c,73994cc5,593846f7,496962a3,48c4f9c7,78cdeee7,5b7a9a0a,9713aa51,9b05f510,272cff0b,49240532,a44bb7d5,55f2d4d9,6d235b51
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------
initialized TensorflowDNNClassifier, version v0.1 ( 12006 )
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------
INFO: command: /var/spool/slurmd/job145590/slurm_script -i /work/berger_p2/VHbb/CMSSW_10_1_0/src/Xbb/python/dumps/Zvv2017_SR_medhigh_Znn_191022_V11finalVarsWP.h5 -c config/default_momentum.cfg -p FINAL_VHLegacy_0lep_WP_SR_medhigh_Znn
INFO: read inputs from disk, metadata is pesent:
INFO:  >   cut (isZnn && H_pt > 120 && abs(TVector2::Phi_mpi_pi(H_phi-MET_Phi)) > 2.0 && min(MHT_pt, MET_Pt) > 100 && Sum$(abs(TVector2::Phi_mpi_pi(Jet_phi-V_phi))<0.5&&Jet_Pt>30&&(Jet_puId>6||Jet_Pt>50)&&Jet_lepFilter)==0 && (H_mass > 90 && H_mass < 150) && Jet_btagDeepB[hJidx[0]] > 0.4941 && Jet_btagDeepB[hJidx[1]] > 0.1522 && abs(TVector2::Phi_mpi_pi(MET_Phi-TkMET_phi)) < 0.5 && Sum$(Jet_Pt>30&&abs(Jet_eta)<2.4&&(Jet_puId>6||Jet_Pt>50)&&Jet_lepFilter&&Iteration$!=hJidx[0]&&Iteration$!=hJidx[1]) < 2 && nAddLeptons==0)&&((MET_Pt >= 150.0))
INFO:  >   cutName SR_medhigh_Znn
INFO:  >   region SR_medhigh_Znn
INFO:  >   samples {'SIG_ALL': ['ZllH_lep_PTV_0_75_hbb', 'ZllH_lep_PTV_75_150_hbb', 'ZllH_lep_PTV_150_250_0J_hbb', 'ZllH_lep_PTV_150_250_GE1J_hbb', 'ZllH_lep_PTV_GT250_hbb', 'ZnnH_lep_PTV_0_75_hbb', 'ZnnH_lep_PTV_75_150_hbb', 'ZnnH_lep_PTV_150_250_0J_hbb', 'ZnnH_lep_PTV_150_250_GE1J_hbb', 'ZnnH_lep_PTV_GT250_hbb', 'ggZllH_lep_PTV_0_75_hbb', 'ggZllH_lep_PTV_75_150_hbb', 'ggZllH_lep_PTV_150_250_0J_hbb', 'ggZllH_lep_PTV_150_250_GE1J_hbb', 'ggZllH_lep_PTV_GT250_hbb', 'ggZnnH_lep_PTV_0_75_hbb', 'ggZnnH_lep_PTV_75_150_hbb', 'ggZnnH_lep_PTV_150_250_0J_hbb', 'ggZnnH_lep_PTV_150_250_GE1J_hbb', 'ggZnnH_lep_PTV_GT250_hbb', 'WminusH_lep_PTV_0_75_hbb', 'WminusH_lep_PTV_75_150_hbb', 'WminusH_lep_PTV_150_250_0J_hbb', 'WminusH_lep_PTV_150_250_GE1J_hbb', 'WminusH_lep_PTV_GT250_hbb', 'WplusH_lep_PTV_0_75_hbb', 'WplusH_lep_PTV_75_150_hbb', 'WplusH_lep_PTV_150_250_0J_hbb', 'WplusH_lep_PTV_150_250_GE1J_hbb', 'WplusH_lep_PTV_GT250_hbb'], 'BKG_ALL': ['WWTo1L1Nu2Qnlo_0b', 'WZTo1L1Nu2Qnlo_0b', 'ZZ_0b', 'WWTo1L1Nu2Qnlo_1b', 'WWTo1L1Nu2Qnlo_2b', 'WZTo1L1Nu2Qnlo_1b', 'WZTo1L1Nu2Qnlo_2b', 'ZZ_1b', 'ZZ_2b', 'ZJetsHT100_0b', 'ZJetsHT100_1b', 'ZJetsHT100_2b', 'ZJetsHT200_0b', 'ZJetsHT200_1b', 'ZJetsHT200_2b', 'ZJetsHT400_0b', 'ZJetsHT400_1b', 'ZJetsHT400_2b', 'ZJetsHT600_0b', 'ZJetsHT600_1b', 'ZJetsHT600_2b', 'ZJetsHT800_0b', 'ZJetsHT800_1b', 'ZJetsHT800_2b', 'ZJetsHT1200_0b', 'ZJetsHT1200_1b', 'ZJetsHT1200_2b', 'ZJetsHT2500_0b', 'ZJetsHT2500_1b', 'ZJetsHT2500_2b', 'ZBJets100_0b', 'ZBJets100_1b', 'ZBJets100_2b', 'ZBJets200_0b', 'ZBJets200_1b', 'ZBJets200_2b', 'ZBGenFilter100_0b', 'ZBGenFilter100_1b', 'ZBGenFilter100_2b', 'ZBGenFilter200_0b', 'ZBGenFilter200_1b', 'ZBGenFilter200_2b', 'WJetsHT0_0b', 'WJetsHT0_1b', 'WJetsHT0_2b', 'WJetsHT100_0b', 'WJetsHT100_1b', 'WJetsHT100_2b', 'WJetsHT200_0b', 'WJetsHT200_1b', 'WJetsHT200_2b', 'WJetsHT400_0b', 'WJetsHT400_1b', 'WJetsHT400_2b', 'WJetsHT600_0b', 'WJetsHT600_1b', 'WJetsHT600_2b', 'WJetsHT800_0b', 'WJetsHT800_1b', 'WJetsHT800_2b', 'WJetsHT1200_0b', 'WJetsHT1200_1b', 'WJetsHT1200_2b', 'WBJets100_0b', 'WBJets100_1b', 'WBJets100_2b', 'WBJets200_0b', 'WBJets200_1b', 'WBJets200_2b', 'WBGenFilter100_0b', 'WBGenFilter100_1b', 'WBGenFilter100_2b', 'WBGenFilter200_0b', 'WBGenFilter200_1b', 'WBGenFilter200_2b', 'M4HT100to200_0b', 'M4HT100to200_1b', 'M4HT100to200_2b', 'M4HT200to400_0b', 'M4HT200to400_1b', 'M4HT200to400_2b', 'M4HT400to600_0b', 'M4HT400to600_1b', 'M4HT400to600_2b', 'M4HT600toInf_0b', 'M4HT600toInf_1b', 'M4HT600toInf_2b', 'HT0to100ZJets_0b', 'HT0to100ZJets_1b', 'HT0to100ZJets_2b', 'HT100to200ZJets_0b', 'HT100to200ZJets_1b', 'HT100to200ZJets_2b', 'HT200to400ZJets_0b', 'HT200to400ZJets_1b', 'HT200to400ZJets_2b', 'HT400to600ZJets_0b', 'HT400to600ZJets_1b', 'HT400to600ZJets_2b', 'HT600to800ZJets_0b', 'HT600to800ZJets_1b', 'HT600to800ZJets_2b', 'HT800to1200ZJets_0b', 'HT800to1200ZJets_1b', 'HT800to1200ZJets_2b', 'HT1200to2500ZJets_0b', 'HT1200to2500ZJets_1b', 'HT1200to2500ZJets_2b', 'HT2500toinfZJets_0b', 'HT2500toinfZJets_1b', 'HT2500toinfZJets_2b', 'DYBJets_100to200_0b', 'DYBJets_100to200_1b', 'DYBJets_100to200_2b', 'DYBJets_200toInf_0b', 'DYBJets_200toInf_1b', 'DYBJets_200toInf_2b', 'DYJetsBGenFilter_100to200_0b', 'DYJetsBGenFilter_100to200_1b', 'DYJetsBGenFilter_100to200_2b', 'DYJetsBGenFilter_200toInf_0b', 'DYJetsBGenFilter_200toInf_1b', 'DYJetsBGenFilter_200toInf_2b', 'TT_2l2n', 'TT_h', 'TT_Sl', 'ST_tW_antitop', 'ST_tW_top', 'ST_s-channel_4f', 'ST_t-channel_top_4f', 'ST_t-channel_antitop_4f']}
INFO:  >   scaleFactors {'ZBGenFilter100_2b': 1.0, 'M4HT600toInf_0b': 1.0, 'WBJets200_1b': 1.0, 'WBGenFilter200_2b': 1.0, 'WBJets100_2b': 1.0, 'ZJetsHT1200_1b': 1.0, 'DYJetsBGenFilter_100to200_1b': 1.0, 'WJetsHT0_0b': 1.0, 'ggZnnH_lep_PTV_GT250_hbb': 1.0, 'ZBJets100_1b': 1.0, 'ggZllH_lep_PTV_150_250_0J_hbb': 1.0, 'ZJetsHT800_1b': 1.0, 'M4HT100to200_2b': 1.0, 'WplusH_lep_PTV_150_250_0J_hbb': 1.0, 'ZZ_1b': 1.0, 'ZllH_lep_PTV_150_250_0J_hbb': 1.0, 'WJetsHT200_1b': 1.0, 'WZTo1L1Nu2Qnlo_0b': 1.0, 'ZJetsHT100_1b': 1.0, 'WminusH_lep_PTV_GT250_hbb': 1.0, 'ST_tW_top': 1.0, 'HT400to600ZJets_1b': 1.0, 'WBGenFilter200_1b': 1.0, 'ZnnH_lep_PTV_75_150_hbb': 1.0, 'WBJets200_0b': 1.0, 'HT100to200ZJets_0b': 1.0, 'DYJetsBGenFilter_200toInf_0b': 1.0, 'WJetsHT800_1b': 1.0, 'ZBJets200_0b': 1.0, 'ZBGenFilter100_1b': 1.0, 'ZJetsHT1200_0b': 1.0, 'DYJetsBGenFilter_100to200_2b': 1.0, 'ZBJets100_0b': 1.0, 'ggZllH_lep_PTV_GT250_hbb': 1.0, 'ZJetsHT200_0b': 1.0, 'WplusH_lep_PTV_GT250_hbb': 1.0, 'ZJetsHT800_2b': 1.0, 'HT800to1200ZJets_0b': 1.0, 'ZJetsHT2500_2b': 1.0, 'ZllH_lep_PTV_GT250_hbb': 1.0, 'WJetsHT200_0b': 1.0, 'WWTo1L1Nu2Qnlo_2b': 1.0, 'ZJetsHT100_0b': 1.0, 'ZllH_lep_PTV_75_150_hbb': 1.0, 'WplusH_lep_PTV_0_75_hbb': 1.0, 'WJetsHT100_1b': 1.0, 'M4HT600toInf_2b': 1.0, 'WminusH_lep_PTV_150_250_0J_hbb': 1.0, 'ggZllH_lep_PTV_0_75_hbb': 1.0, 'WJetsHT0_2b': 1.0, 'ggZnnH_lep_PTV_150_250_0J_hbb': 1.0, 'ZJetsHT400_1b': 1.0, 'TT_2l2n': 1.0, 'HT0to100ZJets_1b': 1.0, 'ZBGenFilter200_1b': 1.0, 'DYBJets_200toInf_2b': 1.0, 'WJetsHT1200_1b': 1.0, 'HT100to200ZJets_2b': 1.0, 'HT1200to2500ZJets_1b': 1.0, 'M4HT100to200_1b': 1.0, 'ZJetsHT600_0b': 1.0, 'WZTo1L1Nu2Qnlo_2b': 1.0, 'M4HT400to600_0b': 1.0, 'WJetsHT400_1b': 1.0, 'HT600to800ZJets_2b': 1.0, 'M4HT600toInf_1b': 1.0, 'HT800to1200ZJets_2b': 1.0, 'ZllH_lep_PTV_0_75_hbb': 1.0, 'DYJetsBGenFilter_100to200_0b': 1.0, 'ggZnnH_lep_PTV_150_250_GE1J_hbb': 1.0, 'WJetsHT0_1b': 1.0, 'WminusH_lep_PTV_0_75_hbb': 1.0, 'ZBGenFilter200_2b': 1.0, 'HT0to100ZJets_2b': 1.0, 'ZJetsHT800_0b': 1.0, 'ZJetsHT400_2b': 1.0, 'ZZ_0b': 1.0, 'HT1200to2500ZJets_0b': 1.0, 'WJetsHT1200_2b': 1.0, 'WplusH_lep_PTV_150_250_GE1J_hbb': 1.0, 'WZTo1L1Nu2Qnlo_1b': 1.0, 'ST_t-channel_antitop_4f': 1.0, 'M4HT400to600_1b': 1.0, 'ZJetsHT600_1b': 1.0, 'ZBJets100_2b': 1.0, 'ZJetsHT200_2b': 1.0, 'WJetsHT400_0b': 1.0, 'ZnnH_lep_PTV_0_75_hbb': 1.0, 'HT400to600ZJets_0b': 1.0, 'M4HT100to200_0b': 1.0, 'ZJetsHT2500_0b': 1.0, 'WBGenFilter100_1b': 1.0, 'HT2500toinfZJets_2b': 1.0, 'HT400to600ZJets_2b': 1.0, 'ST_s-channel_4f': 1.0, 'DYBJets_100to200_0b': 1.0, 'DYBJets_200toInf_0b': 1.0, 'ZllH_lep_PTV_150_250_GE1J_hbb': 1.0, 'M4HT400to600_2b': 1.0, 'ZJetsHT600_2b': 1.0, 'WWTo1L1Nu2Qnlo_1b': 1.0, 'HT2500toinfZJets_1b': 1.0, 'WJetsHT100_0b': 1.0, 'WBGenFilter200_0b': 1.0, 'ZnnH_lep_PTV_150_250_0J_hbb': 1.0, 'WJetsHT600_2b': 1.0, 'HT600to800ZJets_0b': 1.0, 'DYJetsBGenFilter_200toInf_2b': 1.0, 'HT200to400ZJets_2b': 1.0, 'TT_Sl': 1.0, 'M4HT200to400_0b': 1.0, 'WJetsHT1200_0b': 1.0, 'HT1200to2500ZJets_2b': 1.0, 'DYBJets_100to200_1b': 1.0, 'ZBGenFilter200_0b': 1.0, 'HT0to100ZJets_0b': 1.0, 'WBGenFilter100_2b': 1.0, 'ZnnH_lep_PTV_150_250_GE1J_hbb': 1.0, 'WJetsHT800_2b': 1.0, 'ZZ_2b': 1.0, 'WJetsHT400_2b': 1.0, 'ZJetsHT400_0b': 1.0, 'HT200to400ZJets_1b': 1.0, 'WJetsHT600_1b': 1.0, 'WminusH_lep_PTV_75_150_hbb': 1.0, 'HT100to200ZJets_1b': 1.0, 'WJetsHT800_0b': 1.0, 'DYBJets_100to200_2b': 1.0, 'ZBJets200_1b': 1.0, 'ZBGenFilter100_0b': 1.0, 'ZJetsHT200_1b': 1.0, 'HT800to1200ZJets_1b': 1.0, 'DYBJets_200toInf_1b': 1.0, 'ST_tW_antitop': 1.0, 'ggZnnH_lep_PTV_75_150_hbb': 1.0, 'WBJets100_0b': 1.0, 'ST_t-channel_top_4f': 1.0, 'ggZnnH_lep_PTV_0_75_hbb': 1.0, 'ZJetsHT1200_2b': 1.0, 'WJetsHT600_0b': 1.0, 'M4HT200to400_1b': 1.0, 'WplusH_lep_PTV_75_150_hbb': 1.0, 'ZJetsHT2500_1b': 1.0, 'WBGenFilter100_0b': 1.0, 'DYJetsBGenFilter_200toInf_1b': 1.0, 'TT_h': 1.0, 'WJetsHT100_2b': 1.0, 'ZBJets200_2b': 1.0, 'HT200to400ZJets_0b': 1.0, 'ZJetsHT100_2b': 1.0, 'WminusH_lep_PTV_150_250_GE1J_hbb': 1.0, 'ggZllH_lep_PTV_150_250_GE1J_hbb': 1.0, 'ggZllH_lep_PTV_75_150_hbb': 1.0, 'WWTo1L1Nu2Qnlo_0b': 1.0, 'HT600to800ZJets_1b': 1.0, 'WBJets200_2b': 1.0, 'WBJets100_1b': 1.0, 'HT2500toinfZJets_0b': 1.0, 'M4HT200to400_2b': 1.0, 'ZnnH_lep_PTV_GT250_hbb': 1.0, 'WJetsHT200_2b': 1.0}
INFO:  >   systematics []
INFO:  >   testCut ((event%2)==0||isData)
INFO:  >   trainCut !((event%2)==0||isData)
INFO:  >   variables H_mass H_pt MET_Pt abs(TVector2::Phi_mpi_pi(H_phi-V_phi)) (Jet_btagDeepB[hJidx[0]]>0.1522)+(Jet_btagDeepB[hJidx[0]]>0.4941)+(Jet_btagDeepB[hJidx[0]]>0.8001) (Jet_btagDeepB[hJidx[1]]>0.1522)+(Jet_btagDeepB[hJidx[1]]>0.4941)+(Jet_btagDeepB[hJidx[1]]>0.8001) abs(Jet_eta[hJidx[0]]-Jet_eta[hJidx[1]]) abs(TVector2::Phi_mpi_pi(Jet_phi[hJidx[0]]-Jet_phi[hJidx[1]])) max(Jet_PtReg[hJidx[0]],Jet_PtReg[hJidx[1]]) min(Jet_PtReg[hJidx[0]],Jet_PtReg[hJidx[1]]) SA5 Sum$(Jet_Pt>30&&abs(Jet_eta)<2.4&&(Jet_puId>6||Jet_Pt>50)&&Jet_lepFilter&&Iteration$!=hJidx[0]&&Iteration$!=hJidx[1]) -99.0+MaxIf$(99.0+(Jet_btagDeepB>0.1522)+(Jet_btagDeepB>0.4941)+(Jet_btagDeepB>0.8001),Jet_Pt>30&&abs(Jet_eta)<2.4&&(Jet_puId>6||Jet_Pt>50)&&Jet_lepFilter&&Iteration$!=hJidx[0]&&Iteration$!=hJidx[1]) -99.0+MaxIf$(99.0+Jet_Pt,Jet_Pt>30&&abs(Jet_eta)<2.4&&(Jet_puId>6||Jet_Pt>50)&&Jet_lepFilter&&Iteration$!=hJidx[0]&&Iteration$!=hJidx[1]) -99.0+MinIf$(99.0+abs(TVector2::Phi_mpi_pi(Jet_phi-MET_Phi)),Jet_Pt>30&&abs(Jet_eta)<2.4&&(Jet_puId>6||Jet_Pt>50)&&Jet_lepFilter&&Iteration$!=hJidx[0]&&Iteration$!=hJidx[1])
INFO:  >   version 3
INFO:  >   weightF genWeight * puWeight * bTagWeightDeepCSV * 1.0 * EWKw[0] * weightLOtoNLO_2016 * 1.0 * ((isZnn * weight_mettrigSF) + (isWmunu * muonSF[0]) + (isWenu * electronSF[0])) * FitCorr[0] * 1.0
INFO:  >   weightSYS []
INFO:  >   xSecs {'ZBGenFilter100_2b': 2.07747, 'M4HT600toInf_0b': 2.2755, 'WBJets200_1b': 0.96921, 'WBGenFilter200_2b': 3.5525599999999997, 'WBJets100_2b': 6.705819999999999, 'ZJetsHT1200_1b': 0.420537, 'DYJetsBGenFilter_100to200_1b': 3.2853299999999996, 'WJetsHT0_0b': 64057.4, 'ggZnnH_lep_PTV_GT250_hbb': 0.01437, 'ZBJets100_1b': 7.63707, 'ggZllH_lep_PTV_150_250_0J_hbb': 0.0072, 'ZJetsHT800_1b': 1.8327, 'M4HT100to200_2b': 250.92, 'WplusH_lep_PTV_150_250_0J_hbb': 0.17202, 'ZZ_1b': 14.6, 'ZllH_lep_PTV_150_250_0J_hbb': 0.04718, 'WJetsHT200_1b': 493.55899999999997, 'WZTo1L1Nu2Qnlo_0b': 10.87, 'ZJetsHT100_1b': 374.53499999999997, 'WminusH_lep_PTV_GT250_hbb': 0.10899, 'ST_tW_top': 35.85, 'HT400to600ZJets_1b': 8.57064, 'WBGenFilter200_1b': 3.5525599999999997, 'ZnnH_lep_PTV_75_150_hbb': 0.09322, 'WBJets200_0b': 0.96921, 'HT100to200ZJets_0b': 198.153, 'DYJetsBGenFilter_200toInf_0b': 0.48388200000000003, 'WJetsHT800_1b': 6.492859999999999, 'ZBJets200_0b': 0.773178, 'ZBGenFilter100_1b': 2.07747, 'ZJetsHT1200_0b': 0.420537, 'DYJetsBGenFilter_100to200_2b': 3.2853299999999996, 'ZBJets100_0b': 7.63707, 'ggZllH_lep_PTV_GT250_hbb': 0.0072, 'ZJetsHT200_0b': 112.9755, 'WplusH_lep_PTV_GT250_hbb': 0.17202, 'ZJetsHT800_2b': 1.8327, 'HT800to1200ZJets_0b': 0.990396, 'ZJetsHT2500_2b': 0.0063295800000000004, 'ZllH_lep_PTV_GT250_hbb': 0.04718, 'WJetsHT200_0b': 493.55899999999997, 'WWTo1L1Nu2Qnlo_2b': 50.85883, 'ZJetsHT100_0b': 374.53499999999997, 'ZllH_lep_PTV_75_150_hbb': 0.04718, 'WplusH_lep_PTV_0_75_hbb': 0.17202, 'WJetsHT100_1b': 1687.95, 'M4HT600toInf_2b': 2.2755, 'WminusH_lep_PTV_150_250_0J_hbb': 0.10899, 'ggZllH_lep_PTV_0_75_hbb': 0.0072, 'WJetsHT0_2b': 64057.4, 'ggZnnH_lep_PTV_150_250_0J_hbb': 0.01437, 'ZJetsHT400_1b': 16.1253, 'TT_2l2n': 88.29, 'HT0to100ZJets_1b': 6571.89, 'ZBGenFilter200_1b': 0.304548, 'DYBJets_200toInf_2b': 0.40565399999999996, 'WJetsHT1200_1b': 1.2995400000000001, 'HT100to200ZJets_2b': 198.153, 'HT1200to2500ZJets_1b': 0.237759, 'M4HT100to200_1b': 250.92, 'ZJetsHT600_0b': 4.0061100000000005, 'WZTo1L1Nu2Qnlo_2b': 10.87, 'M4HT400to600_0b': 7.00731, 'WJetsHT400_1b': 69.5508, 'HT600to800ZJets_2b': 2.1438900000000003, 'M4HT600toInf_1b': 2.2755, 'HT800to1200ZJets_2b': 0.990396, 'ZllH_lep_PTV_0_75_hbb': 0.04718, 'DYJetsBGenFilter_100to200_0b': 3.2853299999999996, 'ggZnnH_lep_PTV_150_250_GE1J_hbb': 0.01437, 'WJetsHT0_1b': 64057.4, 'WminusH_lep_PTV_0_75_hbb': 0.10899, 'ZBGenFilter200_2b': 0.304548, 'HT0to100ZJets_2b': 6571.89, 'ZJetsHT800_0b': 1.8327, 'ZJetsHT400_2b': 16.1253, 'ZZ_0b': 14.6, 'HT1200to2500ZJets_0b': 0.237759, 'WJetsHT1200_2b': 1.2995400000000001, 'WplusH_lep_PTV_150_250_GE1J_hbb': 0.17202, 'WZTo1L1Nu2Qnlo_1b': 10.87, 'ST_t-channel_antitop_4f': 80.95, 'M4HT400to600_1b': 7.00731, 'ZJetsHT600_1b': 4.0061100000000005, 'ZBJets100_2b': 7.63707, 'ZJetsHT200_2b': 112.9755, 'WJetsHT400_0b': 69.5508, 'ZnnH_lep_PTV_0_75_hbb': 0.09322, 'HT400to600ZJets_0b': 8.57064, 'M4HT100to200_0b': 250.92, 'ZJetsHT2500_0b': 0.0063295800000000004, 'WBGenFilter100_1b': 24.877599999999997, 'HT2500toinfZJets_2b': 0.0042680999999999995, 'HT400to600ZJets_2b': 8.57064, 'ST_s-channel_4f': 3.692, 'DYBJets_100to200_0b': 3.96552, 'DYBJets_200toInf_0b': 0.40565399999999996, 'ZllH_lep_PTV_150_250_GE1J_hbb': 0.04718, 'M4HT400to600_2b': 7.00731, 'ZJetsHT600_2b': 4.0061100000000005, 'WWTo1L1Nu2Qnlo_1b': 50.85883, 'HT2500toinfZJets_1b': 0.0042680999999999995, 'WJetsHT100_0b': 1687.95, 'WBGenFilter200_0b': 3.5525599999999997, 'ZnnH_lep_PTV_150_250_0J_hbb': 0.09322, 'WJetsHT600_2b': 15.5727, 'HT600to800ZJets_0b': 2.1438900000000003, 'DYJetsBGenFilter_200toInf_2b': 0.48388200000000003, 'HT200to400ZJets_2b': 59.8518, 'TT_Sl': 365.34, 'M4HT200to400_0b': 66.8997, 'WJetsHT1200_0b': 1.2995400000000001, 'HT1200to2500ZJets_2b': 0.237759, 'DYBJets_100to200_1b': 3.96552, 'ZBGenFilter200_0b': 0.304548, 'HT0to100ZJets_0b': 6571.89, 'WBGenFilter100_2b': 24.877599999999997, 'ZnnH_lep_PTV_150_250_GE1J_hbb': 0.09322, 'WJetsHT800_2b': 6.492859999999999, 'ZZ_2b': 14.6, 'WJetsHT400_2b': 69.5508, 'ZJetsHT400_0b': 16.1253, 'HT200to400ZJets_1b': 59.8518, 'WJetsHT600_1b': 15.5727, 'WminusH_lep_PTV_75_150_hbb': 0.10899, 'HT100to200ZJets_1b': 198.153, 'WJetsHT800_0b': 6.492859999999999, 'DYBJets_100to200_2b': 3.96552, 'ZBJets200_1b': 0.773178, 'ZBGenFilter100_0b': 2.07747, 'ZJetsHT200_1b': 112.9755, 'HT800to1200ZJets_1b': 0.990396, 'DYBJets_200toInf_1b': 0.40565399999999996, 'ST_tW_antitop': 35.85, 'ggZnnH_lep_PTV_75_150_hbb': 0.01437, 'WBJets100_0b': 6.705819999999999, 'ST_t-channel_top_4f': 136.02, 'ggZnnH_lep_PTV_0_75_hbb': 0.01437, 'ZJetsHT1200_2b': 0.420537, 'WJetsHT600_0b': 15.5727, 'M4HT200to400_1b': 66.8997, 'WplusH_lep_PTV_75_150_hbb': 0.17202, 'ZJetsHT2500_1b': 0.0063295800000000004, 'WBGenFilter100_0b': 24.877599999999997, 'DYJetsBGenFilter_200toInf_1b': 0.48388200000000003, 'TT_h': 377.96, 'WJetsHT100_2b': 1687.95, 'ZBJets200_2b': 0.773178, 'HT200to400ZJets_0b': 59.8518, 'ZJetsHT100_2b': 374.53499999999997, 'WminusH_lep_PTV_150_250_GE1J_hbb': 0.10899, 'ggZllH_lep_PTV_150_250_GE1J_hbb': 0.0072, 'ggZllH_lep_PTV_75_150_hbb': 0.0072, 'WWTo1L1Nu2Qnlo_0b': 50.85883, 'HT600to800ZJets_1b': 2.1438900000000003, 'WBJets200_2b': 0.96921, 'WBJets100_1b': 6.705819999999999, 'HT2500toinfZJets_0b': 0.0042680999999999995, 'M4HT200to400_2b': 66.8997, 'ZnnH_lep_PTV_GT250_hbb': 0.09322, 'WJetsHT200_2b': 493.55899999999997}
INFO: random state: (3, (2147483648, 3197746885, 226567054, 1735638530, 4058328722, 781102771, 4044574658, 2600080072, 4222800911, 2161954550, 2447342644, 2486757048, 166721624, 3163971249, 3122108806, 3443015725, 741980241, 2954478047, 1548762724, 2167172650, 177269, 157974732, 2402908798, 843223978, 2144063920, 4137368871, 4083683140, 4022318807, 2743182003, 3792460710, 1399157427, 1893512320, 1537022590, 3544220246, 2361740436, 507150353, 3512462784, 2525939059, 2528235147, 3407851394, 2327796100, 1747107384, 1521253131, 1186727976, 299544363, 700237624, 1257043293, 1002707241, 4112524579, 913033411, 2508110962, 3872508581, 2646562224, 1131406294, 1808973695, 2711124665, 2480365203, 2007288379, 195638992, 4013330456, 2396073119, 2998655247, 3092606753, 2304885505, 1774440927, 3208591606, 1690782351, 3280319436, 2989406744, 3460303535, 3121419432, 365423509, 4226418633, 1160097635, 1030968408, 1430185137, 3177698655, 114385280, 1990565835, 2066200687, 2390569719, 1938249478, 3464225236, 3042123618, 1142537148, 2419146916, 2820142844, 2029038118, 1631267398, 2326080218, 1133605185, 3377145421, 3989277881, 2073439720, 811214841, 2513780224, 1149828413, 728219746, 1421087096, 1965063082, 2755221262, 4111751644, 860435224, 1775223884, 230244936, 2984436176, 3454914535, 2736228048, 711881302, 4197845916, 3549124372, 1027580, 159756152, 809826607, 1169370632, 1613038266, 1077472526, 1702845933, 2406877617, 2692354182, 1498846077, 4270883142, 3155863967, 3321211184, 312724262, 1034457067, 108707, 2484764430, 794543720, 4178776376, 329840620, 3634019488, 774250646, 523876503, 3064912946, 4166061342, 1793712555, 2537526313, 71726630, 1828457171, 3689069129, 3056638598, 684718914, 978325219, 42341030, 2565443271, 2166846717, 2676748129, 2690438427, 1884293360, 2651277063, 2976183379, 2899475742, 1527419755, 3155867382, 4147882436, 2073222844, 1024121074, 3303395140, 1964460821, 4202746619, 3264234792, 90961379, 3080816663, 3008179420, 3718818690, 194714167, 362361058, 2338799796, 385864542, 1604792239, 3288979317, 2502388481, 1777661514, 3562010576, 883583707, 154704323, 3230936624, 3416261237, 367046951, 2374747424, 3263016463, 829460050, 1752468161, 2546027321, 1632734051, 2847529577, 2297047097, 374753009, 2743981239, 3137076533, 1249539391, 4222565058, 3329388765, 1824310957, 3349501968, 755883338, 2950353400, 1380921098, 4275294365, 3226297437, 1853389011, 661897142, 1318866580, 821501395, 1282614826, 2897776083, 2890902939, 149974037, 718292355, 2046430525, 3260549622, 2759173058, 3566501254, 1538386304, 3223082325, 1415447950, 2381236774, 3691184291, 3795620615, 3915003621, 2204843755, 2413668847, 467257945, 2549126000, 2662152291, 2468778476, 2727350662, 2728854369, 1819945101, 1867059878, 581779625, 2974919432, 4272142555, 666640018, 228548255, 269998163, 3364052508, 1527486727, 2798469618, 2612825456, 2371099512, 2567570932, 1639046926, 2336771323, 3073421549, 1114081500, 3770800004, 1297937181, 3390968241, 4181738165, 309221532, 1306885369, 4290390426, 70370388, 2976010901, 2669514938, 3477582780, 2165506060, 520358420, 2794738960, 1532203123, 3946193002, 3773248893, 3304265742, 423674098, 101644142, 2020837639, 2996802939, 4048435257, 1118526256, 127730347, 543717698, 2584965804, 2914473944, 251311347, 936489523, 58003456, 2653099215, 653066448, 2885848845, 101410875, 2307475399, 1995281902, 1033120857, 2288917359, 792125517, 109528636, 2644753959, 1856723752, 2262952843, 3786958776, 4113496144, 1775711922, 1886734889, 862945033, 1250950312, 3974307828, 3299926579, 3133832338, 4110441095, 440085367, 3430823591, 2922272379, 2095017600, 2532076464, 3601429401, 1094409502, 2382162728, 3541611527, 3572480275, 1566887499, 840919735, 1740724688, 1920195650, 315156458, 2532984344, 3130048349, 1948906446, 1076304486, 1636715548, 3086027964, 583883017, 1846944848, 2392045614, 385623090, 2050792024, 4080427807, 3852208767, 2557747908, 2358607317, 2423611480, 1272108454, 4147533173, 1600202386, 759375854, 2281778258, 995945552, 683062241, 1768574461, 2076361330, 3960368252, 1899559404, 459454298, 3080609958, 2321248501, 3999490670, 3741834271, 3965484693, 1592881251, 1881776801, 1433940047, 2815733431, 1648388616, 3077278840, 1609885417, 4096111738, 2588716820, 2471621826, 3006883889, 684657733, 3940207842, 3805397679, 1546481636, 990999555, 1770000687, 1846745371, 2177303608, 513431225, 2450960657, 417561057, 4248041402, 941842172, 3808756291, 3375236788, 2922695877, 572914343, 552506412, 3901618155, 3984346929, 4239904412, 1269458442, 3178744345, 1078157285, 2771740113, 3018582518, 2984548886, 1900681562, 1877365825, 458692561, 2570737039, 393076494, 2998896748, 1704858137, 4115001349, 486850171, 492167607, 3265881219, 4025760416, 820320745, 3662783621, 979876213, 3871036397, 49296275, 2043741295, 1239412148, 1495829548, 431547704, 1218190582, 3042340279, 4281453656, 4116742350, 4001546011, 1005621907, 371539492, 2329101099, 1660467007, 3327103988, 3540899125, 3514243511, 1568387250, 3027722293, 77351835, 4044346792, 1889350231, 3280213605, 2202251045, 2314966668, 3312172657, 3441552276, 156279623, 4162522885, 873115884, 2797692792, 1805153242, 484949850, 998791835, 3990218683, 3615953125, 229740389, 2891469211, 4247515195, 1715241380, 996156455, 111966606, 3624991857, 3112365143, 1768803318, 2228163747, 2935011321, 2883194540, 3672332194, 3290320792, 1106957222, 3504823824, 3998735065, 2730684205, 1082550976, 403258973, 1353472026, 3839263995, 3107479839, 268299035, 4273809354, 3665184776, 4203496894, 1498242341, 858235920, 4038802997, 207475049, 872985097, 1287485916, 2474612723, 3597096288, 946761827, 2548646580, 1277699383, 1777252400, 1364422777, 4150464711, 1791069304, 3124101134, 974724825, 340505344, 3952184361, 4148728258, 1924890304, 3909245512, 4103467993, 1412889955, 4268845707, 1025123351, 1401789209, 3898742039, 1123138033, 718522771, 65046021, 3494588386, 708177583, 1862779089, 368828324, 2310736711, 1675455592, 2590273999, 2721599342, 2830980132, 3992519586, 3536951230, 2661060466, 3269150832, 2798513334, 1791130004, 800599576, 1502461150, 2682101657, 988814953, 2805240503, 2948037365, 2801707468, 1006164004, 2469508287, 4041906630, 1279053522, 1867826986, 1545306819, 1389367508, 3173255301, 2430598842, 3670504221, 1892444563, 2710630213, 4044957017, 720328504, 2034188461, 90520232, 1236242716, 4065667591, 1976534448, 3280826733, 2548987272, 1349561609, 361448270, 3765599522, 3515863978, 773503831, 1915672311, 3380850245, 3322913805, 1647266203, 4204550456, 2765648846, 797941287, 1636136254, 2099463166, 2946783417, 1723610875, 2150908858, 2710812541, 76354377, 2379521968, 1029476059, 3884066652, 391275241, 4197143175, 485491387, 758420923, 3394690813, 3682791380, 3075772603, 421875856, 3291889099, 2810512586, 1643721557, 1391190281, 3124247898, 4003649829, 896014811, 1529774520, 1975624501, 674261581, 652140124, 1165997583, 942977091, 803041590, 1707886084, 3701167322, 992426444, 2846125121, 4210818628, 267235103, 1019072767, 2767202112, 3813805132, 2075677682, 1772309112, 3578339783, 2618917682, 1704807034, 2262224421, 489130592, 1774300326, 3282387929, 1163711159, 4107710484, 1464105704, 2791930125, 3246463972, 3784883660, 2003055411, 1409132702, 2619827261, 3564408379, 2930527533, 1550944447, 359587722, 156713887, 2177542503, 1042312011, 2071623460, 1413676380, 1056104379, 3174707317, 4016587911, 1459452478, 624), None)
INFO: set 2029 events to 0 because of negative weight
nFeatures =  15
--------------------------------------------------------------------------------
statistics for dataset: train
--------------------------------------------------------------------------------
SIG_ALL (y= 0 ) : 103449  avg weight: 0.0011259254011398174
BKG_ALL (y= 1 ) : 51534  avg weight: 0.17916961754737584
--------------------------------------------------------------------------------
statistics for dataset: test
--------------------------------------------------------------------------------
SIG_ALL (y= 0 ) : 103579  avg weight: 0.0011011181873611126
BKG_ALL (y= 1 ) : 51759  avg weight: 0.17549697622508875
--------------------------------------------------------------------------------
classes and labels
--------------------------------------------------------------------------------
list of classes: (signals in [32mgreen[0m, backgrounds in [31mred[0m)
[32m class 0 => SIG_ALL [0m is defined as a SIGNAL
[31m class 1 => BKG_ALL [0m
--------------------------------------------------------------------------------
weights and weight uncertainty examples
--------------------------------------------------------------------------------
weights:
train 0.00086364476 0.0005766608 0.00019297442 0.00057046785 0.00057266525 0.00068351283 0.0009501989 5.0583716e-05 0.0011144946 0.0007746239
test  9.11683e-05 0.0008143712 0.0011646101 0.00012642311 0.0007826321 0.00052850804 0.0006786319 0.00061136764 0.00073853397 4.7730973e-05
weights errors:
train 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
test  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
--------------------------------------------------------------------------------
input data
--------------------------------------------------------------------------------
feature                                            set   mean       std        examples
H_mass                                             train 1.20e+02   1.71e+01   144.85745 141.68228 143.86443 114.89346
H_mass                                             test  1.21e+02   1.72e+01   139.48787 123.39203 149.11101 140.09445
H_pt                                               train 2.08e+02   6.79e+01   174.94362 161.76953 143.38297 160.85568
H_pt                                               test  2.08e+02   6.76e+01   232.522 193.49237 177.65411 138.5578
MET_Pt                                             train 2.22e+02   5.49e+01   197.02248 212.12532 172.9899 192.33469
MET_Pt                                             test  2.22e+02   5.44e+01   248.02971 189.24365 206.37663 202.84314
abs(TVector2::Phi_mpi_pi(H_phi-V_phi))             train 2.90e+00   2.18e-01   3.1355731 2.734004 3.0655775 3.0740948
abs(TVector2::Phi_mpi_pi(H_phi-V_phi))             test  2.90e+00   2.16e-01   2.6823263 3.09895 3.0120406 2.942267
(Jet_btagDeepB[hJidx[0]]>0.1522)+(Jet_btagDeep...  train 2.69e+00   4.63e-01   3.0 3.0 3.0 3.0
(Jet_btagDeepB[hJidx[0]]>0.1522)+(Jet_btagDeep...  test  2.69e+00   4.63e-01   2.0 3.0 3.0 3.0
(Jet_btagDeepB[hJidx[1]]>0.1522)+(Jet_btagDeep...  train 1.50e+00   7.33e-01   3.0 2.0 3.0 3.0
(Jet_btagDeepB[hJidx[1]]>0.1522)+(Jet_btagDeep...  test  1.49e+00   7.30e-01   1.0 1.0 1.0 2.0
abs(Jet_eta[hJidx[0]]-Jet_eta[hJidx[1]])           train 6.62e-01   4.17e-01   0.9213867 0.2109375 0.3083496 0.25769043
abs(Jet_eta[hJidx[0]]-Jet_eta[hJidx[1]])           test  6.70e-01   4.16e-01   1.050293 1.0399323 0.083984375 0.33935547
abs(TVector2::Phi_mpi_pi(Jet_phi[hJidx[0]]-Jet...  train 8.47e-01   4.69e-01   1.2056885 1.5039062 1.7203369 1.2368164
abs(TVector2::Phi_mpi_pi(Jet_phi[hJidx[0]]-Jet...  test  8.49e-01   4.73e-01   0.48583984 0.710968 1.4038086 1.5287908
max(Jet_PtReg[hJidx[0]],Jet_PtReg[hJidx[1]])       train 1.50e+02   5.73e+01   139.27135 139.14197 139.36815 122.19174
max(Jet_PtReg[hJidx[0]],Jet_PtReg[hJidx[1]])       test  1.50e+02   5.73e+01   126.71625 141.48187 130.20041 104.75809
min(Jet_PtReg[hJidx[0]],Jet_PtReg[hJidx[1]])       train 6.95e+01   2.75e+01   67.24119 73.73819 60.34037 71.963356
min(Jet_PtReg[hJidx[0]],Jet_PtReg[hJidx[1]])       test  6.90e+01   2.72e+01   36.914124 62.839317 101.14816 86.393814
SA5                                                train 2.71e+00   1.92e+00   1.0 0.0 0.0 3.0
SA5                                                test  2.74e+00   1.93e+00   2.0 1.0 1.0 1.0
Sum$(Jet_Pt>30&&abs(Jet_eta)<2.4&&(Jet_puId>6|...  train 5.28e-01   4.99e-01   0.0 0.0 0.0 0.0
Sum$(Jet_Pt>30&&abs(Jet_eta)<2.4&&(Jet_puId>6|...  test  5.28e-01   4.99e-01   0.0 0.0 0.0 0.0
-99.0+MaxIf$(99.0+(Jet_btagDeepB>0.1522)+(Jet_...  train -4.67e+01  4.95e+01   -99.0 -99.0 -99.0 -99.0
-99.0+MaxIf$(99.0+(Jet_btagDeepB>0.1522)+(Jet_...  test  -4.67e+01  4.95e+01   -99.0 -99.0 -99.0 -99.0
-99.0+MaxIf$(99.0+Jet_Pt,Jet_Pt>30&&abs(Jet_et...  train -8.00e+00  9.26e+01   -99.0 -99.0 -99.0 -99.0
-99.0+MaxIf$(99.0+Jet_Pt,Jet_Pt>30&&abs(Jet_et...  test  -7.67e+00  9.30e+01   -99.0 -99.0 -99.0 -99.0
-99.0+MinIf$(99.0+abs(TVector2::Phi_mpi_pi(Jet...  train -4.57e+01  5.05e+01   -99.0 -99.0 -99.0 -99.0
-99.0+MinIf$(99.0+abs(TVector2::Phi_mpi_pi(Jet...  test  -4.56e+01  5.05e+01   -99.0 -99.0 -99.0 -99.0
--------------------------------------------------------------------------------
input scaling
--------------------------------------------------------------------------------
[31mINFO: scaling is done inside tensorflow graph and StandardScaler() should not be used om top of it => scaler.dmp file will not be written![0m
number of expected events (test): {0: 9083.547992434369, 1: 114.05272072867669}
number of expected events (train): {0: 9233.327070686466, 1: 116.47585682251297}
balancing signal/background, reweight class SIG_ALL  from group 1 (signals) by 80.27245458907677
balancing signal/background, reweight class BKG_ALL  from group 0 (backgrounds) by 1.012614722291415
shape train: (154983, 15)
shape test:  (155338, 15)
building tensorflow graph with parameters
 adam_epsilon                             1e-09
 adaptiveRate                             False
 additional_noise                         0.0
 balanceClasses                           False
 balanceSignalBackground                  True
 batchNormalization                       [1, 2, 3, 4, 5, 6, 7, 8]
 batchSize                                1024
 batchSizeTest                            65536
 binMethod                                'SB'
 binTarget                                [0.109, 0.1194, 0.1242, 0.1226, 0.1148, 0.1021, 0.0861, 0.069, 0.0524, 0.0378, 0.0259, 0.0168, 0.0104, 0.0061, 0.0034]
 crossValidation_splitSeed                123456
 dropoutDecay                             1.0
 ignoreLargeWeights                       False
 ignoreNegativeWeights                    True
 learningRate                             {0: 1.0, 50: 0.5, 100: 0.25, 200: 0.1, 300: 0.05, 400: 0.02, 500: 0.01, 600: 0.005, 700: 0.002, 800: 0.001}
 loss                                     'cross_entropy'
 massless_importance                      1.0
 massless_powers                          [1, 2]
 momentum                                 0.9
 mvaScoreRescalingPercentileHigh          0.999
 mvaScoreRescalingPercentileLow           0.01
 nEpochs                                  1000
 nNodes                                   [512, 256, 128, 64, 64, 64]
 optimizer                                'momentum'
 pDropout                                 [0.2, 0.5, 0.5, 0.5, 0.5, 0.5]
 plot-data                                False
 plot-inputs                              True
 plot-jacobian                            False
 plot-scores                              True
 power                                    1.0
 rateGamma                                1.0
 removeFeature                            []
 reweight                                 None
 reweightTraining                         None
 saveCheckpointInterval                   50
 scaleInputsInsideGraph                   True
 shuffle                                  True
 signif_loss_b_epsilon                    1e-08
 signif_loss_low_b_threshold              1.5
 signif_loss_low_b_threshold_width        1.5
 signif_loss_nbins                        15
 signif_loss_smoothness                   500.0
 signif_loss_sysApprox_constant           1.5
 signif_loss_sysApprox_linear             0.1
 signif_loss_sys_variance_offset          0.1
 signif_loss_xe_factor                    0.0
 skipConnections                          {2: [0], 4: [0, 2], 6: [0, 2, 4]}
 statisticsInterval                       20
 systematics_scaling_factor               1.0
 systematics_weight_scaling_factor        1.0
 weight_sys_ntoys                         -1
initialize session...
initialized session!
add layers...
layer  1 :  [15, 512]
> activation with drop-out...
> batch normalization...
layer  2 :  [512, 256]
> activation with drop-out...
> batch normalization...
layer  3 :  [256, 128]
> activation with drop-out...
> batch normalization...
layer  4 :  [128, 64]
> activation with drop-out...
> batch normalization...
layer  5 :  [64, 64]
> activation with drop-out...
> batch normalization...
layer  6 :  [64, 64]
> activation with drop-out...
> batch normalization...
INFO: use MomentumOptimizer
graph built.
trainable variables: 231746
initialized.
conditions:
  loss(train,training)    training dataset, loss as seen by gradient descend optimizer
  loss(train,testing)     training dataset, loss with 'testing' conditions, e.g. is_training: False, no dropout etc
  loss(test)              test dataset
start training with batch size 1024 and learning rate 1.0 
 epoch     loss(train,training) loss(train,testing) loss(test)
         1    0.06757    0.06616    0.06493 significance (train): 2.419 significance: 2.408 
         2    0.06468    0.06261    0.06149 
         3    0.06371    0.06287    0.06194 
         4    0.06361    0.06292    0.06191 
         5    0.06326    0.06129    0.06024 
         6    0.06328    0.06266    0.06169 
         7    0.06317    0.06159    0.06062 
         8    0.06339    0.06149    0.06056 
         9    0.06319    0.06122    0.06029 
        10    0.06312    0.06172    0.06090 
        11    0.06287    0.06097    0.06014 
        12    0.06354    0.06068    0.05978 
        13    0.06307    0.06126    0.06046 
        14    0.06265    0.06083    0.05998 
        15    0.06230    0.06073    0.05994 
        16    0.06274    0.06102    0.06024 
        17    0.06263    0.06109    0.06034 
        18    0.06257    0.06106    0.06011 
        19    0.06307    0.06130    0.06049 
        20    0.06253    0.06072    0.06005 
        21    0.06249    0.06129    0.06065 significance (train): 2.560 significance: 2.521 
        22    0.06280    0.06092    0.06015 
        23    0.06239    0.06034    0.05956 
        24    0.06225    0.06148    0.06093 
        25    0.06242    0.06037    0.05973 
        26    0.06234    0.06065    0.06000 
        27    0.06286    0.06091    0.06010 
        28    0.06230    0.06059    0.05996 
        29    0.06223    0.06122    0.06042 
        30    0.06226    0.06035    0.05972 
        31    0.06265    0.06106    0.06036 
        32    0.06245    0.06063    0.05991 
        33    0.06211    0.06124    0.06068 
        34    0.06245    0.06147    0.06073 
        35    0.06243    0.06182    0.06126 
        36    0.06252    0.06054    0.05982 
        37    0.06231    0.06022    0.05957 
        38    0.06218    0.06091    0.06017 
        39    0.06213    0.06021    0.05960 
        40    0.06243    0.06031    0.05972 
        41    0.06213    0.06073    0.06007 significance (train): 2.714 significance: 2.648 
        42    0.06231    0.06019    0.05959 
        43    0.06227    0.06088    0.06038 
        44    0.06236    0.06053    0.05989 
        45    0.06187    0.06144    0.06064 
        46    0.06208    0.06261    0.06182 
        47    0.06218    0.06096    0.06058 
        48    0.06191    0.06030    0.05989 
        49    0.06242    0.06011    0.05948 
        50    0.06216    0.06013    0.05949 
set learning rate to: 0.5
        51    0.06132    0.06016    0.05958 
        52    0.06130    0.06013    0.05959 
        53    0.06116    0.06029    0.05998 
        54    0.06120    0.05990    0.05953 
        55    0.06148    0.06007    0.05965 
        56    0.06134    0.05996    0.05959 
        57    0.06154    0.05997    0.05943 
        58    0.06121    0.05979    0.05934 
        59    0.06129    0.06003    0.05955 
        60    0.06123    0.05982    0.05952 
        61    0.06135    0.06016    0.05984 significance (train): 2.643 significance: 2.581 
        62    0.06126    0.06018    0.05991 
        63    0.06140    0.05981    0.05948 
        64    0.06125    0.05984    0.05935 
        65    0.06108    0.06000    0.05949 
        66    0.06120    0.05974    0.05943 
        67    0.06137    0.05979    0.05938 
        68    0.06118    0.06043    0.05991 
        69    0.06130    0.05973    0.05934 
        70    0.06103    0.05985    0.05942 
        71    0.06111    0.05976    0.05948 
        72    0.06114    0.05987    0.05947 
        73    0.06114    0.06030    0.05975 
        74    0.06142    0.05988    0.05956 
        75    0.06129    0.06007    0.05972 
        76    0.06132    0.05976    0.05939 
        77    0.06100    0.05973    0.05934 
        78    0.06104    0.05972    0.05934 
        79    0.06135    0.06060    0.06005 
        80    0.06104    0.05981    0.05941 
        81    0.06110    0.06019    0.05970 significance (train): 2.772 significance: 2.659 
        82    0.06107    0.05970    0.05931 
        83    0.06102    0.05962    0.05928 
        84    0.06108    0.05978    0.05954 
        85    0.06111    0.06043    0.06037 
        86    0.06092    0.05978    0.05945 
        87    0.06124    0.05967    0.05944 
        88    0.06094    0.05959    0.05931 
        89    0.06107    0.05970    0.05938 
        90    0.06103    0.05964    0.05927 
        91    0.06101    0.05959    0.05932 
        92    0.06117    0.05962    0.05940 
        93    0.06107    0.05969    0.05938 
        94    0.06105    0.05971    0.05933 
        95    0.06118    0.05994    0.05982 
        96    0.06100    0.05971    0.05937 
        97    0.06106    0.05981    0.05970 
        98    0.06100    0.05960    0.05938 
        99    0.06119    0.05980    0.05938 
       100    0.06109    0.05951    0.05927 
set learning rate to: 0.25
       101    0.06080    0.05963    0.05932 significance (train): 2.742 significance: 2.625 
       102    0.06067    0.05943    0.05919 
       103    0.06076    0.05945    0.05920 
       104    0.06061    0.05943    0.05926 
       105    0.06066    0.05947    0.05924 
       106    0.06064    0.05957    0.05935 
       107    0.06072    0.05945    0.05930 
       108    0.06060    0.05961    0.05930 
       109    0.06067    0.05940    0.05918 
       110    0.06075    0.05941    0.05926 
       111    0.06066    0.05952    0.05924 
       112    0.06063    0.05938    0.05925 
       113    0.06061    0.05947    0.05937 
       114    0.06067    0.05985    0.05950 
       115    0.06065    0.05940    0.05920 
       116    0.06065    0.05936    0.05921 
       117    0.06056    0.05946    0.05922 
       118    0.06053    0.05945    0.05923 
       119    0.06070    0.05933    0.05919 
       120    0.06063    0.05959    0.05934 
       121    0.06067    0.05937    0.05915 significance (train): 2.772 significance: 2.645 
       122    0.06064    0.05944    0.05925 
       123    0.06060    0.05934    0.05919 
       124    0.06060    0.05935    0.05929 
       125    0.06067    0.05941    0.05921 
       126    0.06062    0.05950    0.05930 
       127    0.06054    0.05932    0.05918 
       128    0.06059    0.05935    0.05926 
       129    0.06053    0.05946    0.05947 
       130    0.06062    0.05956    0.05934 
       131    0.06056    0.05935    0.05930 
       132    0.06063    0.05927    0.05917 
       133    0.06061    0.05930    0.05926 
       134    0.06061    0.05932    0.05919 
       135    0.06054    0.05948    0.05933 
       136    0.06058    0.05924    0.05919 
       137    0.06045    0.05927    0.05923 
       138    0.06065    0.05932    0.05918 
       139    0.06067    0.05936    0.05922 
       140    0.06059    0.05935    0.05919 
       141    0.06064    0.05937    0.05923 significance (train): 2.793 significance: 2.646 
       142    0.06053    0.05956    0.05964 
       143    0.06062    0.05942    0.05922 
       144    0.06071    0.05934    0.05923 
       145    0.06067    0.05935    0.05919 
       146    0.06065    0.05931    0.05919 
       147    0.06053    0.05924    0.05914 
       148    0.06052    0.05924    0.05918 
       149    0.06044    0.05935    0.05923 
       150    0.06047    0.05956    0.05934 
       151    0.06051    0.05948    0.05933 
       152    0.06053    0.05930    0.05935 
       153    0.06058    0.05926    0.05913 
       154    0.06065    0.05944    0.05925 
       155    0.06048    0.05962    0.05941 
       156    0.06066    0.05948    0.05931 
       157    0.06064    0.05941    0.05948 
       158    0.06051    0.05940    0.05923 
       159    0.06059    0.05941    0.05924 
       160    0.06059    0.05920    0.05920 
       161    0.06042    0.05956    0.05965 significance (train): 2.710 significance: 2.612 
       162    0.06056    0.05921    0.05914 
       163    0.06059    0.05920    0.05915 
       164    0.06057    0.05925    0.05923 
       165    0.06043    0.05925    0.05917 
       166    0.06061    0.05931    0.05927 
       167    0.06048    0.05924    0.05918 
       168    0.06048    0.05939    0.05929 
       169    0.06056    0.05928    0.05927 
       170    0.06052    0.05939    0.05919 
       171    0.06058    0.05924    0.05928 
       172    0.06070    0.05943    0.05926 
       173    0.06058    0.05951    0.05964 
       174    0.06060    0.05944    0.05929 
       175    0.06044    0.05924    0.05919 
       176    0.06042    0.05927    0.05924 
       177    0.06053    0.05929    0.05921 
       178    0.06047    0.05923    0.05930 
       179    0.06042    0.05915    0.05918 
       180    0.06063    0.05933    0.05923 
       181    0.06045    0.05921    0.05930 significance (train): 2.760 significance: 2.641 
       182    0.06050    0.05926    0.05936 
       183    0.06049    0.05953    0.05937 
       184    0.06042    0.05934    0.05946 
       185    0.06055    0.05926    0.05930 
       186    0.06057    0.05917    0.05915 
       187    0.06037    0.05919    0.05922 
       188    0.06057    0.05914    0.05922 
       189    0.06042    0.05914    0.05919 
       190    0.06057    0.05917    0.05920 
       191    0.06044    0.05939    0.05929 
       192    0.06040    0.05936    0.05947 
       193    0.06042    0.05915    0.05915 
       194    0.06042    0.05952    0.05940 
       195    0.06046    0.05908    0.05915 
       196    0.06037    0.05917    0.05915 
       197    0.06034    0.05912    0.05918 
       198    0.06051    0.05909    0.05919 
       199    0.06034    0.05911    0.05923 
       200    0.06054    0.05916    0.05913 
set learning rate to: 0.1
       201    0.06027    0.05929    0.05922 significance (train): 2.830 significance: 2.657 
       202    0.06027    0.05909    0.05914 
       203    0.06024    0.05905    0.05910 
       204    0.06038    0.05906    0.05912 
       205    0.06018    0.05917    0.05918 
       206    0.06019    0.05907    0.05913 
       207    0.06030    0.05905    0.05917 
       208    0.06029    0.05910    0.05914 
       209    0.06018    0.05907    0.05913 
       210    0.06041    0.05953    0.05941 
       211    0.06031    0.05904    0.05912 
       212    0.06018    0.05906    0.05922 
       213    0.06023    0.05908    0.05919 
       214    0.06030    0.05907    0.05911 
       215    0.06027    0.05916    0.05918 
       216    0.06024    0.05915    0.05932 
       217    0.06018    0.05903    0.05912 
       218    0.06018    0.05908    0.05924 
       219    0.06021    0.05901    0.05912 
       220    0.06018    0.05902    0.05911 
       221    0.06028    0.05913    0.05933 significance (train): 2.750 significance: 2.625 
       222    0.06016    0.05906    0.05919 
       223    0.06031    0.05901    0.05912 
       224    0.06023    0.05902    0.05917 
       225    0.06026    0.05915    0.05936 
       226    0.06012    0.05905    0.05912 
       227    0.06029    0.05922    0.05920 
       228    0.06021    0.05908    0.05909 
       229    0.06014    0.05900    0.05917 
       230    0.06030    0.05905    0.05909 
       231    0.06015    0.05909    0.05913 
       232    0.06023    0.05918    0.05917 
       233    0.06013    0.05906    0.05913 
       234    0.06030    0.05901    0.05909 
       235    0.06019    0.05900    0.05912 
       236    0.06017    0.05901    0.05911 
       237    0.06006    0.05911    0.05917 
       238    0.06026    0.05903    0.05909 
       239    0.06026    0.05941    0.05935 
       240    0.06034    0.05900    0.05911 
       241    0.06021    0.05913    0.05913 significance (train): 2.819 significance: 2.645 
       242    0.06017    0.05902    0.05917 
       243    0.06016    0.05903    0.05914 
       244    0.06020    0.05921    0.05920 
       245    0.06018    0.05901    0.05908 
       246    0.06018    0.05901    0.05924 
       247    0.06021    0.05898    0.05916 
       248    0.06011    0.05902    0.05912 
       249    0.06016    0.05903    0.05920 
       250    0.06017    0.05904    0.05910 
       251    0.06023    0.05898    0.05910 
       252    0.06016    0.05902    0.05911 
       253    0.06019    0.05898    0.05913 
       254    0.06011    0.05911    0.05917 
       255    0.06029    0.05899    0.05912 
       256    0.06012    0.05899    0.05916 
       257    0.06022    0.05907    0.05914 
       258    0.06023    0.05896    0.05913 
       259    0.06021    0.05907    0.05914 
       260    0.06013    0.05902    0.05910 
       261    0.06014    0.05894    0.05911 significance (train): 2.803 significance: 2.640 
       262    0.06021    0.05896    0.05910 
       263    0.06011    0.05898    0.05923 
       264    0.06011    0.05895    0.05915 
       265    0.06020    0.05911    0.05916 
       266    0.06012    0.05900    0.05912 
       267    0.06027    0.05895    0.05911 
       268    0.06018    0.05905    0.05915 
       269    0.06015    0.05898    0.05911 
       270    0.06017    0.05897    0.05920 
       271    0.06024    0.05894    0.05909 
       272    0.06013    0.05901    0.05911 
       273    0.06017    0.05895    0.05909 
       274    0.06017    0.05917    0.05920 
       275    0.06011    0.05900    0.05910 
       276    0.06020    0.05904    0.05930 
       277    0.06005    0.05897    0.05913 
       278    0.06012    0.05901    0.05914 
       279    0.06009    0.05896    0.05911 
       280    0.06012    0.05904    0.05913 
       281    0.06006    0.05896    0.05918 significance (train): 2.781 significance: 2.633 
       282    0.06017    0.05905    0.05916 
       283    0.06013    0.05895    0.05913 
       284    0.06018    0.05907    0.05931 
       285    0.06024    0.05901    0.05914 
       286    0.06018    0.05893    0.05911 
       287    0.06019    0.05894    0.05913 
       288    0.06013    0.05919    0.05922 
       289    0.06013    0.05892    0.05908 
       290    0.06017    0.05897    0.05925 
       291    0.06027    0.05906    0.05915 
       292    0.06019    0.05892    0.05910 
       293    0.06019    0.05895    0.05909 
       294    0.06017    0.05894    0.05908 
       295    0.06025    0.05898    0.05912 
       296    0.06010    0.05896    0.05918 
       297    0.06013    0.05893    0.05908 
       298    0.06015    0.05894    0.05910 
       299    0.06021    0.05900    0.05928 
       300    0.06019    0.05899    0.05913 
set learning rate to: 0.05
       301    0.06014    0.05894    0.05907 significance (train): 2.820 significance: 2.650 
       302    0.06003    0.05890    0.05909 
       303    0.05998    0.05897    0.05908 
       304    0.06011    0.05902    0.05911 
       305    0.06009    0.05891    0.05907 
       306    0.06005    0.05889    0.05909 
       307    0.06006    0.05894    0.05913 
       308    0.06003    0.05910    0.05917 
       309    0.06012    0.05890    0.05913 
       310    0.06010    0.05889    0.05912 
       311    0.06001    0.05894    0.05908 
       312    0.06010    0.05890    0.05910 
       313    0.06005    0.05891    0.05907 
       314    0.06014    0.05894    0.05908 
       315    0.06005    0.05892    0.05908 
       316    0.06001    0.05887    0.05911 
       317    0.06005    0.05887    0.05909 
       318    0.05996    0.05895    0.05910 
       319    0.06017    0.05887    0.05911 
       320    0.06011    0.05887    0.05910 
       321    0.06001    0.05888    0.05913 significance (train): 2.794 significance: 2.636 
       322    0.06001    0.05887    0.05909 
       323    0.06012    0.05890    0.05909 
       324    0.06009    0.05888    0.05913 
       325    0.06009    0.05888    0.05908 
       326    0.06001    0.05891    0.05910 
       327    0.06009    0.05899    0.05910 
       328    0.06016    0.05888    0.05910 
       329    0.06001    0.05891    0.05909 
       330    0.06003    0.05888    0.05907 
       331    0.06006    0.05892    0.05923 
       332    0.06010    0.05888    0.05912 
       333    0.06003    0.05888    0.05907 
       334    0.06004    0.05890    0.05907 
       335    0.06009    0.05888    0.05907 
       336    0.06012    0.05889    0.05907 
       337    0.06001    0.05887    0.05911 
       338    0.05999    0.05887    0.05910 
       339    0.06000    0.05906    0.05916 
       340    0.05999    0.05890    0.05909 
       341    0.06010    0.05891    0.05909 significance (train): 2.810 significance: 2.649 
       342    0.06003    0.05887    0.05913 
       343    0.06008    0.05888    0.05908 
       344    0.06004    0.05888    0.05908 
       345    0.06006    0.05890    0.05909 
       346    0.06006    0.05901    0.05913 
       347    0.06008    0.05886    0.05908 
       348    0.06005    0.05889    0.05907 
       349    0.06003    0.05890    0.05907 
       350    0.06009    0.05900    0.05913 
       351    0.05999    0.05888    0.05908 
       352    0.06005    0.05886    0.05908 
       353    0.06009    0.05886    0.05912 
       354    0.06007    0.05895    0.05910 
       355    0.06015    0.05886    0.05911 
       356    0.06005    0.05888    0.05911 
       357    0.06019    0.05888    0.05908 
       358    0.06008    0.05886    0.05907 
       359    0.05995    0.05885    0.05909 
       360    0.06014    0.05903    0.05916 
       361    0.06003    0.05889    0.05910 significance (train): 2.819 significance: 2.650 
       362    0.06000    0.05892    0.05909 
       363    0.05997    0.05891    0.05912 
       364    0.06000    0.05889    0.05912 
       365    0.06002    0.05884    0.05909 
       366    0.05999    0.05884    0.05913 
       367    0.05996    0.05899    0.05912 
       368    0.05999    0.05885    0.05907 
       369    0.06002    0.05884    0.05907 
       370    0.05999    0.05887    0.05916 
       371    0.05999    0.05887    0.05909 
       372    0.06004    0.05884    0.05911 
       373    0.05999    0.05888    0.05925 
       374    0.05999    0.05885    0.05914 
       375    0.06001    0.05885    0.05908 
       376    0.06007    0.05886    0.05909 
       377    0.06003    0.05883    0.05911 
       378    0.06011    0.05889    0.05908 
       379    0.06010    0.05896    0.05910 
       380    0.06005    0.05886    0.05911 
       381    0.06000    0.05885    0.05909 significance (train): 2.801 significance: 2.644 
       382    0.05996    0.05885    0.05911 
       383    0.05999    0.05883    0.05909 
       384    0.05995    0.05885    0.05907 
       385    0.06013    0.05894    0.05910 
       386    0.06002    0.05893    0.05909 
       387    0.06002    0.05891    0.05912 
       388    0.05997    0.05890    0.05909 
       389    0.06008    0.05886    0.05908 
       390    0.06007    0.05884    0.05912 
       391    0.05999    0.05886    0.05909 
       392    0.06019    0.05887    0.05906 
       393    0.06004    0.05886    0.05907 
       394    0.06008    0.05887    0.05907 
       395    0.05993    0.05884    0.05914 
       396    0.06009    0.05887    0.05905 
       397    0.06001    0.05887    0.05907 
       398    0.06000    0.05896    0.05912 
       399    0.06008    0.05885    0.05906 
       400    0.06000    0.05883    0.05906 
set learning rate to: 0.02
       401    0.06013    0.05883    0.05907 significance (train): 2.818 significance: 2.646 
       402    0.05993    0.05883    0.05908 
       403    0.05998    0.05887    0.05908 
       404    0.06003    0.05885    0.05908 
       405    0.05997    0.05882    0.05907 
       406    0.06003    0.05883    0.05906 
       407    0.05990    0.05883    0.05907 
       408    0.06003    0.05882    0.05907 
       409    0.06006    0.05884    0.05906 
       410    0.06003    0.05886    0.05906 
       411    0.05992    0.05888    0.05907 
       412    0.05993    0.05881    0.05909 
       413    0.05997    0.05883    0.05908 
       414    0.05999    0.05883    0.05907 
       415    0.05994    0.05882    0.05907 
       416    0.05995    0.05884    0.05906 
       417    0.05997    0.05884    0.05907 
       418    0.06006    0.05886    0.05907 
       419    0.05993    0.05880    0.05909 
       420    0.05983    0.05885    0.05907 
       421    0.06001    0.05882    0.05906 significance (train): 2.829 significance: 2.634 
       422    0.06000    0.05882    0.05906 
       423    0.05997    0.05883    0.05906 
       424    0.06002    0.05888    0.05908 
       425    0.05997    0.05880    0.05909 
       426    0.05997    0.05883    0.05907 
       427    0.05991    0.05886    0.05907 
       428    0.05999    0.05883    0.05907 
       429    0.06000    0.05882    0.05906 
       430    0.06002    0.05883    0.05906 
       431    0.05997    0.05881    0.05908 
       432    0.05993    0.05881    0.05907 
       433    0.06002    0.05887    0.05908 
       434    0.05996    0.05887    0.05908 
       435    0.06001    0.05883    0.05906 
       436    0.05999    0.05881    0.05907 
       437    0.05994    0.05883    0.05907 
       438    0.06000    0.05879    0.05908 
       439    0.06000    0.05882    0.05907 
       440    0.05988    0.05881    0.05911 
       441    0.06001    0.05880    0.05907 significance (train): 2.825 significance: 2.637 
       442    0.05997    0.05882    0.05906 
       443    0.06002    0.05881    0.05906 
       444    0.05996    0.05885    0.05907 
       445    0.05996    0.05881    0.05906 
       446    0.05998    0.05881    0.05907 
       447    0.05997    0.05892    0.05909 
       448    0.05989    0.05889    0.05908 
       449    0.06005    0.05885    0.05906 
       450    0.05997    0.05884    0.05907 
       451    0.06009    0.05880    0.05908 
       452    0.06002    0.05883    0.05907 
       453    0.05997    0.05880    0.05907 
       454    0.06003    0.05881    0.05907 
       455    0.05998    0.05881    0.05906 
       456    0.05999    0.05884    0.05907 
       457    0.05998    0.05880    0.05908 
       458    0.05994    0.05883    0.05907 
       459    0.06001    0.05880    0.05911 
       460    0.05990    0.05886    0.05906 
       461    0.05985    0.05882    0.05907 significance (train): 2.817 significance: 2.649 
       462    0.05994    0.05881    0.05907 
       463    0.06001    0.05880    0.05908 
       464    0.05996    0.05880    0.05909 
       465    0.05993    0.05883    0.05906 
       466    0.05993    0.05881    0.05906 
       467    0.05994    0.05880    0.05907 
       468    0.05997    0.05881    0.05909 
       469    0.05994    0.05885    0.05908 
       470    0.05986    0.05883    0.05907 
       471    0.06001    0.05880    0.05907 
       472    0.05995    0.05881    0.05906 
       473    0.05989    0.05879    0.05909 
       474    0.06002    0.05883    0.05907 
       475    0.05996    0.05886    0.05907 
       476    0.05994    0.05889    0.05909 
       477    0.05994    0.05881    0.05907 
       478    0.05999    0.05883    0.05905 
       479    0.05997    0.05890    0.05909 
       480    0.06000    0.05883    0.05906 
       481    0.06000    0.05880    0.05907 significance (train): 2.823 significance: 2.644 
       482    0.05983    0.05881    0.05907 
       483    0.06000    0.05880    0.05907 
       484    0.05998    0.05884    0.05907 
       485    0.05997    0.05888    0.05907 
       486    0.05983    0.05880    0.05906 
       487    0.06003    0.05880    0.05906 
       488    0.05992    0.05884    0.05907 
       489    0.05996    0.05882    0.05906 
       490    0.06002    0.05880    0.05907 
       491    0.05998    0.05879    0.05911 
       492    0.05998    0.05881    0.05906 
       493    0.05996    0.05884    0.05907 
       494    0.05997    0.05887    0.05908 
       495    0.05999    0.05882    0.05907 
       496    0.05990    0.05881    0.05907 
       497    0.06000    0.05880    0.05907 
       498    0.05997    0.05881    0.05906 
       499    0.05994    0.05880    0.05908 
       500    0.05991    0.05879    0.05907 
set learning rate to: 0.01
       501    0.05985    0.05879    0.05907 significance (train): 2.810 significance: 2.641 
       502    0.06001    0.05882    0.05906 
       503    0.05991    0.05879    0.05907 
       504    0.06006    0.05883    0.05906 
       505    0.05997    0.05885    0.05907 
       506    0.05996    0.05880    0.05906 
       507    0.05990    0.05880    0.05906 
       508    0.05991    0.05878    0.05908 
       509    0.05991    0.05882    0.05906 
       510    0.05994    0.05880    0.05906 
       511    0.06002    0.05881    0.05906 
       512    0.05993    0.05880    0.05906 
       513    0.05984    0.05879    0.05908 
       514    0.06003    0.05880    0.05907 
       515    0.06003    0.05881    0.05906 
       516    0.05995    0.05883    0.05906 
       517    0.05995    0.05880    0.05906 
       518    0.05999    0.05879    0.05907 
       519    0.05998    0.05881    0.05906 
       520    0.05994    0.05879    0.05907 
       521    0.05999    0.05882    0.05906 significance (train): 2.818 significance: 2.648 
       522    0.05987    0.05882    0.05906 
       523    0.05995    0.05879    0.05906 
       524    0.05984    0.05878    0.05908 
       525    0.05994    0.05880    0.05906 
       526    0.06001    0.05881    0.05906 
       527    0.06000    0.05879    0.05907 
       528    0.05991    0.05879    0.05906 
       529    0.05992    0.05878    0.05907 
       530    0.05994    0.05880    0.05906 
       531    0.05983    0.05880    0.05906 
       532    0.05997    0.05880    0.05906 
       533    0.06001    0.05878    0.05907 
       534    0.06001    0.05878    0.05907 
       535    0.05988    0.05878    0.05907 
       536    0.05996    0.05880    0.05906 
       537    0.06000    0.05879    0.05906 
       538    0.05990    0.05880    0.05906 
       539    0.05994    0.05879    0.05907 
       540    0.05990    0.05879    0.05907 
       541    0.05989    0.05879    0.05907 significance (train): 2.823 significance: 2.636 
       542    0.06002    0.05880    0.05906 
       543    0.06001    0.05880    0.05906 
       544    0.05996    0.05878    0.05907 
       545    0.05989    0.05880    0.05906 
       546    0.06001    0.05880    0.05906 
       547    0.06010    0.05880    0.05906 
       548    0.05991    0.05881    0.05906 
       549    0.05998    0.05884    0.05907 
       550    0.05996    0.05884    0.05907 
       551    0.06001    0.05879    0.05906 
       552    0.05992    0.05882    0.05906 
       553    0.05986    0.05885    0.05907 
       554    0.05999    0.05882    0.05906 
       555    0.05989    0.05880    0.05906 
       556    0.05991    0.05882    0.05906 
       557    0.05996    0.05878    0.05907 
       558    0.05988    0.05881    0.05907 
       559    0.05988    0.05879    0.05907 
       560    0.05988    0.05879    0.05907 
       561    0.05988    0.05880    0.05906 significance (train): 2.823 significance: 2.644 
       562    0.05996    0.05881    0.05906 
       563    0.05991    0.05879    0.05906 
       564    0.06012    0.05880    0.05906 
       565    0.05976    0.05879    0.05906 
       566    0.05995    0.05880    0.05906 
       567    0.05985    0.05878    0.05909 
       568    0.05994    0.05880    0.05907 
       569    0.05986    0.05880    0.05907 
       570    0.05990    0.05882    0.05907 
       571    0.05991    0.05880    0.05907 
       572    0.05992    0.05878    0.05907 
       573    0.05990    0.05877    0.05909 
       574    0.05992    0.05878    0.05906 
       575    0.05987    0.05879    0.05906 
       576    0.05989    0.05877    0.05907 
       577    0.05991    0.05880    0.05906 
       578    0.05991    0.05879    0.05906 
       579    0.05990    0.05878    0.05907 
       580    0.05996    0.05879    0.05907 
       581    0.05990    0.05880    0.05907 significance (train): 2.825 significance: 2.642 
       582    0.05994    0.05880    0.05906 
       583    0.05998    0.05878    0.05907 
       584    0.05992    0.05880    0.05906 
       585    0.05981    0.05879    0.05906 
       586    0.05998    0.05880    0.05907 
       587    0.05999    0.05878    0.05907 
       588    0.05996    0.05878    0.05907 
       589    0.05996    0.05879    0.05906 
       590    0.06010    0.05877    0.05908 
       591    0.06000    0.05880    0.05906 
       592    0.05984    0.05879    0.05906 
       593    0.05992    0.05879    0.05906 
       594    0.05988    0.05877    0.05908 
       595    0.05996    0.05879    0.05906 
       596    0.05989    0.05880    0.05906 
       597    0.05981    0.05880    0.05906 
       598    0.05988    0.05878    0.05907 
       599    0.05991    0.05879    0.05906 
       600    0.05989    0.05877    0.05908 
set learning rate to: 0.005
       601    0.05982    0.05878    0.05907 significance (train): 2.828 significance: 2.637 
       602    0.05988    0.05878    0.05906 
       603    0.06009    0.05880    0.05906 
       604    0.05994    0.05880    0.05906 
       605    0.05988    0.05878    0.05906 
       606    0.05998    0.05880    0.05906 
       607    0.05984    0.05878    0.05906 
       608    0.06000    0.05880    0.05906 
       609    0.05994    0.05878    0.05907 
       610    0.05985    0.05878    0.05906 
       611    0.05991    0.05879    0.05906 
       612    0.05996    0.05879    0.05906 
       613    0.05996    0.05878    0.05906 
       614    0.05986    0.05877    0.05907 
       615    0.05992    0.05879    0.05906 
       616    0.05998    0.05878    0.05906 
       617    0.05989    0.05879    0.05906 
       618    0.05991    0.05878    0.05906 
       619    0.06009    0.05880    0.05906 
       620    0.05981    0.05877    0.05907 
       621    0.05991    0.05879    0.05906 significance (train): 2.815 significance: 2.647 
       622    0.05989    0.05878    0.05906 
       623    0.05983    0.05878    0.05906 
       624    0.06000    0.05878    0.05906 
       625    0.05985    0.05878    0.05907 
       626    0.05986    0.05879    0.05906 
       627    0.05990    0.05878    0.05907 
       628    0.05986    0.05877    0.05907 
       629    0.05996    0.05879    0.05906 
       630    0.05989    0.05878    0.05906 
       631    0.05995    0.05879    0.05906 
       632    0.06000    0.05880    0.05906 
       633    0.05996    0.05878    0.05906 
       634    0.05991    0.05877    0.05907 
       635    0.05984    0.05879    0.05906 
       636    0.05992    0.05879    0.05906 
       637    0.05999    0.05878    0.05906 
       638    0.05992    0.05880    0.05906 
       639    0.06004    0.05878    0.05906 
       640    0.05985    0.05880    0.05906 
       641    0.05987    0.05879    0.05906 significance (train): 2.820 significance: 2.643 
       642    0.06004    0.05878    0.05906 
       643    0.05994    0.05881    0.05906 
       644    0.05992    0.05879    0.05906 
       645    0.06008    0.05878    0.05906 
       646    0.06000    0.05877    0.05907 
       647    0.05985    0.05879    0.05906 
       648    0.05997    0.05878    0.05906 
       649    0.05994    0.05878    0.05906 
       650    0.05993    0.05880    0.05906 
       651    0.05983    0.05878    0.05907 
       652    0.05988    0.05879    0.05906 
       653    0.05990    0.05881    0.05906 
       654    0.05996    0.05880    0.05906 
       655    0.05996    0.05879    0.05906 
       656    0.05996    0.05878    0.05907 
       657    0.05987    0.05878    0.05906 
       658    0.05986    0.05878    0.05906 
       659    0.05987    0.05878    0.05906 
       660    0.05985    0.05877    0.05906 
       661    0.06003    0.05880    0.05906 significance (train): 2.820 significance: 2.650 
       662    0.06000    0.05880    0.05906 
       663    0.06000    0.05879    0.05906 
       664    0.05997    0.05877    0.05907 
       665    0.05995    0.05878    0.05907 
       666    0.05988    0.05879    0.05906 
       667    0.05999    0.05878    0.05907 
       668    0.05996    0.05878    0.05906 
       669    0.05982    0.05878    0.05906 
       670    0.05997    0.05878    0.05906 
       671    0.05995    0.05879    0.05907 
       672    0.05993    0.05877    0.05907 
       673    0.05999    0.05878    0.05907 
       674    0.05989    0.05878    0.05906 
       675    0.06001    0.05877    0.05907 
       676    0.05986    0.05878    0.05906 
       677    0.05999    0.05879    0.05906 
       678    0.06001    0.05877    0.05906 
       679    0.05999    0.05879    0.05906 
       680    0.06000    0.05878    0.05906 
       681    0.05990    0.05879    0.05906 significance (train): 2.819 significance: 2.646 
       682    0.05992    0.05878    0.05906 
       683    0.05983    0.05879    0.05906 
       684    0.05989    0.05878    0.05906 
       685    0.05993    0.05877    0.05907 
       686    0.05989    0.05878    0.05906 
       687    0.05988    0.05879    0.05906 
       688    0.06000    0.05878    0.05906 
       689    0.05999    0.05878    0.05906 
       690    0.05981    0.05877    0.05907 
       691    0.06003    0.05879    0.05906 
       692    0.05992    0.05880    0.05906 
       693    0.06005    0.05878    0.05906 
       694    0.05986    0.05879    0.05906 
       695    0.06003    0.05879    0.05907 
       696    0.05995    0.05879    0.05906 
       697    0.05997    0.05878    0.05906 
       698    0.05987    0.05879    0.05906 
       699    0.05999    0.05878    0.05907 
       700    0.05991    0.05880    0.05906 
set learning rate to: 0.002
       701    0.05994    0.05878    0.05906 significance (train): 2.820 significance: 2.647 
       702    0.06001    0.05878    0.05906 
       703    0.05986    0.05878    0.05906 
       704    0.05997    0.05878    0.05906 
       705    0.06007    0.05879    0.05906 
       706    0.05995    0.05878    0.05906 
       707    0.05987    0.05877    0.05906 
       708    0.05996    0.05878    0.05906 
       709    0.05996    0.05878    0.05906 
       710    0.05992    0.05878    0.05906 
       711    0.05989    0.05877    0.05906 
       712    0.05982    0.05877    0.05906 
       713    0.05990    0.05878    0.05906 
       714    0.06001    0.05878    0.05906 
       715    0.05985    0.05878    0.05906 
       716    0.05993    0.05878    0.05906 
       717    0.05992    0.05879    0.05906 
       718    0.05998    0.05879    0.05906 
       719    0.05995    0.05879    0.05906 
       720    0.05986    0.05878    0.05906 
       721    0.05996    0.05878    0.05906 significance (train): 2.820 significance: 2.645 
       722    0.05989    0.05878    0.05906 
       723    0.05995    0.05878    0.05906 
       724    0.05999    0.05879    0.05906 
       725    0.05987    0.05878    0.05906 
       726    0.05983    0.05878    0.05906 
       727    0.05992    0.05877    0.05906 
       728    0.05989    0.05878    0.05906 
       729    0.05992    0.05878    0.05906 
       730    0.05991    0.05878    0.05906 
       731    0.05996    0.05878    0.05906 
       732    0.05983    0.05878    0.05906 
       733    0.05992    0.05878    0.05906 
       734    0.05982    0.05878    0.05906 
       735    0.05988    0.05878    0.05906 
       736    0.05992    0.05878    0.05906 
       737    0.05988    0.05878    0.05906 
       738    0.05987    0.05878    0.05906 
       739    0.06000    0.05878    0.05906 
       740    0.05998    0.05878    0.05906 
       741    0.05989    0.05878    0.05906 significance (train): 2.825 significance: 2.640 
       742    0.05982    0.05878    0.05907 
       743    0.05988    0.05878    0.05906 
       744    0.05992    0.05878    0.05906 
       745    0.05986    0.05878    0.05906 
       746    0.05986    0.05877    0.05906 
       747    0.05996    0.05878    0.05906 
       748    0.05992    0.05877    0.05906 
       749    0.05986    0.05878    0.05906 
       750    0.05996    0.05878    0.05906 
       751    0.05996    0.05878    0.05906 
       752    0.05997    0.05877    0.05906 
       753    0.05993    0.05878    0.05906 
       754    0.05999    0.05878    0.05906 
       755    0.05998    0.05878    0.05906 
       756    0.05987    0.05878    0.05906 
       757    0.05999    0.05879    0.05906 
       758    0.05993    0.05878    0.05906 
       759    0.05984    0.05878    0.05906 
       760    0.05980    0.05877    0.05906 
       761    0.05989    0.05877    0.05906 significance (train): 2.831 significance: 2.637 
       762    0.06000    0.05878    0.05906 
       763    0.05984    0.05878    0.05906 
       764    0.05991    0.05878    0.05906 
       765    0.05996    0.05878    0.05906 
       766    0.05987    0.05878    0.05906 
       767    0.05997    0.05877    0.05906 
       768    0.05995    0.05878    0.05906 
       769    0.05996    0.05878    0.05906 
       770    0.05996    0.05878    0.05906 
       771    0.05985    0.05878    0.05906 
       772    0.05989    0.05878    0.05906 
       773    0.05992    0.05878    0.05906 
       774    0.05986    0.05877    0.05906 
       775    0.06000    0.05878    0.05906 
       776    0.05994    0.05878    0.05906 
       777    0.06000    0.05878    0.05906 
       778    0.05995    0.05879    0.05906 
       779    0.05991    0.05878    0.05906 
       780    0.05998    0.05879    0.05906 
       781    0.05998    0.05878    0.05906 significance (train): 2.824 significance: 2.645 
       782    0.05988    0.05878    0.05906 
       783    0.05972    0.05877    0.05906 
       784    0.05994    0.05877    0.05907 
       785    0.05989    0.05878    0.05906 
       786    0.06000    0.05878    0.05906 
       787    0.05995    0.05878    0.05906 
       788    0.05985    0.05877    0.05906 
       789    0.05995    0.05878    0.05906 
       790    0.05974    0.05877    0.05907 
       791    0.05993    0.05878    0.05907 
       792    0.05993    0.05877    0.05906 
       793    0.05989    0.05877    0.05906 
       794    0.06003    0.05878    0.05906 
       795    0.05999    0.05877    0.05906 
       796    0.05986    0.05877    0.05906 
       797    0.05993    0.05877    0.05907 
       798    0.05995    0.05877    0.05906 
       799    0.05991    0.05878    0.05906 
       800    0.05985    0.05877    0.05906 
set learning rate to: 0.001
       801    0.05981    0.05877    0.05906 significance (train): 2.825 significance: 2.641 
       802    0.05980    0.05877    0.05906 
       803    0.05988    0.05877    0.05906 
       804    0.05988    0.05878    0.05906 
       805    0.05993    0.05878    0.05906 
       806    0.06000    0.05878    0.05906 
       807    0.05988    0.05878    0.05906 
       808    0.05988    0.05878    0.05906 
       809    0.05990    0.05877    0.05906 
       810    0.05992    0.05878    0.05906 
       811    0.05991    0.05878    0.05906 
       812    0.05996    0.05878    0.05906 
       813    0.05987    0.05877    0.05906 
       814    0.06001    0.05878    0.05906 
       815    0.05989    0.05878    0.05906 
       816    0.05990    0.05877    0.05906 
       817    0.05996    0.05878    0.05906 
       818    0.05987    0.05878    0.05906 
       819    0.05998    0.05878    0.05906 
       820    0.05991    0.05878    0.05906 
       821    0.05999    0.05878    0.05906 significance (train): 2.823 significance: 2.644 
       822    0.05998    0.05877    0.05906 
       823    0.06000    0.05878    0.05906 
       824    0.05984    0.05878    0.05906 
       825    0.06000    0.05878    0.05906 
       826    0.05988    0.05878    0.05906 
       827    0.05991    0.05878    0.05906 
       828    0.05995    0.05877    0.05906 
       829    0.05996    0.05878    0.05906 
       830    0.05985    0.05878    0.05906 
       831    0.05991    0.05877    0.05906 
       832    0.05999    0.05878    0.05906 
       833    0.05990    0.05878    0.05906 
       834    0.05990    0.05878    0.05906 
       835    0.05998    0.05877    0.05906 
       836    0.05997    0.05878    0.05906 
       837    0.05981    0.05877    0.05906 
       838    0.05986    0.05878    0.05906 
       839    0.05998    0.05878    0.05906 
       840    0.05988    0.05878    0.05906 
       841    0.05986    0.05877    0.05906 significance (train): 2.825 significance: 2.644 
       842    0.05975    0.05877    0.05906 
       843    0.05994    0.05878    0.05906 
       844    0.05983    0.05877    0.05906 
       845    0.05990    0.05877    0.05906 
       846    0.05993    0.05877    0.05906 
       847    0.05990    0.05878    0.05906 
       848    0.06001    0.05878    0.05906 
       849    0.05991    0.05878    0.05906 
       850    0.05986    0.05878    0.05906 
       851    0.05985    0.05878    0.05906 
       852    0.06003    0.05878    0.05906 
       853    0.05989    0.05878    0.05906 
       854    0.05996    0.05878    0.05906 
       855    0.05989    0.05878    0.05906 
       856    0.05984    0.05878    0.05906 
       857    0.05993    0.05878    0.05906 
       858    0.05991    0.05878    0.05906 
       859    0.05997    0.05878    0.05906 
       860    0.05978    0.05877    0.05906 
       861    0.05992    0.05877    0.05906 significance (train): 2.825 significance: 2.641 
       862    0.05991    0.05877    0.05906 
       863    0.05994    0.05878    0.05906 
       864    0.05995    0.05878    0.05906 
       865    0.05985    0.05877    0.05906 
       866    0.05989    0.05877    0.05906 
       867    0.05989    0.05877    0.05906 
       868    0.05987    0.05877    0.05906 
       869    0.05994    0.05877    0.05906 
       870    0.05992    0.05878    0.05906 
       871    0.05992    0.05878    0.05906 
       872    0.05995    0.05878    0.05906 
       873    0.06006    0.05878    0.05906 
       874    0.05992    0.05878    0.05906 
       875    0.05989    0.05877    0.05906 
       876    0.05994    0.05877    0.05906 
       877    0.05996    0.05878    0.05906 
       878    0.05991    0.05878    0.05906 
       879    0.05996    0.05877    0.05906 
       880    0.05982    0.05877    0.05906 
       881    0.05985    0.05877    0.05906 significance (train): 2.826 significance: 2.645 
       882    0.05987    0.05877    0.05906 
       883    0.05986    0.05878    0.05906 
       884    0.05990    0.05878    0.05906 
       885    0.05993    0.05878    0.05906 
       886    0.05988    0.05878    0.05906 
       887    0.06004    0.05878    0.05906 
       888    0.05988    0.05878    0.05906 
       889    0.05990    0.05878    0.05906 
       890    0.05979    0.05877    0.05906 
       891    0.05993    0.05878    0.05907 
       892    0.05984    0.05878    0.05906 
       893    0.05995    0.05878    0.05906 
       894    0.05990    0.05878    0.05906 
       895    0.05991    0.05878    0.05906 
       896    0.05988    0.05878    0.05906 
       897    0.05993    0.05878    0.05906 
       898    0.05989    0.05877    0.05906 
       899    0.06001    0.05878    0.05906 
       900    0.05991    0.05878    0.05906 
       901    0.05998    0.05878    0.05906 significance (train): 2.822 significance: 2.646 
       902    0.05994    0.05878    0.05906 
       903    0.05985    0.05878    0.05906 
       904    0.05983    0.05877    0.05906 
       905    0.05996    0.05877    0.05906 
       906    0.05984    0.05877    0.05906 
       907    0.05995    0.05877    0.05906 
       908    0.05994    0.05877    0.05906 
       909    0.05991    0.05877    0.05906 
       910    0.05978    0.05877    0.05906 
       911    0.05990    0.05877    0.05906 
       912    0.05992    0.05878    0.05906 
       913    0.05985    0.05877    0.05906 
       914    0.05994    0.05878    0.05906 
       915    0.05991    0.05878    0.05906 
       916    0.05995    0.05877    0.05906 
       917    0.05983    0.05877    0.05906 
       918    0.05991    0.05877    0.05906 
       919    0.05992    0.05878    0.05906 
       920    0.05989    0.05878    0.05906 
       921    0.05994    0.05877    0.05906 significance (train): 2.826 significance: 2.640 
       922    0.05994    0.05877    0.05906 
       923    0.05996    0.05878    0.05906 
       924    0.05991    0.05877    0.05906 
       925    0.05986    0.05877    0.05906 
       926    0.06000    0.05878    0.05906 
       927    0.05987    0.05878    0.05906 
       928    0.05989    0.05877    0.05906 
       929    0.05996    0.05877    0.05906 
       930    0.05996    0.05877    0.05906 
       931    0.05987    0.05877    0.05906 
       932    0.05984    0.05878    0.05906 
       933    0.05988    0.05877    0.05906 
       934    0.05989    0.05877    0.05906 
       935    0.05985    0.05877    0.05906 
       936    0.05988    0.05877    0.05906 
       937    0.06000    0.05878    0.05906 
       938    0.05989    0.05878    0.05906 
       939    0.06000    0.05878    0.05906 
       940    0.05992    0.05878    0.05906 
       941    0.05981    0.05877    0.05906 significance (train): 2.822 significance: 2.642 
       942    0.05989    0.05877    0.05906 
       943    0.05995    0.05877    0.05906 
       944    0.05992    0.05877    0.05906 
       945    0.05991    0.05877    0.05906 
       946    0.05987    0.05877    0.05906 
       947    0.05988    0.05877    0.05906 
       948    0.05987    0.05877    0.05906 
       949    0.05984    0.05877    0.05906 
       950    0.05996    0.05877    0.05906 
       951    0.05989    0.05877    0.05906 
       952    0.05984    0.05877    0.05906 
       953    0.05989    0.05877    0.05906 
       954    0.05988    0.05878    0.05906 
       955    0.05985    0.05877    0.05906 
       956    0.06003    0.05877    0.05906 
       957    0.05997    0.05877    0.05906 
       958    0.05988    0.05877    0.05906 
       959    0.05995    0.05878    0.05906 
       960    0.06005    0.05878    0.05906 
       961    0.05979    0.05877    0.05906 significance (train): 2.820 significance: 2.645 
       962    0.05992    0.05877    0.05906 
       963    0.05994    0.05877    0.05906 
       964    0.05989    0.05878    0.05906 
       965    0.05987    0.05877    0.05906 
       966    0.05992    0.05877    0.05906 
       967    0.05995    0.05877    0.05906 
       968    0.05986    0.05877    0.05906 
       969    0.05988    0.05877    0.05906 
       970    0.05985    0.05877    0.05906 
       971    0.05996    0.05877    0.05906 
       972    0.05992    0.05877    0.05906 
       973    0.05985    0.05877    0.05906 
       974    0.05989    0.05877    0.05906 
       975    0.05989    0.05877    0.05906 
       976    0.05990    0.05877    0.05906 
       977    0.05982    0.05877    0.05906 
       978    0.05996    0.05877    0.05906 
       979    0.06000    0.05878    0.05906 
       980    0.05991    0.05877    0.05906 
       981    0.05983    0.05877    0.05906 significance (train): 2.822 significance: 2.646 
       982    0.05989    0.05878    0.05906 
       983    0.05993    0.05877    0.05906 
       984    0.05999    0.05878    0.05906 
       985    0.05982    0.05877    0.05906 
       986    0.05989    0.05877    0.05906 
       987    0.05991    0.05877    0.05906 
       988    0.05993    0.05878    0.05906 
       989    0.05984    0.05877    0.05906 
       990    0.05994    0.05878    0.05906 
       991    0.05983    0.05877    0.05906 
       992    0.05987    0.05877    0.05906 
       993    0.05996    0.05877    0.05906 
       994    0.05989    0.05878    0.05906 
       995    0.06001    0.05878    0.05906 
       996    0.05993    0.05877    0.05906 
       997    0.05989    0.05877    0.05906 
       998    0.05991    0.05877    0.05906 
       999    0.05983    0.05877    0.05906 
      1000    0.05995    0.05877    0.05906 significance (train): 2.823 significance: 2.646 
FINAL RESULTS:       1000   0.059949   0.059063 significance (train): 2.823 significance: 2.646 
TRAINING TIME: 0:22:18.985826 (1339.0 seconds)
GRADIENT UPDATES: 151000
MIN TEST LOSS: 0.05905484426682937
training done.
> results//FINAL_VHLegacy_0lep_WP_SR_medhigh_Znn/Zvv2017_SR_medhigh_Znn_191022_V11finalVarsWP.h5/512-256-128-64-64-64/0.20-0.50-0.50-0.50-0.50-0.50/1.000e+00/rnd_1/checkpoints/model.ckpt
saved checkpoint to [34m results//FINAL_VHLegacy_0lep_WP_SR_medhigh_Znn/Zvv2017_SR_medhigh_Znn_191022_V11finalVarsWP.h5/512-256-128-64-64-64/0.20-0.50-0.50-0.50-0.50-0.50/1.000e+00/rnd_1/checkpoints/model.ckpt [0m
LOSS(train, unmodified):  0.05877466532146921
LOSS(test):               0.05906300016700969
---
S    B
---
 0.57 881.18
 1.76 1274.62
 2.91 1143.63
 3.67 1021.64
 4.70 950.43
 5.96 868.67
 6.70 662.68
 5.83 436.77
 6.11 340.31
 7.33 375.04
 9.59 339.68
11.74 302.51
15.24 263.14
20.04 174.98
11.90 48.25
---
significance: 2.646 
area under ROC: AUC_test =  83.64032261318735
area under ROC: AUC_train =  84.49733916395762
INFO: set range to: 90.001884 149.99982
INFO: set range to: 120.00072 1065.798
INFO: set range to: 170.00119 1158.9447
INFO: set range to: 2.0000293 3.1415925
INFO: set range to: 2.0 3.0
INFO: set range to: 1.0 3.0
INFO: set range to: 0.0 2.0849
INFO: set range to: 0.0 2.826154
INFO: set range to: 38.97972 1027.864
INFO: set range to: 35.000088 298.75113
INFO: set range to: -1.0 16.0
INFO: set range to: 0.0 1.0
INFO: set range to: -99.0 3.0
INFO: set range to: -99.0 899.48395
INFO: set range to: -99.0 3.1415594
-------------------------
with optimized binning:
 method: SB
 target: 0.1090, 0.1194, 0.1242, 0.1226, 0.1148, 0.1021, 0.0861, 0.0690, 0.0524, 0.0378, 0.0259, 0.0168, 0.0104, 0.0061, 0.0034
 bins:   0.0000, 0.0725, 0.1300, 0.1964, 0.2692, 0.3441, 0.4181, 0.5141, 0.6303, 0.7195, 0.7905, 0.8447, 0.8883, 0.9233, 0.9511, 1.0000
-------------------------
---
S    B
---
 0.70 1001.66
 1.51 1096.56
 2.89 1139.69
 3.97 1124.00
 5.55 1050.34
 6.78 934.62
 9.11 782.84
10.80 623.87
11.58 470.61
12.23 335.75
11.39 226.78
11.47 143.32
10.93 84.84
 8.41 48.13
 6.74 20.55
---
significance: 2.705 (for optimized binning)
significance: 2.686 ( 1% background uncertainty, for optimized binning)
significance: 2.398 ( 5% background uncertainty, for optimized binning)
significance: 2.000 (10% background uncertainty, for optimized binning)
significance: 1.681 (15% background uncertainty, for optimized binning)
significance: 1.432 (20% background uncertainty, for optimized binning)
[32mPLOTS: use n=S+B Asimov data in the plots![0m
[32mPLOTS: use n=S+B Asimov data in the plots![0m
[32mPLOTS: use n=S+B Asimov data in the plots![0m
INFO: search optimal cut position for sensitivity
optimal position for analysis based on single cut on score > x:
AMS_cut_position = 0.8490371704101562
AMC_cut_significance = 2.142894779683771
INFO: convert to histogram
