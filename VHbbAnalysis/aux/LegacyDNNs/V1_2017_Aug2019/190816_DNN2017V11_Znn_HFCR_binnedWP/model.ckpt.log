saving logfile to [34m results//V11_0lep_Znn_HFCR_binnedWP/Zvv2017_BDT_Znn_HFCR_190624_V11base.h5/32-32-32-32-32-32-32-32/0.10-0.10-0.10-0.10-0.10-0.10-0.10-0.10/1.000e-04/rnd_1/output.txt [0m
INFO: numpy random state =  MT19937 ,e118ad3a,cdd35d75,c8a2b14a,4b7585f8,9208e04c,166d506d,1a2fc568,ab3476,2ae1b589,a92b46c1,e2b9e798,255ab4e9,bb0996c6,3b38559d,ac3b84e0,68b3cfbb,60fde3f9,7fdc94fe,b77d4313,88876417,9d367648,54ab57b9,61e23ae,25993749,374bdf5,fa8b130b,fa8dc671,86bd3dcb,af5b181b,1452ab4d,773608f1,810de673,611f8c36,11985b69,34241929,e440a080,b067e8d3,a5d5b762,275862e1,f256187d,9dc73b7,31a0b1fd,4c8a9497,71244b4e,e5396c48,b2552e18,52bc20bc,7d9ff631,c94dcd6,5de524d,96db83ca,e5240c25,e08d3217,e2f9a5ed,fd74f835,423e91f8,f0745e45,bdcfd994,814f41cf,59863be6,e29c75dd,c0a361e,e61fbadc,1e80f2a,87d73983,8964625d,b67e9ce7,dd4752e9,ace00fe4,5b5ed5f7,906352d3,fb646191,e3448414,72011c55,ca7edc6a,c430bc46,5b92f102,53814ffa,ad376a34,49aae56a,2092d087,35191c35,a8b0dbfa,51859cf,1c520309,c6847d76,54b646f0,fcf3717b,ad50130f,3e87dcd3,c2d9a4c9,77034ffc,d81bd36f,b854225f,2bd95529,457d9ba9,4da0aef,dda0d1a4,a21bd838,aae8018,18480a9c,9ba0a3ac,43536ebb,f40a411d,5dccaff6,2cf8ab4b,6eeb8fca,bf00aeb9,c358f964,8c52ae7c,59a2da36,84527903,dafd7ae9,4bf6153f,6c0251d4,4528520d,99a54c19,8787659d,ca227ea,412201e3,c66622c4,5ae28ec8,aa760400,53110aab,b310260,b3e606e9,c41d76c3,4961313,2b9fc70c,520f63,155b6a4d,da6035a,5d7d7790,dee7c446,9317b19f,60e7301,e3417555,ecb66d6f,adfce7f1,48638690,af3ed063,87739c58,cd29671e,25e90aea,874724c8,7b6ac731,4669fd51,1004c2a9,1d9094e4,161ae27b,9ccc7928,7c646368,a18b598e,50340132,64a4689,dba9b1e1,55a3c30a,6c6a5593,df740672,caf0a569,46d613b,749f8dce,e0b02e21,4170b61c,1368d61,70bafe87,38d39ccc,fb40723c,9b34b162,4d1ae0d3,996933c,65491df6,cf3bcff5,97280974,da363be9,b74e486a,b14b19a0,c227df01,a552dc7,13ea6aa7,cd67cfd6,7d98ebaf,39b4a45f,5a9065ad,225e2161,226f884e,e2ead293,5f1b317a,6ddf3d80,942d5de0,a50d6a22,4980540e,5270e78c,fc3237d,71a33fd0,a5a366cb,1ccd9d4e,a788cc77,812b5b13,e7a356ba,75746a6b,f4be4059,3f7d9107,19341f79,c9278d8d,4db21412,52b218db,4ed2b72b,174ca18f,aef19610,eaf47d43,574fff03,d2d1f6ad,f83526e4,d9d92593,65cfab45,5875e462,9d6424db,ccd5a28d,5c8b25e4,a040bcf6,212c94e9,ad6e3d66,279cf009,874830b4,f1e1b9c7,2e0f0af,9344254f,d25fb8a7,845050ea,e03813a1,b03c0b73,65f44a25,9655528c,2befef52,5ed1419c,d05e034f,5db7db9c,1470b4e9,ff6c308f,8a37d16b,8639a6e8,be168046,5416a0f3,db274f26,2b2c08f8,ce7b5a29,a8fe17f6,dde1024,967fd3f7,5121381e,5c9ea2e9,ba96e96b,917986bd,8085c8ef,ea32b409,7d146209,d85d3411,e8693060,fea0980b,369fdc31,8a889a6a,e127a2e1,c7b580f5,cef41c1b,a3904c7e,6c189c7a,50576212,807febfa,bb054376,e3fe53d9,e432402,1b7efd29,20e188f6,a2dcea40,f7b26a7f,bd744e89,3584ea8f,8d4d26e0,fff12e6e,7f0d96e4,5cef3426,4d6bd2e3,4da7684c,9f47287a,8fbcf174,363ba4cf,f19bf8ca,2b3ce070,e9f4de6b,aca0ed9d,af711f0d,62b3ede2,15878a67,212affc3,91871e1f,1b3661fd,112efb12,7c3c827d,2a544f18,a2bcd5cd,196383f1,a2fdb464,443def6a,fdbadb1d,97d37247,cef42ed2,61173ca2,fc56d425,ef642469,f8801d1e,8a9011c,3dc08ad7,6958bb55,57d370db,453a545f,351dd147,55f2063c,66f2410a,f56b8b09,60d9ea95,cca06863,816410f1,f5334503,dd3e1470,8677a33f,5baafd93,70aa819b,a19d8bc6,9f3b3f4d,e3a93522,cd377724,a3ea00bf,50299a3,8a7dd6e,8efe348e,115d181b,60a2698c,6161cfc0,b903bd33,842fc207,4279f44f,e2269586,b67f80e0,5733529c,b6afe883,416a2fab,ee63dc46,5593466f,d844eb30,1801adda,8e3efe6e,c2c6a806,9c255a94,98e6c67b,86a1073e,b00ff598,34276549,cfab6aca,ad169f83,ac3a204e,2a8e0727,690f234d,1ff417f3,2e4a8595,fd408a0e,b5c01085,d2c8875c,94cd0a19,1e1ccd02,98cc9113,711f46d2,12fdee60,b788fc43,f02e925,7f23ede1,a97a6753,4a92c7b,ad0d17c4,1ac22b81,7e05c8d6,24c8677e,c03e67b2,83ca8cad,23874ee,e544aecf,83fba304,d723ed13,d749f51,c684a49f,6829ff1,5dd70699,956f368d,69d44170,ec2fdf59,4237fd3a,6b31dc72,aef69e17,3504f7ba,b5360442,9800b47b,2c891c2c,17aecffe,3a42f178,6c3b0993,bf8aa5fa,dfcc937a,61fef635,b2505d22,d66813cc,52525778,76ed163e,1667cf3b,842e39b3,39041a87,3d129783,834d7038,e922ddad,2b08472,cf20e056,58b87d9,ba84d812,62392d77,7ff07883,e738a4a6,2df0942b,9933972c,b30a6db1,2c96b8d7,a788278d,93b487dd,2f296246,7eee8ae1,92df88e5,27b27a1f,80a5eb0c,87d03bed,97323ef4,a8497f46,d55d4a26,1d206908,2d09280b,697bccb0,ff54a3eb,507c2bfd,90d50a77,10aead29,7b24bb84,f39289a2,2ecb43c3,24228ad1,297b19cb,83b9a49,a254c58f,dca5f252,45b04e49,74f13f41,ca125e36,bbc8e8ba,2ab9acfc,a86f897,b6d796e0,8b691826,6e032756,586d2c80,cbacab79,11704c94,f9c8df6f,480e6ace,ec6e0571,72d4e15c,b5812bff,fb3ad002,7de61c5f,d6bf213e,a3fb2f46,96fef979,f7a0ba24,435702b5,54e7f107,8c6a7586,ba88e1ff,70fd5ee5,654681b2,903abdea,bbf252aa,dc4875fa,5b83b5ff,3b24b163,921c8c6b,da761a88,4e332b2,a5c486c5,33f81d8d,b84fe5f8,9ca0411e,23036c23,a1eb6fde,d0720712,47465f93,8f2ea95b,2472806b,346ee495,f1c8591d,d3a2b532,ce45a8fc,4bc79ce2,3c7ac00c,f27cae63,b92db9e5,fb480d82,840637ce,4561a428,93505bb,538e8867,88622c5d,4ce4955c,ff8bb0ea,d23a996e,34a513dd,fbc1be67,241cd65,44212bca,8d84e019,41a28147,fe8bbd7e,32eb1fb2,23897d31,fa5a8763,156b6f8e,d855b168,f79fef72,f1d2d550,ed76e06f,10d8ae63,6689dd9e,a49cc6d6,40c8787f,659b35d2,6152206,8dd0bccf,de5c1ebb,9957de8f,8b326c47,ee0c98d8,fc1bc26e,9cc085ee,da2fdf6d,639be6a1,efb133a2,f82d080f,2d95a0c4,955f43de,88210224,74b5c66e,2e09f09f,88e974e5,53661597,9df9e832,cf75a7ca,f83db252,1c4d22d8,72bdd6ae,ad383e74,93d000c3,34762149,14455ba3,624d0079,a63380c4,dce5ae49,6664cf83,c6d4bb15,259f41e1,9155ccb9,26930c7d,824953ae,82e86f43,fb9812ec,fcd3a17a,f4ff259,fe82cdfe,f6e92d22,7ac7e64e,f638c80f,82d2ac99,2033e5ae,e6cc67ec,69599049,a18b2358,a0d0dec3,26501216,77b428a7,f499e76e,ad2fffa7,2bfd6a19,d43c60fd,79b6abaf,117eff56,771e3e63,395aec97,4db9b29,bfbb93fc,ef555926,7ad7c64f,c28f4ed8,db8cf21,51547928,293ed5a,d8dc4dae,7f284e63,51875081,8f95e80,71fbdf37,4135424a,48db1f0d,861dc69,8591e613,e29e082a,6a53f476,4ba0bdb5,8dab5fba,592892d6,1a95dd67,c75641f9
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------
initialized TensorflowDNNClassifier, version v0.1 ( 12006 )
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------
INFO: command: /var/lib/slurm-llnl/slurmd/job07227/slurm_script -i /data/VHbb/2017/Znn/Zvv2017_BDT_Znn_HFCR_190624_V11base.h5 -c /shome/berger_p2/tfVHbbDNN/results/tfZllDNN/Zvv2017_BDT_Znn_HFCR_190624_V11base.h5/32-32-32-32-32-32-32-32/0.10-0.10-0.10-0.10-0.10-0.10-0.10-0.10/1.000e-04/rnd_4/config.cfg -p V11_0lep_Znn_HFCR_binnedWP --set='ignoreNegativeWeights=True;plot-roc=False;preprocess="np.concatenate((x[:4],np.atleast_1d(np.piecewise(x[4],[x[4]<0.1522,(x[4]>=0.1522)&(x[4]<0.4941),(x[4]>=0.4941)&(x[4]<0.8001),x[4]>=0.8001],[0,1,2,3])),np.atleast_1d(np.piecewise(x[5],[x[5]<0.1522,(x[5]>=0.1522)&(x[5]<0.4941),(x[5]>=0.4941)&(x[5]<0.8001),x[5]>=0.8001],[0,1,2,3])),x[6:]))";'
INFO: DATA included in H5 file, can make DATA/MC plots!
INFO: read inputs from disk, metadata is pesent:
INFO:  >   cut (min(MHT_pt, MET_pt) > 100 && Jet_btagDeepB[hJidx[1]] > 0.1522 && H_mass < 500 && H_pt > 120.0 && (isWmunu || isWenu || isZnn) && max(Jet_PtReg[hJidx[0]],Jet_PtReg[hJidx[1]]) > 60.0 && min(Jet_PtReg[hJidx[0]],Jet_PtReg[hJidx[1]]) > 35.0) && isZnn && abs(TVector2::Phi_mpi_pi(H_phi-MET_Phi)) > 2.0 && Sum$(abs(TVector2::Phi_mpi_pi(Jet_phi-V_phi))<0.5&&Jet_Pt>30&&Jet_puId>0&&Jet_lepFilter)==0 && (H_mass < 60 || H_mass > 160) && Jet_btagDeepB[hJidx[0]] > 0.8001 && abs(TVector2::Phi_mpi_pi(MET_Phi-TkMET_phi)) < 0.5 && Sum$(Jet_Pt>30&&abs(Jet_eta)<2.4&&Jet_puId>6&&Jet_lepFilter&&Iteration$!=hJidx[0]&&Iteration$!=hJidx[1])==0
INFO:  >   cutName HighPt_Zbb
INFO:  >   region BDT_Znn_HFCR
INFO:  >   samples {u'ZLIGHT': [u'WJetsHT100_0b', u'WJetsHT200_0b', u'WJetsHT400_0b', u'WJetsHT600_0b', u'WJetsHT800_0b', u'WJetsHT1200_0b', u'WBJets100_0b', u'WBJets200_0b', u'WBGenFilter100_0b', u'WBGenFilter200_0b', u'ZJetsHT100_0b', u'ZJetsHT200_0b', u'ZJetsHT400_0b', u'ZJetsHT600_0b', u'ZJetsHT800_0b', u'ZJetsHT2500_0b', u'ZBJets100_0b', u'ZBJets200_0b', u'ZBGenFilter100_0b', u'ZBGenFilter200_0b', u'WWnlo_0b', u'WZnlo_0b', u'ZZ_0b', u'WWnlo_1b', u'WWnlo_2b', u'WZnlo_1b', u'WZnlo_2b', u'ZZ_1b', u'ZZ_2b', u'ZH_Zll', u'ZH_Znunu', u'ggZH_Zll', u'ggZH_Znunu', u'WplusH', u'WminusH'], u'TT': [u'TT_2l2n', u'TT_h', u'TT_Sl'], u'ZBB': [u'WJetsHT100_2b', u'WJetsHT200_2b', u'WJetsHT400_2b', u'WJetsHT600_2b', u'WJetsHT800_2b', u'WJetsHT1200_2b', u'WBJets100_2b', u'WBJets200_2b', u'WBGenFilter100_2b', u'WBGenFilter200_2b', u'ZJetsHT100_2b', u'ZJetsHT200_2b', u'ZJetsHT400_2b', u'ZJetsHT600_2b', u'ZJetsHT800_2b', u'ZJetsHT2500_2b', u'ZBJets100_2b', u'ZBJets200_2b', u'ZBGenFilter100_2b', u'ZBGenFilter200_2b'], u'ZB': [u'WJetsHT100_1b', u'WJetsHT200_1b', u'WJetsHT400_1b', u'WJetsHT600_1b', u'WJetsHT800_1b', u'WJetsHT1200_1b', u'WBJets100_1b', u'WBJets200_1b', u'WBGenFilter100_1b', u'WBGenFilter200_1b', u'ZJetsHT100_1b', u'ZJetsHT200_1b', u'ZJetsHT400_1b', u'ZJetsHT600_1b', u'ZJetsHT800_1b', u'ZJetsHT2500_1b', u'ZBJets100_1b', u'ZBJets200_1b', u'ZBGenFilter100_1b', u'ZBGenFilter200_1b'], u'ST': [u'ST_tW_antitop', u'ST_tW_top', u'ST_s-channel_4f', u'ST_t-channel_top_4f', u'ST_t-channel_antitop_4f']}
INFO:  >   systematics []
INFO:  >   testCut ((event%2)==0||isData)
INFO:  >   trainCut !((event%2)==0||isData)
INFO:  >   variables H_mass H_pt MET_Pt abs(TVector2::Phi_mpi_pi(H_phi-V_phi)) Jet_btagDeepB[hJidx[0]] Jet_btagDeepB[hJidx[1]] abs(Jet_eta[hJidx[0]]-Jet_eta[hJidx[1]]) abs(TVector2::Phi_mpi_pi(Jet_phi[hJidx[0]]-Jet_phi[hJidx[1]])) max(Jet_PtReg[hJidx[0]],Jet_PtReg[hJidx[1]]) min(Jet_PtReg[hJidx[0]],Jet_PtReg[hJidx[1]]) SA5 Sum$(Jet_Pt>30&&abs(Jet_eta)<2.4&&Jet_puId>6&&Iteration$!=hJidx[0]&&Iteration$!=hJidx[1]) MaxIf$(Jet_btagDeepB,Jet_Pt>30&&abs(Jet_eta)<2.4&&Jet_puId>6&&Iteration$!=hJidx[0]&&Iteration$!=hJidx[1]) MaxIf$(Jet_Pt,Jet_Pt>30&&abs(Jet_eta)<2.4&&Jet_puId>6&&Iteration$!=hJidx[0]&&Iteration$!=hJidx[1]) MinIf$(abs(TVector2::Phi_mpi_pi(Jet_phi-V_phi))-3.1415,Jet_Pt>30&&abs(Jet_eta)<2.4&&Jet_puId>6)
INFO:  >   version 3
INFO:  >   weightF genWeight * puWeight * bTagWeightDeepCSV * 1.0 * EWKw[0] * weightLOtoNLO * 1.0 * ((isZnn * weight_mettrigSF) + (isWmunu * muonSF[0]) + (isWenu * electronSF[0])) * FitCorrV2[0] * 1.0
INFO:  >   weightSYS []
INFO: random state: (3, (2147483648L, 1820554349L, 2182160589L, 1046322966L, 3861240257L, 2334298850L, 818871261L, 3650327569L, 1934867658L, 1411959443L, 3552613842L, 1606938389L, 1840214163L, 750737534L, 2936573764L, 3746364264L, 3142497210L, 91206039L, 1004784553L, 2746155647L, 2448678729L, 4170394047L, 195932667L, 2704289983L, 3066647937L, 3839854493L, 1863959775L, 1797666184L, 517585192L, 2729807356L, 2545659994L, 957025890L, 687305202L, 3145269577L, 497048052L, 3911389211L, 409048941L, 4223326093L, 1521964107L, 1663192867L, 456994659L, 786072758L, 1588246979L, 517603033L, 2963893236L, 1016442204L, 2072789957L, 1550182936L, 4192771638L, 67826338L, 2620852786L, 4213783967L, 1764404400L, 3586029398L, 865134865L, 1916075397L, 225040173L, 2907959908L, 2303965320L, 2119619073L, 1211565678L, 2937186457L, 2856537896L, 657217556L, 4149518152L, 1139436304L, 3970976600L, 2399895599L, 2437512072L, 3616046697L, 974477750L, 2642651862L, 1896498245L, 1197249351L, 2156889442L, 1538720102L, 746872261L, 2370061896L, 2139087733L, 920848052L, 4270777826L, 3950915349L, 3737941335L, 304284867L, 3203655272L, 4091569316L, 2161292428L, 312328477L, 3322399322L, 629289428L, 2720095247L, 3010546319L, 2176792366L, 2543560168L, 1459029131L, 3066039219L, 2484006668L, 986491866L, 3860391090L, 3471032727L, 4135317432L, 1621253076L, 1059660727L, 2600012342L, 3275632316L, 1564836681L, 1208403496L, 2297540062L, 2901525564L, 891684885L, 1030858879L, 2983979880L, 2629562755L, 2489295132L, 2720799249L, 2140148343L, 955240408L, 2633223268L, 2789225386L, 3120491307L, 1223965174L, 524956514L, 1948811098L, 21756910L, 3735007678L, 842371853L, 1236037584L, 667806430L, 1483362266L, 68718949L, 1329678116L, 3641715954L, 485398864L, 2478721064L, 1926091020L, 2718805309L, 1069986725L, 2108111979L, 2219650266L, 2449125757L, 562870062L, 2819688211L, 2849273207L, 2565185718L, 4278994439L, 458972004L, 3746487858L, 2389423556L, 3259556092L, 2318503902L, 1354912181L, 2329803681L, 780774603L, 2641092889L, 108070843L, 3334884343L, 3221658141L, 92723407L, 488859109L, 2438530821L, 2161735701L, 175930415L, 3123940322L, 2210592053L, 3536857952L, 1732466853L, 987247394L, 3390032583L, 4227377414L, 3730241337L, 2982607698L, 4287050144L, 997663617L, 2253070064L, 1809832925L, 957809222L, 429524338L, 907522881L, 3378815904L, 2673537221L, 1156470336L, 837506967L, 1089425657L, 3822553834L, 3461066696L, 3163923215L, 419502926L, 2704643243L, 2869838360L, 2137753121L, 2142472487L, 2557424739L, 2092827611L, 1309622932L, 3546919644L, 4785902L, 1124387597L, 617677300L, 3219467292L, 4209962787L, 92733792L, 2498267403L, 4260134050L, 576453194L, 768888990L, 4263828593L, 1008228825L, 1427627489L, 2066801465L, 3050804509L, 1880319747L, 4029405136L, 895854645L, 3797260056L, 1498754763L, 3835066989L, 504970797L, 1829555273L, 3160304543L, 2354351858L, 3521836538L, 1644301096L, 604776046L, 3434923092L, 1329186263L, 3941654046L, 2445724051L, 3440302224L, 2281508391L, 2227653883L, 503651169L, 394359807L, 3488704990L, 1200004360L, 2135082429L, 783299560L, 2373707389L, 937629347L, 1389881729L, 1965411728L, 1611226460L, 4240153326L, 303668018L, 3782822007L, 3768769612L, 3774763899L, 3103246786L, 2692573431L, 869823898L, 856945886L, 838494986L, 1443741974L, 3195591587L, 3850065669L, 3383471704L, 1269646759L, 834794932L, 783209583L, 2543372641L, 302195518L, 1087423834L, 2369183676L, 561305671L, 1831828142L, 617629650L, 4017384427L, 4039064798L, 1032204782L, 2429510120L, 1907593457L, 3375029434L, 370667957L, 300087381L, 3457884971L, 3519815323L, 3868131064L, 1688315012L, 3192263340L, 803781624L, 1358180962L, 453504214L, 3003505834L, 223202694L, 38566930L, 432055954L, 2678081821L, 3174739048L, 3463610268L, 1660533633L, 3724629110L, 1505491486L, 258921088L, 2769897955L, 3521635707L, 2505945683L, 2436332483L, 1867643977L, 1997480440L, 696268568L, 3832851606L, 1408271295L, 3851527853L, 1221283732L, 495587423L, 1030130217L, 2185218053L, 2432545350L, 33429057L, 4062454414L, 3584797194L, 808285526L, 4188558310L, 1424530337L, 3157003205L, 417664348L, 3999445858L, 4164289263L, 4093516341L, 843339348L, 3728772241L, 1506638868L, 4134623100L, 1336279615L, 2978228069L, 1831381761L, 3914742952L, 3906917840L, 901623644L, 2454314963L, 3879607191L, 2259678602L, 304425980L, 2179086277L, 1861158747L, 1092870268L, 1707935029L, 1424242404L, 278777005L, 3172055873L, 2185751799L, 1790577032L, 2481976920L, 2177803387L, 3531279607L, 3876054983L, 188051829L, 3350483616L, 2935031280L, 805324364L, 36495771L, 1596348188L, 4076721203L, 4094802636L, 3150289489L, 4133585366L, 3423673008L, 2076604271L, 230434049L, 3772519338L, 2532072507L, 281460642L, 3368686226L, 4195405958L, 2035242835L, 4063520313L, 1614759427L, 2584917929L, 2780827983L, 1466584611L, 3003752066L, 3855930568L, 851353375L, 2478058112L, 3895823787L, 2654944163L, 1519855874L, 2398748178L, 4223335348L, 3842009060L, 1493939083L, 1278872253L, 2197810541L, 4191148931L, 170757446L, 1309679166L, 1487507117L, 2752193057L, 3001147890L, 3222725925L, 2376749397L, 4255237790L, 2589255489L, 2299579693L, 338970934L, 2710367249L, 3616477681L, 3017703653L, 4055932982L, 982828327L, 1109148424L, 653732500L, 3321546835L, 1354701479L, 1165346722L, 2544193762L, 1711402490L, 2350573341L, 3726378216L, 1639220709L, 2188119433L, 2547640903L, 2546207849L, 1113005996L, 2830873167L, 4170715326L, 1678609906L, 1724140123L, 2018350473L, 2695694006L, 4052535310L, 2309719213L, 3370073520L, 2306028077L, 2645666423L, 258251358L, 1874255969L, 1337780681L, 3622164714L, 3562378871L, 3491376903L, 4228911867L, 389049030L, 2868878604L, 1161208740L, 1283976621L, 3629590412L, 3807417254L, 31708225L, 2330163982L, 3696161618L, 2934101764L, 2894260863L, 3215169663L, 1189565892L, 3633186394L, 2653606790L, 1463846029L, 1203742707L, 3558934116L, 3953091086L, 1266918570L, 559419931L, 2393148074L, 565063406L, 1789193301L, 2357800710L, 2421822630L, 3533513177L, 893545862L, 938531149L, 3015427000L, 2137678097L, 1521002274L, 695529795L, 491214518L, 151225760L, 43440749L, 2304411992L, 4291969503L, 605813942L, 1185374274L, 3616186883L, 96906626L, 2377988921L, 4062055631L, 2958686729L, 3658749857L, 3731118036L, 1229655170L, 2162546816L, 738613573L, 1140453343L, 1212959969L, 438750390L, 3801818181L, 2169766178L, 4239642885L, 1463336520L, 2312844308L, 1292598936L, 1066465055L, 71379455L, 2978914707L, 1341303986L, 923027979L, 2398599346L, 2642906323L, 4218647614L, 4186632800L, 3615369881L, 2021393891L, 3438853395L, 2197621433L, 1250726004L, 1733602016L, 458140094L, 753259820L, 2120299798L, 2146430886L, 2747407681L, 3969990827L, 412550148L, 2971261951L, 3798893343L, 3752963461L, 803228545L, 1943929113L, 968820725L, 3920234818L, 4284702378L, 1294828503L, 4026038270L, 130968700L, 3724453092L, 299838434L, 3053084018L, 3676859278L, 941002535L, 1487410513L, 4135532349L, 297287295L, 3572349920L, 3249734929L, 235427417L, 1918862981L, 1261994844L, 3035644112L, 3889804101L, 2258101121L, 2660420235L, 172500971L, 1281766310L, 3804331927L, 2489656435L, 1124474332L, 3577976336L, 2585924548L, 3330229352L, 4289271849L, 2594051921L, 1922880276L, 2994132501L, 2992869950L, 4065199147L, 3567799255L, 2290818897L, 1458135713L, 3417490043L, 2672166367L, 1404481953L, 2801954189L, 1474048392L, 82558018L, 4211988082L, 1913962119L, 4103403932L, 3737994566L, 4201823103L, 2640592155L, 413521828L, 3794842169L, 1334817499L, 1883332187L, 935294543L, 2138309237L, 3840425669L, 812820247L, 1099724526L, 1301579842L, 3419174758L, 3504104922L, 1246598991L, 3800946993L, 782185603L, 2731046341L, 903995116L, 854567984L, 3618276787L, 1839701062L, 4008144744L, 3225704740L, 167935358L, 3990493637L, 721215336L, 1558888402L, 3699991837L, 796217106L, 2908707628L, 4282066511L, 3739554528L, 2178895307L, 3997356322L, 2351911864L, 2442964774L, 3413771385L, 2374391962L, 903846070L, 2664531801L, 1628066795L, 1585289496L, 3765953242L, 2534181952L, 2456190116L, 555336717L, 2390915393L, 1469220824L, 2310455599L, 3125051871L, 4102157503L, 1985320504L, 738170765L, 3509158567L, 3161988148L, 1659045680L, 624L), None)
INFO: preprocess test  X, mapfn= np.concatenate((x[:4],np.atleast_1d(np.piecewise(x[4],[x[4]<0.1522,(x[4]>=0.1522)&(x[4]<0.4941),(x[4]>=0.4941)&(x[4]<0.8001),x[4]>=0.8001],[0,1,2,3])),np.atleast_1d(np.piecewise(x[5],[x[5]<0.1522,(x[5]>=0.1522)&(x[5]<0.4941),(x[5]>=0.4941)&(x[5]<0.8001),x[5]>=0.8001],[0,1,2,3])),x[6:]))
INFO: preprocess train  X, mapfn= np.concatenate((x[:4],np.atleast_1d(np.piecewise(x[4],[x[4]<0.1522,(x[4]>=0.1522)&(x[4]<0.4941),(x[4]>=0.4941)&(x[4]<0.8001),x[4]>=0.8001],[0,1,2,3])),np.atleast_1d(np.piecewise(x[5],[x[5]<0.1522,(x[5]>=0.1522)&(x[5]<0.4941),(x[5]>=0.4941)&(x[5]<0.8001),x[5]>=0.8001],[0,1,2,3])),x[6:]))
INFO: preprocess data  X, mapfn= np.concatenate((x[:4],np.atleast_1d(np.piecewise(x[4],[x[4]<0.1522,(x[4]>=0.1522)&(x[4]<0.4941),(x[4]>=0.4941)&(x[4]<0.8001),x[4]>=0.8001],[0,1,2,3])),np.atleast_1d(np.piecewise(x[5],[x[5]<0.1522,(x[5]>=0.1522)&(x[5]<0.4941),(x[5]>=0.4941)&(x[5]<0.8001),x[5]>=0.8001],[0,1,2,3])),x[6:]))
INFO: set 922 events to 0 because of negative weight
INFO: set 0 events to 0 because if they were too large (>100.0): [] [] (max 100  events are printed)
INFO: set 0 events to 0 because if they were too large (>100.0): [] [] (max 100  events are printed)
nFeatures =  15
--------------------------------------------------------------------------------
statistics for dataset: train
--------------------------------------------------------------------------------
ZLIGHT (y= 0 ) : 7803  avg weight: 0.06594522613029397
ZB (y= 1 ) : 11349  avg weight: 0.11786406386966657
ZBB (y= 2 ) : 21833  avg weight: 0.11331779665014095
ST (y= 3 ) : 9574  avg weight: 0.07318651167233939
TT (y= 4 ) : 11533  avg weight: 0.22982712394129115
--------------------------------------------------------------------------------
statistics for dataset: test
--------------------------------------------------------------------------------
ZLIGHT (y= 0 ) : 7717  avg weight: 0.06727050914394785
ZB (y= 1 ) : 11468  avg weight: 0.11933816094805291
ZBB (y= 2 ) : 21943  avg weight: 0.11309737748436526
ST (y= 3 ) : 9832  avg weight: 0.0672397938791718
TT (y= 4 ) : 11512  avg weight: 0.22893738641816186
--------------------------------------------------------------------------------
classes and labels
--------------------------------------------------------------------------------
ERROR: no signal or no background defined!
 => using bogus signal ID = 0
list of classes: (signals in [32mgreen[0m, backgrounds in [31mred[0m)
[32m class 0 => ZLIGHT [0m is defined as a SIGNAL
[31m class 1 => ZB [0m
[31m class 2 => ZBB [0m
[31m class 3 => ST [0m
[31m class 4 => TT [0m
--------------------------------------------------------------------------------
weights and weight uncertainty examples
--------------------------------------------------------------------------------
weights:
train 5.272553 6.2100267 7.797219 4.4195514 0.0 6.652471 0.21028672 8.108367 6.4350796 4.9184437
test  5.9788737 6.157375 0.4324847 3.8313646 6.4617677 3.1897027 1.8591018 6.149644 5.2115617 4.0773478
weights errors:
train 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
test  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
--------------------------------------------------------------------------------
input data
--------------------------------------------------------------------------------
feature                                            set   mean       std        examples
H_mass                                             train 2.53e+02   1.04e+02   210.54965 49.462025 163.29361 51.68678
H_mass                                             test  2.52e+02   1.03e+02   47.751022 220.85242 229.74277 58.87056
H_pt                                               train 2.07e+02   6.58e+01   141.78133 171.33122 171.84222 189.41864
H_pt                                               test  2.07e+02   6.49e+01   184.96759 188.65399 126.56315 156.64656
MET_Pt                                             train 2.23e+02   5.72e+01   190.45529 181.23492 188.55977 200.78278
MET_Pt                                             test  2.24e+02   5.59e+01   208.68152 187.14607 182.23657 173.51013
abs(TVector2::Phi_mpi_pi(H_phi-V_phi))             train 2.97e+00   1.57e-01   3.125851 3.102851 2.9667306 2.8086445
abs(TVector2::Phi_mpi_pi(H_phi-V_phi))             test  2.97e+00   1.60e-01   2.928063 2.9816718 2.9582913 2.8200614
Jet_btagDeepB[hJidx[0]]                            train 3.00e+00   2.38e-07   3.0 3.0 3.0 3.0
Jet_btagDeepB[hJidx[0]]                            test  3.00e+00   2.38e-07   3.0 3.0 3.0 3.0
Jet_btagDeepB[hJidx[1]]                            train 1.74e+00   8.26e-01   1.0 1.0 2.0 1.0
Jet_btagDeepB[hJidx[1]]                            test  1.74e+00   8.28e-01   1.0 1.0 1.0 1.0
abs(Jet_eta[hJidx[0]]-Jet_eta[hJidx[1]])           train 1.24e+00   8.28e-01   1.3547745 0.473938 1.8601685 0.3305664
abs(Jet_eta[hJidx[0]]-Jet_eta[hJidx[1]])           test  1.23e+00   8.19e-01   0.17070007 2.2277222 2.802002 0.5888672
abs(TVector2::Phi_mpi_pi(Jet_phi[hJidx[0]]-Jet...  train 1.53e+00   8.11e-01   2.4816716 0.07296753 0.22314453 0.39257812
abs(TVector2::Phi_mpi_pi(Jet_phi[hJidx[0]]-Jet...  test  1.53e+00   8.13e-01   0.39697266 0.19189453 0.6725464 0.15820312
max(Jet_PtReg[hJidx[0]],Jet_PtReg[hJidx[1]])       train 1.89e+02   7.08e+01   173.8841 88.02986 128.158 148.01984
max(Jet_PtReg[hJidx[0]],Jet_PtReg[hJidx[1]])       test  1.89e+02   7.02e+01   105.833725 144.6027 96.235085 108.62604
min(Jet_PtReg[hJidx[0]],Jet_PtReg[hJidx[1]])       train 7.30e+01   3.26e+01   43.896652 83.41537 44.5052 43.996483
min(Jet_PtReg[hJidx[0]],Jet_PtReg[hJidx[1]])       test  7.28e+01   3.22e+01   82.781235 44.67885 36.183937 48.43944
SA5                                                train 1.98e+00   1.80e+00   0.0 3.0 5.0 1.0
SA5                                                test  1.98e+00   1.77e+00   1.0 1.0 3.0 4.0
Sum$(Jet_Pt>30&&abs(Jet_eta)<2.4&&Jet_puId>6&&...  train 0.00e+00   0.00e+00   0.0 0.0 0.0 0.0
Sum$(Jet_Pt>30&&abs(Jet_eta)<2.4&&Jet_puId>6&&...  test  0.00e+00   0.00e+00   0.0 0.0 0.0 0.0
MaxIf$(Jet_btagDeepB,Jet_Pt>30&&abs(Jet_eta)<2...  train 0.00e+00   0.00e+00   0.0 0.0 0.0 0.0
MaxIf$(Jet_btagDeepB,Jet_Pt>30&&abs(Jet_eta)<2...  test  0.00e+00   0.00e+00   0.0 0.0 0.0 0.0
MaxIf$(Jet_Pt,Jet_Pt>30&&abs(Jet_eta)<2.4&&Jet...  train 0.00e+00   0.00e+00   0.0 0.0 0.0 0.0
MaxIf$(Jet_Pt,Jet_Pt>30&&abs(Jet_eta)<2.4&&Jet...  test  0.00e+00   0.00e+00   0.0 0.0 0.0 0.0
MinIf$(abs(TVector2::Phi_mpi_pi(Jet_phi-V_phi)...  train -1.14e+00  7.85e-01   -0.17513072 -0.07611514 -0.34056902 -0.42183322
MinIf$(abs(TVector2::Phi_mpi_pi(Jet_phi-V_phi)...  test  -1.13e+00  7.88e-01   -0.43650103 -0.114644945 -0.3622718 -0.37017477
--------------------------------------------------------------------------------
input scaling
--------------------------------------------------------------------------------
[31mINFO: scaling is done inside tensorflow graph and StandardScaler() should not be used om top of it => scaler.dmp file will not be written![0m
balancing classes, reweight ZLIGHT by 14.920326200757344
balancing classes, reweight ZB by 5.739635058903671
balancing classes, reweight ZBB by 3.103214176538481
balancing classes, reweight ST by 10.957180504130742
balancing classes, reweight TT by 2.8965412153866796
shape train: (62092, 15)
shape test:  (62472, 15)
building tensorflow graph with parameters
 adam_epsilon                             1e-11
 adaptiveRate                             False
 additional_noise                         0.0
 backgroundOnly                           True
 balanceClasses                           True
 balanceSignalBackground                  False
 batchNormalization                       [1, 2, 3, 4, 5, 6, 7, 8]
 batchSize                                32
 batchSizeAtEpoch                         {0: 32, 32: 8192, 3: 64, 40: 16384, 5: 128, 6: 256, 7: 512, 24: 4096, 9: 2048, 8: 1024}
 batchSizeTest                            65536
 bin_opt_cumulative_background            [0.125155, 0.13149, 0.13149, 0.125155, 0.113385, 0.0977718, 0.0802467, 0.0626894, 0.0466138, 0.0329904, 0.0222236, 0.0142493, 0.00869618, 0.00505144, 0.0027929]
 crossValidation_splitSeed                123456
 dropoutDecay                             1.0
 ignoreLargeWeights                       True
 ignoreNegativeWeights                    True
 learningRate                             0.0001
 learning_rate_adam_start                 0.0001
 loss                                     'cross_entropy'
 massless_importance                      1.0
 massless_powers                          [1, 2]
 maxWeight                                100
 mvaScoreRescalingPercentileHigh          0.999
 mvaScoreRescalingPercentileLow           0.01
 nEpochs                                  200
 nNodes                                   [32, 32, 32, 32, 32, 32, 32, 32]
 nStepsPerEpoch                           -1
 pDropout                                 [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
 plot-roc                                 False
 power                                    1.0
 preprocess                               'np.concatenate((x[:4],np.atleast_1d(np.piecewise(x[4],[x[4]<0.1522,(x[4]>=0.1522)&(x[4]<0.4941),(x[4]>=0.4941)&(x[4]<0.8001),x[4]>=0.8001],[0,1,2,3])),np.atleast_1d(np.piecewise(x[5],[x[5]<0.1522,(x[5]>=0.1522)&(x[5]<0.4941),(x[5]>=0.4941)&(x[5]<0.8001),x[5]>=0.8001],[0,1,2,3])),x[6:]))'
 rateGamma                                1.0
 regularization_strength                  0.0
 removeFeature                            []
 reweight                                 {}
 reweightTraining                         None
 saveCheckpointInterval                   50
 scaleInputsInsideGraph                   True
 shuffle                                  True
 signif_loss_b_epsilon                    1e-08
 signif_loss_low_b_threshold              1.5
 signif_loss_low_b_threshold_width        1.5
 signif_loss_nbins                        15
 signif_loss_smoothness                   500.0
 signif_loss_sysApprox_constant           1.5
 signif_loss_sysApprox_linear             0.1
 signif_loss_sys_variance_offset          0.1
 signif_loss_xe_factor                    0.0
 skipConnections                          {8: [0, 2, 4, 6], 2: [0], 4: [0, 2], 6: [0, 2, 4]}
 statisticsInterval                       10
 systematics_scaling_factor               1.0
 systematics_weight_scaling_factor        1.0
 weight_sys_ntoys                         -1
initialize session...
initialized session!
add layers...
layer  1 :  [15, 32]
> activation with drop-out...
> batch normalization...
layer  2 :  [32, 32]
> activation with drop-out...
> batch normalization...
layer  3 :  [32, 32]
> activation with drop-out...
> batch normalization...
layer  4 :  [32, 32]
> activation with drop-out...
> batch normalization...
layer  5 :  [32, 32]
> activation with drop-out...
> batch normalization...
layer  6 :  [32, 32]
> activation with drop-out...
> batch normalization...
layer  7 :  [32, 32]
> activation with drop-out...
> batch normalization...
layer  8 :  [32, 32]
> activation with drop-out...
> batch normalization...
INFO: use ADAM optimizer!
graph built.
trainable variables: 16133
initialized.
conditions:
  loss(train,training)    training dataset, loss as seen by gradient descend optimizer
  loss(train,testing)     training dataset, loss with 'testing' conditions, e.g. is_training: False, no dropout etc
  loss(test)              test dataset
start training with batch size 32 
 epoch     loss(train,training) loss(train,testing) loss(test)
nSamples =  62092
set batch size to: 32
         1    1.06764    0.89228    0.89800 significance (train): 7.127 significance: 6.829 
         2    0.96384    0.86954    0.87564 
         3    0.92664    0.86187    0.86577 
nSamples =  62092
set batch size to: 64
         4    0.91853    0.85507    0.86095 
         5    0.91163    0.84983    0.85625 
nSamples =  62092
set batch size to: 128
         6    0.90321    0.84753    0.85428 
nSamples =  62092
set batch size to: 256
         7    0.89556    0.84647    0.85301 
nSamples =  62092
set batch size to: 512
         8    0.89178    0.84570    0.85239 
nSamples =  62092
set batch size to: 1024
         9    0.89175    0.84525    0.85207 
nSamples =  62092
set batch size to: 2048
        10    0.89410    0.84507    0.85193 
        11    0.88344    0.84490    0.85180 significance (train): 8.244 significance: 7.498 
        12    0.88712    0.84474    0.85169 
        13    0.88983    0.84456    0.85154 
        14    0.89241    0.84437    0.85141 
        15    0.88446    0.84419    0.85132 
        16    0.89041    0.84403    0.85120 
        17    0.88723    0.84388    0.85106 
        18    0.88863    0.84375    0.85096 
        19    0.88300    0.84359    0.85085 
        20    0.88238    0.84337    0.85072 
        21    0.88806    0.84322    0.85062 significance (train): 8.278 significance: 7.495 
        22    0.89202    0.84300    0.85050 
        23    0.88786    0.84282    0.85042 
        24    0.88458    0.84266    0.85026 
nSamples =  62092
set batch size to: 4096
        25    0.89382    0.84258    0.85017 
        26    0.89037    0.84248    0.85012 
        27    0.88687    0.84241    0.85009 
        28    0.88490    0.84231    0.85002 
        29    0.88705    0.84225    0.84997 
        30    0.88231    0.84214    0.84990 
        31    0.88354    0.84205    0.84988 significance (train): 8.313 significance: 7.484 
        32    0.88200    0.84195    0.84981 
nSamples =  62092
set batch size to: 8192
        33    0.88933    0.84190    0.84978 
        34    0.87475    0.84186    0.84976 
        35    0.88228    0.84181    0.84973 
        36    0.88540    0.84177    0.84969 
        37    0.88534    0.84173    0.84968 
        38    0.88652    0.84169    0.84965 
        39    0.88149    0.84165    0.84962 
        40    0.88864    0.84162    0.84960 
nSamples =  62092
set batch size to: 16384
        41    0.87481    0.84161    0.84959 significance (train): 8.349 significance: 7.507 
        42    0.89422    0.84159    0.84958 
        43    0.89408    0.84157    0.84957 
        44    0.87271    0.84156    0.84956 
        45    0.88782    0.84155    0.84955 
        46    0.88688    0.84153    0.84954 
        47    0.89245    0.84151    0.84953 
        48    0.88663    0.84149    0.84952 
        49    0.87838    0.84147    0.84951 
        50    0.88048    0.84146    0.84950 
        51    0.87501    0.84144    0.84949 significance (train): 8.427 significance: 7.503 
        52    0.87981    0.84142    0.84948 
        53    0.88106    0.84140    0.84946 
        54    0.89935    0.84138    0.84945 
        55    0.88173    0.84136    0.84944 
        56    0.88433    0.84134    0.84942 
        57    0.87826    0.84133    0.84941 
        58    0.89382    0.84131    0.84940 
        59    0.88673    0.84129    0.84939 
        60    0.88238    0.84128    0.84938 
        61    0.89153    0.84126    0.84936 significance (train): 8.421 significance: 7.491 
        62    0.89329    0.84124    0.84935 
        63    0.88278    0.84123    0.84934 
        64    0.87786    0.84121    0.84933 
        65    0.88063    0.84119    0.84933 
        66    0.88852    0.84117    0.84932 
        67    0.88867    0.84115    0.84930 
        68    0.87539    0.84114    0.84929 
        69    0.88613    0.84112    0.84928 
        70    0.88514    0.84110    0.84927 
        71    0.88141    0.84108    0.84926 significance (train): 8.427 significance: 7.483 
        72    0.88783    0.84106    0.84925 
        73    0.87525    0.84105    0.84924 
        74    0.88002    0.84103    0.84922 
        75    0.87299    0.84101    0.84921 
        76    0.88265    0.84099    0.84919 
        77    0.87772    0.84097    0.84917 
        78    0.88860    0.84095    0.84916 
        79    0.88799    0.84093    0.84915 
        80    0.89234    0.84091    0.84914 
        81    0.88974    0.84090    0.84913 significance (train): 8.433 significance: 7.488 
        82    0.88771    0.84088    0.84912 
        83    0.89898    0.84086    0.84911 
        84    0.87495    0.84085    0.84910 
        85    0.89671    0.84083    0.84909 
        86    0.87362    0.84081    0.84908 
        87    0.89150    0.84080    0.84907 
        88    0.88428    0.84078    0.84905 
        89    0.88645    0.84076    0.84905 
        90    0.89452    0.84075    0.84903 
        91    0.89295    0.84073    0.84903 significance (train): 8.449 significance: 7.495 
        92    0.88313    0.84072    0.84902 
        93    0.88216    0.84071    0.84901 
        94    0.88347    0.84069    0.84900 
        95    0.88229    0.84068    0.84899 
        96    0.88820    0.84067    0.84898 
        97    0.88417    0.84065    0.84897 
        98    0.88192    0.84064    0.84895 
        99    0.88290    0.84062    0.84894 
       100    0.88169    0.84060    0.84893 
       101    0.88066    0.84059    0.84892 significance (train): 8.644 significance: 7.502 
       102    0.87477    0.84057    0.84891 
       103    0.88789    0.84056    0.84889 
       104    0.87591    0.84054    0.84888 
       105    0.88091    0.84052    0.84887 
       106    0.87394    0.84050    0.84886 
       107    0.87824    0.84048    0.84885 
       108    0.88933    0.84046    0.84883 
       109    0.87122    0.84044    0.84882 
       110    0.87548    0.84042    0.84881 
       111    0.88641    0.84040    0.84879 significance (train): 8.637 significance: 7.484 
       112    0.88729    0.84038    0.84878 
       113    0.88342    0.84036    0.84876 
       114    0.87892    0.84034    0.84874 
       115    0.87663    0.84032    0.84873 
       116    0.88174    0.84030    0.84872 
       117    0.87242    0.84029    0.84870 
       118    0.88762    0.84027    0.84869 
       119    0.87518    0.84025    0.84867 
       120    0.88905    0.84024    0.84865 
       121    0.87723    0.84022    0.84864 significance (train): 8.643 significance: 7.496 
       122    0.87311    0.84021    0.84862 
       123    0.87976    0.84019    0.84860 
       124    0.87668    0.84018    0.84858 
       125    0.87490    0.84017    0.84856 
       126    0.88093    0.84015    0.84854 
       127    0.88483    0.84013    0.84853 
       128    0.87736    0.84012    0.84851 
       129    0.87874    0.84010    0.84849 
       130    0.88469    0.84008    0.84848 
       131    0.87834    0.84006    0.84846 significance (train): 8.611 significance: 7.492 
       132    0.87842    0.84004    0.84845 
       133    0.88656    0.84003    0.84844 
       134    0.88054    0.84001    0.84843 
       135    0.87321    0.84000    0.84841 
       136    0.86395    0.83998    0.84840 
       137    0.89746    0.83996    0.84839 
       138    0.89026    0.83994    0.84837 
       139    0.88752    0.83993    0.84835 
       140    0.87685    0.83991    0.84834 
       141    0.87342    0.83989    0.84833 significance (train): 8.610 significance: 7.489 
       142    0.88210    0.83987    0.84833 
       143    0.87929    0.83986    0.84832 
       144    0.88691    0.83984    0.84831 
       145    0.88808    0.83982    0.84829 
       146    0.88631    0.83981    0.84829 
       147    0.89441    0.83979    0.84828 
       148    0.88468    0.83977    0.84827 
       149    0.87945    0.83975    0.84826 
       150    0.88305    0.83974    0.84824 
       151    0.87586    0.83972    0.84824 significance (train): 8.616 significance: 7.533 
       152    0.88972    0.83970    0.84823 
       153    0.87991    0.83968    0.84822 
       154    0.88258    0.83967    0.84822 
       155    0.87319    0.83965    0.84821 
       156    0.87695    0.83963    0.84820 
       157    0.88828    0.83961    0.84819 
       158    0.88499    0.83959    0.84818 
       159    0.87706    0.83957    0.84817 
       160    0.88796    0.83955    0.84816 
       161    0.87449    0.83953    0.84815 significance (train): 8.609 significance: 7.538 
       162    0.88404    0.83952    0.84814 
       163    0.88853    0.83950    0.84814 
       164    0.88296    0.83948    0.84813 
       165    0.88144    0.83946    0.84812 
       166    0.88171    0.83945    0.84811 
       167    0.88571    0.83943    0.84810 
       168    0.88342    0.83942    0.84809 
       169    0.88341    0.83940    0.84809 
       170    0.87865    0.83938    0.84808 
       171    0.88318    0.83936    0.84807 significance (train): 8.661 significance: 7.503 
       172    0.87504    0.83935    0.84806 
       173    0.88528    0.83933    0.84804 
       174    0.88016    0.83930    0.84802 
       175    0.88045    0.83929    0.84801 
       176    0.87545    0.83927    0.84799 
       177    0.87567    0.83925    0.84798 
       178    0.87528    0.83923    0.84797 
       179    0.88139    0.83922    0.84796 
       180    0.88395    0.83920    0.84795 
       181    0.89365    0.83918    0.84793 significance (train): 8.638 significance: 7.507 
       182    0.86367    0.83917    0.84793 
       183    0.87873    0.83916    0.84792 
       184    0.87443    0.83914    0.84790 
       185    0.87832    0.83912    0.84789 
       186    0.88308    0.83910    0.84788 
       187    0.87754    0.83909    0.84787 
       188    0.87985    0.83907    0.84786 
       189    0.88519    0.83905    0.84785 
       190    0.87618    0.83903    0.84784 
       191    0.88010    0.83901    0.84783 significance (train): 8.623 significance: 7.504 
       192    0.88280    0.83899    0.84782 
       193    0.88573    0.83897    0.84781 
       194    0.87882    0.83895    0.84780 
       195    0.89036    0.83893    0.84780 
       196    0.88927    0.83892    0.84779 
       197    0.88492    0.83890    0.84778 
       198    0.87806    0.83888    0.84777 
       199    0.88508    0.83886    0.84776 
       200    0.89122    0.83885    0.84775 significance (train): 8.644 significance: 7.481 
FINAL RESULTS:        200   0.891224   0.847751 significance (train): 8.644 significance: 7.481 
TRAINING TIME: 0:01:31.869355 (91.9 seconds)
GRADIENT UPDATES: 9774
MIN TEST LOSS: 0.847750663757
training done.
> results//V11_0lep_Znn_HFCR_binnedWP/Zvv2017_BDT_Znn_HFCR_190624_V11base.h5/32-32-32-32-32-32-32-32/0.10-0.10-0.10-0.10-0.10-0.10-0.10-0.10/1.000e-04/rnd_1/checkpoints/model.ckpt
saved checkpoint to [34m results//V11_0lep_Znn_HFCR_binnedWP/Zvv2017_BDT_Znn_HFCR_190624_V11base.h5/32-32-32-32-32-32-32-32/0.10-0.10-0.10-0.10-0.10-0.10-0.10-0.10/1.000e-04/rnd_1/checkpoints/model.ckpt [0m
LOSS(train, unmodified):  0.838847219944
LOSS(test):               0.847750663757
---
S    B
---
12.08 1308.19
25.56 1292.64
86.04 1274.91
128.63 1465.36
159.30 1189.72
79.26 482.99
23.57 110.50
 3.58 17.19
 0.40  4.07
 0.52  1.19
 0.18  0.07
 0.00  0.05
 0.00  0.00
 0.00  0.00
 0.00  0.00
---
significance: 7.481 
-------------------------
with optimized binning:
 method: B
 target: 0.1252, 0.1315, 0.1315, 0.1252, 0.1134, 0.0978, 0.0802, 0.0627, 0.0466, 0.0330, 0.0222, 0.0142, 0.0087, 0.0051, 0.0028
 bins:   0.0000, 0.0492, 0.0903, 0.1443, 0.1904, 0.2269, 0.2590, 0.2855, 0.3107, 0.3335, 0.3563, 0.3798, 0.4056, 0.4364, 0.4783, 1.0000
-------------------------
---
S    B
---
 8.34 894.35
 7.48 940.08
31.22 939.77
55.43 894.65
61.54 810.56
62.92 698.75
77.74 574.14
52.60 448.10
54.81 333.36
23.35 235.85
33.13 159.00
27.43 101.86
11.92 62.38
 8.58 36.06
 2.66 17.98
---
significance: 7.687 (for optimized binning)
significance: 7.524 ( 1% background uncertainty, for optimized binning)
significance: 5.599 ( 5% background uncertainty, for optimized binning)
significance: 3.905 (10% background uncertainty, for optimized binning)
significance: 2.952 (15% background uncertainty, for optimized binning)
significance: 2.357 (20% background uncertainty, for optimized binning)
[32mPLOTS: use n=S+B Asimov data in the plots![0m
[32mPLOTS: use n=S+B Asimov data in the plots![0m
[32mPLOTS: use n=S+B Asimov data in the plots![0m
now optimizing the bins again for signal region only!
-------------------------
with optimized binning:
 method: B
 target: 0.1252, 0.1315, 0.1315, 0.1252, 0.1134, 0.0978, 0.0802, 0.0627, 0.0466, 0.0330, 0.0222, 0.0142, 0.0087, 0.0051, 0.0028
 bins:   0.0000, 0.2815, 0.3035, 0.3216, 0.3384, 0.3530, 0.3677, 0.3824, 0.3993, 0.4154, 0.4368, 0.4576, 0.4847, 0.5284, 0.6425, 1.0000
-------------------------
---
S    B
---
13.61 129.25
13.43 136.15
23.23 136.08
17.51 129.47
10.96 117.66
15.79 101.06
17.44 83.35
15.29 64.73
10.06 48.36
 9.82 34.45
 5.43 23.02
 3.15 14.58
 1.46  9.45
 0.94  5.19
 0.25  0.57
---
significance: 5.063 (for optimized binning)
significance: 5.040 ( 1% background uncertainty, for optimized binning)
significance: 4.583 ( 5% background uncertainty, for optimized binning)
significance: 3.758 (10% background uncertainty, for optimized binning)
significance: 3.076 (15% background uncertainty, for optimized binning)
significance: 2.573 (20% background uncertainty, for optimized binning)
.....
[32mPLOTS: use n=S+B Asimov data in the plots![0m
confusion matrix:
ZLIGHT     158.4     184.6     68.1      58.5      49.6      
ZB         366.8     567.9     173.0     140.6     120.2     
ZBB        289.6     460.4     1203.7    251.0     277.0     
ST         112.2     108.8     83.9      189.1     167.2     
TT         264.9     290.5     294.6     465.3     1320.3    
confusion matrix (normalized to output category)
ZLIGHT     13.3      11.5      3.7       5.3       2.6       
ZB         30.8      35.2      9.5       12.7      6.2       
ZBB        24.3      28.6      66.0      22.7      14.3      
ST         9.4       6.7       4.6       17.1      8.6       
TT         22.2      18.0      16.2      42.1      68.3      
confusion matrix (normalized to label)
ZLIGHT     30.5      35.6      13.1      11.3      9.5       
ZB         26.8      41.5      12.6      10.3      8.8       
ZBB        11.7      18.6      48.5      10.1      11.2      
ST         17.0      16.5      12.7      28.6      25.3      
TT         10.0      11.0      11.2      17.7      50.1      
----
class      efficiency    purity       product
ZLIGHT     30.51        13.29        405.42      
ZB         41.50        35.23        1,461.89    
ZBB        48.51        66.02        3,202.38    
ST         28.60        17.12        489.66      
TT         50.10        68.26        3,419.42    
--------------------------------------------------------------------------------
statistics for dataset: test
--------------------------------------------------------------------------------
ZLIGHT (y= 0 ) : 7717  avg weight: 0.06727050914394785
ZB (y= 1 ) : 11468  avg weight: 0.11933816094805291
ZBB (y= 2 ) : 21943  avg weight: 0.11309737748436526
ST (y= 3 ) : 9832  avg weight: 0.0672397938791718
TT (y= 4 ) : 11512  avg weight: 0.22893738641816186
test set predictions:
correct: 25321 wrong: 37151 error: 59.47
      fun: 8.775319711759108
 hess_inv: array([[ 3.23648402e-01, -1.15635557e-01,  7.97001814e-04,
        -3.69476909e-03],
       [-1.15635557e-01,  4.48523445e-02, -1.27749536e-03,
         8.59448099e-04],
       [ 7.97001814e-04, -1.27749536e-03,  1.02279481e-03,
        -2.68755520e-04],
       [-3.69476909e-03,  8.59448099e-04, -2.68755520e-04,
         8.68009703e-04]])
      jac: array([-4.76837158e-07, -3.21865082e-06, -4.76837158e-06,  6.43730164e-06])
  message: 'Optimization terminated successfully.'
     nfev: 84
      nit: 9
     njev: 14
   status: 0
  success: True
        x: array([0.6506753 , 1.18594423, 0.83649406, 1.18727283])
[34mINFO: TwoHighest : estimated process scale-factors (without systematics): [0.6506753  1.18594423 0.83649406 1.18727283] [0m
[34mINFO: TwoHighest : estimated process scale-factor uncertainties (stat only): [0.5689010473105256, 0.2117837209945241, 0.03198116332774723, 0.0294620043885112] [0m
[34mINFO: TwoHighest : estimated process scale-factor relative uncertainties (stat only): [0.8743240198843053, 0.17857814516022136, 0.038232385467751484, 0.02481485610475515] [0m
[32mPLOTS: use real data in the plots![0m
scale ZLIGHT by 0.6506753038602385
scale ZB by 1.1859442307708512
scale ZBB by 0.8364940595904727
scale TT by 1.187272828185594
[32mPLOTS: use real data in the plots![0m
[32mPLOTS: use real data in the plots![0m
scale ZLIGHT by 0.6506753038602385
scale ZB by 1.1859442307708512
scale ZBB by 0.8364940595904727
scale TT by 1.187272828185594
[32mPLOTS: use real data in the plots![0m
bin-list CRs  [10, 20, 30, 40, 50, 60, 70, 80, 90]  =  [0.0, 0.2774913555665245, 0.29637521404903194, 0.3112214772464245, 0.32463881425421737, 0.3380026401031242, 0.35138697567629956, 0.3659162635689724, 0.383702780933772, 0.4118110206629099, 1.0, 1.272890876833053, 1.2923568339516316, 1.310586146823698, 1.3262445285026514, 1.3437390950458474, 1.360469155391391, 1.3801852319790404, 1.4061053857042507, 1.448019771582196, 2.0, 2.308691729736147, 2.3443907624804243, 2.378821264549325, 2.4151580463466518, 2.458757427923178, 2.5109038622848603, 2.569367941894428, 2.628565735074969, 2.7024726599837345, 3.0, 3.270795136465778, 3.2922095504526814, 3.3132293331603826, 3.3333934690141858, 3.3554329415550925, 3.377232464866914, 3.400976500683351, 3.4265139064954484, 3.466623233147774, 4.0, 4.294138391672583, 4.329784422333406, 4.359806334434541, 4.3879350212976505, 4.417006988369578, 4.451263886556061, 4.494256276384361, 4.547996321261282, 4.626876866214384, 5.0]
      fun: 49.00032243620049
 hess_inv: array([[ 1.58964613e+01, -4.92521999e+00,  4.63866471e-02,
         3.01618782e-02],
       [-4.92521999e+00,  1.55288933e+00, -1.76280355e-02,
        -1.08120109e-02],
       [ 4.63866471e-02, -1.76280355e-02,  2.20998692e-03,
        -1.98314949e-04],
       [ 3.01618782e-02, -1.08120109e-02, -1.98314949e-04,
         1.38801896e-03]])
      jac: array([-4.76837158e-07, -9.53674316e-07,  9.53674316e-07,  2.86102295e-06])
  message: 'Optimization terminated successfully.'
     nfev: 174
      nit: 24
     njev: 29
   status: 0
  success: True
        x: array([11.61163155, -2.13507166,  0.84916072,  1.13440487])
[34mINFO: 10binsFlat : estimated process scale-factors (without systematics): [11.61163155 -2.13507166  0.84916072  1.13440487] [0m
[34mINFO: 10binsFlat : estimated process scale-factor uncertainties (stat only): [3.9870366625829647, 1.246149803417794, 0.04701049800615778, 0.037256126474551024] [0m
[34mINFO: 10binsFlat : estimated process scale-factor relative uncertainties (stat only): [0.34336575738185904, -0.5836571315353835, 0.05536113126383936, 0.03284200143049938] [0m
[32mPLOTS: use real data in the plots![0m
scale ZLIGHT by 11.611631552848639
scale ZB by -2.135071664659764
scale ZBB by 0.8491607185936241
scale TT by 1.1344048733872953
[32mPLOTS: use real data in the plots![0m
[32mPLOTS: use real data in the plots![0m
scale ZLIGHT by 11.611631552848639
scale ZB by -2.135071664659764
scale ZBB by 0.8491607185936241
scale TT by 1.1344048733872953
[32mPLOTS: use real data in the plots![0m
bin-list CRs  [2, 6, 14, 26, 41, 59, 74, 86, 94, 98]  =  [0.0, 0.2537673238550585, 0.2681733248056558, 0.2849805316609242, 0.3059349310714323, 0.32595770509703514, 0.34989826854674316, 0.3731028952233583, 0.39925241496887076, 0.4323077084472193, 0.4735045309055479, 1.0, 1.2501248054403606, 1.2635569723361362, 1.2804900503178995, 1.303053462238383, 1.3279385048635186, 1.358933140591711, 1.389300780761036, 1.4297362731781496, 1.4719754508572593, 1.525080578263029, 2.0, 2.255868887113613, 2.2838065975727346, 2.3255473775262816, 2.3654170091995956, 2.4197829425621458, 2.5049124882358536, 2.590235061674014, 2.6712970729263468, 2.745801039296224, 2.8035188537307407, 3.0, 3.248444974535224, 3.260720839628952, 3.279675064621432, 3.305411771585116, 3.3359155981760895, 3.375099822813027, 3.4094796152931677, 3.4481600049773964, 3.4914155493820185, 3.5279821044945354, 4.0, 4.259337342778343, 4.280189255903163, 4.310310106502736, 4.347468580478117, 4.390749138644815, 4.447226848654758, 4.513365352997263, 4.592248157113918, 4.675599982218231, 4.765763719125885, 5.0]
      fun: 79.41111066156006
 hess_inv: array([[ 1.29289554e+00, -3.30397387e-01, -4.66586126e-05,
        -7.25105481e-03],
       [-3.30397387e-01,  9.12297435e-02, -1.30316100e-03,
         1.15303167e-03],
       [-4.66586126e-05, -1.30316100e-03,  1.11267536e-03,
        -2.37920365e-04],
       [-7.25105481e-03,  1.15303167e-03, -2.37920365e-04,
         9.53909277e-04]])
      jac: array([-9.53674316e-07, -1.90734863e-06,  0.00000000e+00,  1.90734863e-06])
  message: 'Optimization terminated successfully.'
     nfev: 132
      nit: 18
     njev: 22
   status: 0
  success: True
        x: array([4.25641943, 0.14182434, 0.85364914, 1.1208504 ])
[34mINFO: 10binsGauss : estimated process scale-factors (without systematics): [4.25641943 0.14182434 0.85364914 1.1208504 ] [0m
[34mINFO: 10binsGauss : estimated process scale-factor uncertainties (stat only): [1.1370556443713529, 0.3020426186761092, 0.0333567888685428, 0.03088542176221071] [0m
[34mINFO: 10binsGauss : estimated process scale-factor relative uncertainties (stat only): [0.2671390032003641, 2.1296952404173473, 0.039075525571809, 0.02755534701551354] [0m
[32mPLOTS: use real data in the plots![0m
scale ZLIGHT by 4.256419432390108
scale ZB by 0.14182433849874182
scale ZBB by 0.8536491417688831
scale TT by 1.1208504013693732
[32mPLOTS: use real data in the plots![0m
[32mPLOTS: use real data in the plots![0m
scale ZLIGHT by 4.256419432390108
scale ZB by 0.14182433849874182
scale ZBB by 0.8536491417688831
scale TT by 1.1208504013693732
[32mPLOTS: use real data in the plots![0m
bin-list CRs  [20, 40, 60, 80]  =  [0.0, 0.29637521404903194, 0.32463881425421737, 0.35138697567629956, 0.383702780933772, 1.0, 1.2923568339516316, 1.3262445285026514, 1.360469155391391, 1.4061053857042507, 2.0, 2.3443907624804243, 2.4151580463466518, 2.5109038622848603, 2.628565735074969, 3.0, 3.2922095504526814, 3.3333934690141858, 3.377232464866914, 3.4265139064954484, 4.0, 4.329784422333406, 4.3879350212976505, 4.451263886556061, 4.547996321261282, 5.0]
      fun: 27.323008710459774
 hess_inv: array([[ 5.78763506e-01, -7.99108922e-02,  9.51467077e-03,
        -9.04808987e-03],
       [-7.99108922e-02,  2.11913230e-02, -2.44976104e-03,
         2.49717964e-04],
       [ 9.51467077e-03, -2.44976104e-03,  2.69996595e-03,
        -7.73347134e-04],
       [-9.04808987e-03,  2.49717964e-04, -7.73347134e-04,
         2.26257277e-03]])
      jac: array([ 4.76837158e-07, -2.38418579e-07,  7.39097595e-06,  7.15255737e-07])
  message: 'Optimization terminated successfully.'
     nfev: 198
      nit: 27
     njev: 33
   status: 0
  success: True
        x: array([14.9925217 , -3.46147148,  0.86038021,  1.09775573])
[34mINFO: 5binsFlat : estimated process scale-factors (without systematics): [14.9925217  -3.46147148  0.86038021  1.09775573] [0m
[34mINFO: 5binsFlat : estimated process scale-factor uncertainties (stat only): [0.7607650794745824, 0.14557239772377703, 0.0519611965343657, 0.047566508888035375] [0m
[34mINFO: 5binsFlat : estimated process scale-factor relative uncertainties (stat only): [0.05074297002535774, -0.04205506203592293, 0.06039329554084779, 0.04333068594072845] [0m
[32mPLOTS: use real data in the plots![0m
scale ZLIGHT by 14.99252170486682
scale ZB by -3.4614714775460524
scale ZBB by 0.8603802138802157
scale TT by 1.0977557325794718
[32mPLOTS: use real data in the plots![0m
[32mPLOTS: use real data in the plots![0m
scale ZLIGHT by 14.99252170486682
scale ZB by -3.4614714775460524
scale ZBB by 0.8603802138802157
scale TT by 1.0977557325794718
[32mPLOTS: use real data in the plots![0m
bin-list CRs  [50, 70, 85, 95]  =  [0.0, 0.3380026401031242, 0.3659162635689724, 0.39584284356451577, 0.43937731650815404, 1.0, 1.3437390950458474, 1.3801852319790404, 1.4245333331296774, 1.4797300632749963, 2.0, 2.458757427923178, 2.569367941894428, 2.6629735205293255, 2.7555488431706667, 3.0, 3.3554329415550925, 3.400976500683351, 3.4431739268052923, 3.4971151731035164, 4.0, 4.417006988369578, 4.494256276384361, 4.583206985939005, 4.691779292510485, 5.0]
      fun: 32.74653925292582
 hess_inv: array([[ 8.64221111e-01, -2.79030355e-01,  2.70980767e-03,
        -1.44234275e-02],
       [-2.79030355e-01,  9.45293349e-02, -1.86172060e-03,
         4.10130605e-03],
       [ 2.70980767e-03, -1.86172060e-03,  9.58942896e-04,
        -2.60766855e-04],
       [-1.44234275e-02,  4.10130605e-03, -2.60766855e-04,
         1.11981465e-03]])
      jac: array([4.76837158e-07, 1.90734863e-06, 4.76837158e-07, 9.53674316e-07])
  message: 'Optimization terminated successfully.'
     nfev: 114
      nit: 15
     njev: 19
   status: 0
  success: True
        x: array([2.33564857, 0.62756342, 0.85486334, 1.13316137])
[34mINFO: 5bins_50_20_15_10_5 : estimated process scale-factors (without systematics): [2.33564857 0.62756342 0.85486334 1.13316137] [0m
[34mINFO: 5bins_50_20_15_10_5 : estimated process scale-factor uncertainties (stat only): [0.9296349343235298, 0.30745623249581455, 0.030966803125438657, 0.033463631707499] [0m
[34mINFO: 5bins_50_20_15_10_5 : estimated process scale-factor relative uncertainties (stat only): [0.3980200388426504, 0.4899205734632532, 0.03622427326866023, 0.029531214794704257] [0m
[32mPLOTS: use real data in the plots![0m
scale ZLIGHT by 2.335648569420504
scale ZB by 0.6275634238472647
scale ZBB by 0.8548633369611276
scale TT by 1.13316136637562
[32mPLOTS: use real data in the plots![0m
[32mPLOTS: use real data in the plots![0m
scale ZLIGHT by 2.335648569420504
scale ZB by 0.6275634238472647
scale ZBB by 0.8548633369611276
scale TT by 1.13316136637562
[32mPLOTS: use real data in the plots![0m
bin-list CRs  [30, 58, 79, 93]  =  [0.0, 0.3112214772464245, 0.3486324074704928, 0.3819625488853857, 0.42508355263786357, 1.0, 1.310586146823698, 1.3569120003100843, 1.4024533182488466, 1.4663061483773712, 2.0, 2.378821264549325, 2.4997622296397775, 2.6217527909333675, 2.7343756233363514, 3.0, 3.3132293331603826, 3.3723391744989764, 3.423600488856007, 3.4847210905668327, 4.0, 4.359806334434541, 4.442786277004368, 4.540939527460259, 4.663430892951342, 5.0]
      fun: 24.6052073687507
 hess_inv: array([[ 5.25302909e-01, -1.78641884e-01,  1.95146631e-04,
        -7.35761972e-03],
       [-1.78641884e-01,  6.41921798e-02, -9.48459367e-04,
         2.00540506e-03],
       [ 1.95146631e-04, -9.48459367e-04,  9.19803841e-04,
        -2.21457201e-04],
       [-7.35761972e-03,  2.00540506e-03, -2.21457201e-04,
         9.07555073e-04]])
      jac: array([-1.19209290e-06, -1.19209290e-06,  1.90734863e-06, -4.76837158e-07])
  message: 'Optimization terminated successfully.'
     nfev: 84
      nit: 9
     njev: 14
   status: 0
  success: True
        x: array([0.85426096, 1.14131989, 0.83423649, 1.16938793])
[34mINFO: 5bins_30_28_21_14_7 : estimated process scale-factors (without systematics): [0.85426096 1.14131989 0.83423649 1.16938793] [0m
[34mINFO: 5bins_30_28_21_14_7 : estimated process scale-factor uncertainties (stat only): [0.7247778344853889, 0.2533617567112096, 0.030328268012255914, 0.03012565473521256] [0m
[34mINFO: 5bins_30_28_21_14_7 : estimated process scale-factor relative uncertainties (stat only): [0.8484267323930174, 0.2219901352032655, 0.036354521046062925, 0.025761899867905183] [0m
[32mPLOTS: use real data in the plots![0m
scale ZLIGHT by 0.8542609595069307
scale ZB by 1.1413198900897945
scale ZBB by 0.8342364894266807
scale TT by 1.1693879290612355
[32mPLOTS: use real data in the plots![0m
[32mPLOTS: use real data in the plots![0m
scale ZLIGHT by 0.8542609595069307
scale ZB by 1.1413198900897945
scale ZBB by 0.8342364894266807
scale TT by 1.1693879290612355
[32mPLOTS: use real data in the plots![0m
bin-list CRs  [33, 67]  =  [0.0, 0.3150951980989734, 0.361708201477126, 1.0, 1.3161196095606882, 1.3741094403458207, 2.0, 2.391060438807542, 2.553009727714476, 3.0, 3.3185892450280567, 3.393365180822938, 4.0, 4.367760176728033, 4.478882411617135, 5.0]
      fun: 7.539106884140648
 hess_inv: array([[ 1.25672391e+00, -4.08280051e-01, -2.81089620e-03,
        -1.74377491e-02],
       [-4.08280051e-01,  1.37476152e-01, -1.71126836e-04,
         5.16146942e-03],
       [-2.81089620e-03, -1.71126836e-04,  9.89222488e-04,
        -1.89422565e-04],
       [-1.74377491e-02,  5.16146942e-03, -1.89422565e-04,
         1.05351031e-03]])
      jac: array([7.15255737e-07, 1.43051147e-06, 2.98023224e-06, 4.52995300e-06])
  message: 'Optimization terminated successfully.'
     nfev: 108
      nit: 14
     njev: 18
   status: 0
  success: True
        x: array([2.58235255, 0.53558836, 0.83262639, 1.1579771 ])
[34mINFO: 3binsFlat : estimated process scale-factors (without systematics): [2.58235255 0.53558836 0.83262639 1.1579771 ] [0m
[34mINFO: 3binsFlat : estimated process scale-factor uncertainties (stat only): [1.1210369800324993, 0.37077776638970394, 0.03145190754287787, 0.032457823498539824] [0m
[34mINFO: 3binsFlat : estimated process scale-factor relative uncertainties (stat only): [0.43411461412622443, 0.6922812295007622, 0.0377743341885392, 0.028029762786096302] [0m
[32mPLOTS: use real data in the plots![0m
scale ZLIGHT by 2.582352548275519
scale ZB by 0.5355883571436566
scale ZBB by 0.8326263908688676
scale TT by 1.1579771026332049
[32mPLOTS: use real data in the plots![0m
[32mPLOTS: use real data in the plots![0m
scale ZLIGHT by 2.582352548275519
scale ZB by 0.5355883571436566
scale ZBB by 0.8326263908688676
scale TT by 1.1579771026332049
[32mPLOTS: use real data in the plots![0m
bin-list CRs  [52, 84]  =  [0.0, 0.3407420704247829, 0.3934512980911484, 1.0, 1.347017225758059, 1.4202224273816724, 2.0, 2.469430092909848, 2.655476949998656, 3.0, 3.3595502414839937, 3.4390024649726607, 4.0, 4.422490457790461, 4.57534980857376, 5.0]
      fun: 24.712163348236736
 hess_inv: array([[ 7.44787149e-01, -2.63153755e-01,  3.74437557e-03,
        -1.00325332e-02],
       [-2.63153755e-01,  9.66264915e-02, -2.26718540e-03,
         3.07106912e-03],
       [ 3.74437557e-03, -2.26718540e-03,  9.77894152e-04,
        -2.80200294e-04],
       [-1.00325332e-02,  3.07106912e-03, -2.80200294e-04,
         9.54900437e-04]])
      jac: array([2.86102295e-06, 4.05311584e-06, 3.33786011e-06, 4.76837158e-07])
  message: 'Optimization terminated successfully.'
     nfev: 78
      nit: 9
     njev: 13
   status: 0
  success: True
        x: array([1.48592015, 0.89543397, 0.85822683, 1.14927102])
[34mINFO: 3bins_52_32_16 : estimated process scale-factors (without systematics): [1.48592015 0.89543397 0.85822683 1.14927102] [0m
[34mINFO: 3bins_52_32_16 : estimated process scale-factor uncertainties (stat only): [0.8630105151847388, 0.3108480199358696, 0.031271299167090004, 0.030901463351390768] [0m
[34mINFO: 3bins_52_32_16 : estimated process scale-factor relative uncertainties (stat only): [0.5807919846895442, 0.34714789920844813, 0.036437102611208416, 0.02688788190795266] [0m
[32mPLOTS: use real data in the plots![0m
scale ZLIGHT by 1.4859201537467006
scale ZB by 0.8954339653060036
scale ZBB by 0.8582268327084448
scale TT by 1.1492710157377999
[32mPLOTS: use real data in the plots![0m
[32mPLOTS: use real data in the plots![0m
scale ZLIGHT by 1.4859201537467006
scale ZB by 0.8954339653060036
scale ZBB by 0.8582268327084448
scale TT by 1.1492710157377999
[32mPLOTS: use real data in the plots![0m
bin-list CRs  [50]  =  [0.0, 0.3380026401031242, 1.0, 1.3437390950458474, 2.0, 2.458757427923178, 3.0, 3.3554329415550925, 4.0, 4.417006988369578, 5.0]
      fun: 7.23755952679841
 hess_inv: array([[ 1.23291483e+00, -4.46989202e-01,  1.93365200e-03,
        -1.22132118e-02],
       [-4.46989202e-01,  1.65579865e-01, -1.65430486e-03,
         3.96989236e-03],
       [ 1.93365200e-03, -1.65430486e-03,  1.00259464e-03,
        -2.73940254e-04],
       [-1.22132118e-02,  3.96989236e-03, -2.73940254e-04,
         9.16763076e-04]])
      jac: array([-2.32458115e-06, -7.56978989e-06, -9.65595245e-06, -1.60932541e-06])
  message: 'Optimization terminated successfully.'
     nfev: 114
      nit: 11
     njev: 19
   status: 0
  success: True
        x: array([-0.43414648,  1.57257364,  0.86458041,  1.17356475])
[34mINFO: 2binsFlat : estimated process scale-factors (without systematics): [-0.43414648  1.57257364  0.86458041  1.17356475] [0m
[34mINFO: 2binsFlat : estimated process scale-factor uncertainties (stat only): [1.1103669795276612, 0.4069150591996305, 0.03166377482285654, 0.03027809564097847] [0m
[34mINFO: 2binsFlat : estimated process scale-factor relative uncertainties (stat only): [-2.557586061418305, 0.25875739613038046, 0.036623285075822645, 0.025800106616433824] [0m
[32mPLOTS: use real data in the plots![0m
scale ZLIGHT by -0.4341464775233836
scale ZB by 1.5725736357100981
scale ZBB by 0.8645804099032014
scale TT by 1.1735647488251972
[32mPLOTS: use real data in the plots![0m
[32mPLOTS: use real data in the plots![0m
scale ZLIGHT by -0.4341464775233836
scale ZB by 1.5725736357100981
scale ZBB by 0.8645804099032014
scale TT by 1.1735647488251972
[32mPLOTS: use real data in the plots![0m
bin-list CRs  []  =  [0.0, 1.0, 2.0, 3.0, 4.0, 5.0]
      fun: 0.009379642330696861
 hess_inv: array([[ 8.42320246e-01, -2.99532899e-01, -2.82332897e-03,
        -7.11490102e-03],
       [-2.99532899e-01,  1.10186696e-01, -1.37137324e-05,
         2.00471489e-03],
       [-2.82332897e-03, -1.37137324e-05,  1.09088452e-03,
        -2.68391636e-04],
       [-7.11490102e-03,  2.00471489e-03, -2.68391636e-04,
         9.53087455e-04]])
      jac: array([-1.39116310e-07, -5.40749170e-07, -2.21771188e-07,  1.36205927e-08])
  message: 'Optimization terminated successfully.'
     nfev: 90
      nit: 10
     njev: 15
   status: 0
  success: True
        x: array([0.69034478, 1.17266279, 0.84483598, 1.18033512])
[34mINFO: 1bin : estimated process scale-factors (without systematics): [0.69034478 1.17266279 0.84483598 1.18033512] [0m
[34mINFO: 1bin : estimated process scale-factor uncertainties (stat only): [0.917780064289358, 0.331943814354474, 0.033028541018137236, 0.0308721145210891] [0m
[34mINFO: 1bin : estimated process scale-factor relative uncertainties (stat only): [1.32945172974637, 0.2830684299214662, 0.039094619256589136, 0.026155380702606378] [0m
[32mPLOTS: use real data in the plots![0m
scale ZLIGHT by 0.6903447818029844
scale ZB by 1.172662788452134
scale ZBB by 0.8448359811707463
scale TT by 1.1803351238551347
[32mPLOTS: use real data in the plots![0m
[32mPLOTS: use real data in the plots![0m
scale ZLIGHT by 0.6903447818029844
scale ZB by 1.172662788452134
scale ZBB by 0.8448359811707463
scale TT by 1.1803351238551347
[32mPLOTS: use real data in the plots![0m
