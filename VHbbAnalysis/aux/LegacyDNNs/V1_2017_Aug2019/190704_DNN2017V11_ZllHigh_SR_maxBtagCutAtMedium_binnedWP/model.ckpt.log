saving logfile to [34m results//2lepHigh_V11_v6_St_v2_DeepCSVbins_cutMaxAtMedium/Zll2017_ZllBDT_highpt_190627_V11-kinFitv6-Stitching.h5/512-256-128-64-64-64/0.20-0.40-0.50-0.60-0.70-0.80/5.000e-04/rnd_10/output.txt [0m
INFO: numpy random state =  MT19937 ,cc3683ef,59235d62,770ebdb7,98682b34,56746965,e38923a5,71aaf070,911d5f44,89b3d20c,2246ed99,7324041b,8c3a3c41,86bb248f,10e75244,59e0f4f9,70b1c006,419d04f,2ae81022,1a9521b,47ed2a2b,7a884052,443d6cb8,70a5476d,17473b26,a80bf8f9,7fd0590,7550f326,6bbadf49,154c0c5,36aaa0d2,23dd2bb,a2c033a7,c4dddf6f,b100e0c6,bf55709a,2c949c84,a6f93a66,173cb9e6,28ef6eac,2cce4d58,504a95f1,f495461a,90c715dd,5cc51da2,c46a2d90,d2dbf830,de360a88,f33e6a4d,587c72ea,ad762ab7,e92a00dc,feeb5970,37b77ca,d444176e,977590ec,1a8fd32e,131a987f,9bf1a309,5707e5af,b8f55c15,11a2eebf,87f39172,a27339d5,7c9d1f1,3de7ff6a,69d6868d,b04ced38,afe86637,10f4558e,39b7c520,d9f67351,8149e59f,987e36ac,979e3006,78124ff3,c511843c,82110be2,2ac067cf,ac7a20f4,72714b9f,e57403b0,11498aef,e3407db0,90369f0d,e3780b07,9645e40f,d9cec86c,9752920e,684a2e55,610baf1f,10c4e13f,c459fc5e,f74b1b08,5aa8d542,7eecbdbd,79b6fafd,e82e2f59,9d40710c,5285ac38,217d017b,de4883b6,fd79ed41,2b4eb306,75d27b7d,a0b56290,e512e8c4,a2b4ec1e,7979a66a,7a25592e,e19d1585,be8271dc,4205f1d0,b125d845,333033f8,1062674a,274f85f9,3fc94af,b3cb5ec,b484ac5d,813c4c06,b0e48dd0,5fc1dd03,ade4757a,f7104095,7e37f680,d8fd781e,e83f15cb,c6233a24,f33f28ca,27243eb,2d2de51,8e4db5a2,2d3a4c6e,cd4334a3,9327943f,934f634c,47df623c,7a4f68e1,2987935f,e4f569ad,afc55b6a,ea962eb1,d6e18f31,2f88ae5d,cdb3e17c,d58b747a,30704c0e,79fe0adc,58a620c4,625c29f1,6a0d1ef5,7478296f,c89c8f4f,3c4353c9,1fe38638,9247cb94,bd82a6c8,967182a,d1fd892c,5d55cc1c,fd0e0846,152c29a2,2cf29698,612a4594,a19b12be,c02c8d90,fc03e2fc,6b22f800,33ea3941,bb7a3934,722db2be,5a44bb58,73a71476,54c84fc1,e8146854,e3a66ac3,16523520,5294e0bd,89336630,3bda9cbb,66c641ac,9583d6f2,6f3aab55,5a1ca175,ca09d143,b37b1946,733e6e57,96c9318d,7cee6c45,497fe683,1c798510,6e697075,ce5eb1c8,f2a82712,f66cee02,e40571bd,a08e99b5,c12f02e2,fbfa0269,b695754,aa9184eb,a0d8e798,4b9cd83d,baec4770,4497f284,349ec30c,c499561a,4b7ad91d,9e1be297,1d3803b7,387e1ed9,f9f2fdb6,37c2606b,ef48a2fe,10e6476e,c63b3147,32407e41,85d7816f,e02a0eaa,2aecc25,1367681a,5fc4c457,7a07ae9f,b0dfa31d,62cc4548,52301cfa,9872d6b2,af09ca8b,befd6230,c45f4ce,40b8328f,41968c7e,3b75e1e8,7c9d999e,3ff99c63,77fc7cf4,4c913f11,22d8a29,709ee799,1d09f8db,aa1e1d5,83ce5010,b65e0ca5,94c5869b,54ccf91e,291fc5fb,bfa713af,a6b7c2de,c4f92331,a5e322d,187c3f68,5eda52c7,c64d767d,de25484,ce5b4d00,3c0b47e2,461072f2,656949e9,21fd233f,d712f3df,fa12d7cd,997add89,543239a8,49448dfc,1bdc0d48,e9b5e0ba,6924e7f6,a6818854,816f4f8e,975eee75,5049eee7,bfd7f71f,e3a03d05,7f5c7169,26456e94,2cf903fc,3d0add72,430e5d70,16931729,c64440b4,d2f8f543,3cd68c39,4fb8698a,d43bceb4,ea85c3b1,d6331800,831ae338,649173bb,3dc63420,d913f79c,f5096366,537e40eb,732b590b,7d4260ca,41495a04,4e556e97,944a19f0,72056f4d,4b4a0e4b,3200b407,5e66854c,f3a6e12a,cb473ab6,6fa93c81,b2606709,39fb3d16,138825f3,364234a6,6396d1c9,fa519463,cc689928,993d8700,8f072ef8,da0a0b85,1779a238,2a12bf47,d46246d2,f798637,1a0fb963,9286de93,c1330d65,54850401,b5cccec3,ed9cb42a,6c97d4a8,79dbd853,f0d554cc,bcbd3f6a,18dbe7cb,39a5e243,b67fe362,1fd674e,ae0a3bc3,252a5193,eb67715e,6f582b92,eb3893bc,9e14ca89,817a9e91,de28043b,fcc5c01,99781e97,34bc0ad8,83f037f1,586c0749,eccd88b4,2c380567,6f01f9ae,6ecb06e9,996247a,38066055,c7bc6288,a8ccbf88,fc327e97,af468bd5,660d3634,befb237b,ed2b2022,45b7a456,b2b2cc0f,f0a2a298,2a2ce5ae,fe419445,2eb51a59,6842fe51,3ae487df,f12ff4be,4395d721,f1a36a10,7a864b0f,5cd8bb56,de53ebbc,d6b1144e,aaf184e,49cc7a4e,50d10de2,734c653b,a6e46f24,7749804b,bfe51965,6b594229,27ee58b8,7457421c,424f3510,9408822f,baf04425,1f349c16,82d0c391,68ab82bf,d8dc157,b1626d9a,44cf63c8,246cde42,d1edf217,7b4a5e9f,2c4db072,297ce9fa,74171455,4432e1bd,8eb5e69d,27d1d214,5f70d128,581919bc,8af9bae,8b094284,4573d389,ef1f09e,469307ff,fe1fcd7a,7024ec9b,c7696c6a,d8745c5f,207fb50d,3674638b,4d9d4a11,5a8405da,85ec97c6,2bd15177,635d14d7,d5641850,2b4511b,80de210d,abadf4f1,45df78d0,6425f6f6,d0f1e793,3757e18a,cecd8bef,f0894374,d933e897,967eb14f,7b878f34,cd94b6b,996d53b0,f8c244d9,740f6a9a,5cd6068,810d2135,a527f2b5,121cad3f,2a5b23a1,207821a3,91d43b27,199d5d75,5b3ef10a,48f5d5d5,e6c7d53,2b4171e6,b2671899,73c5a4,acd3a9f0,543bb22a,4e31410a,9c293ec3,2e99e16d,1e827dfb,2b35f8f5,a2bf0aa2,b980bb82,253cebb4,2ddc84e0,979405d5,76928dfb,5df99109,8ff8dc75,bb71da69,ea6cf05,9b889b97,a5686e08,73752902,56abb9b4,20fe4a10,2ae30636,5befdc1b,605e4f20,1f9e7d6e,7ec62287,8d59c33e,7249d931,9db8a6e1,60aabe84,d73742f0,2ee46c83,e6bc6007,dbe8003d,fa36a6,9382f976,be8d9395,a111a985,f9d360fb,fb6f1b2b,666e602c,3ccdef9f,80a287ee,5c50e38,4b1a0a95,8537fa20,cbfaf771,2d79bfaa,626875dc,962047df,bfd85c91,7bebf8ac,694b7c73,6397e72d,9e2aadde,703a9c59,806447db,2248ab0c,3e5bc9c5,cba0c0df,fb95f661,5fd42eca,16b6e83b,2de4619,30ea5a2a,d64a3881,cb399233,965a3f49,1cd78fae,41091ba2,22645aae,845d2e60,eb89680f,7c974a32,6af05f0e,b2931ba6,26fcec25,53db3214,b85c4848,677080f8,84d00950,4f56e886,861284cc,b3ad292d,e745b2a0,2d6e1e1c,26e2dc01,44b39674,5fc019b1,7a24c5eb,e11f945d,15d73823,27406a1a,dec80aca,f0b34d32,b398d896,8ec0f851,f9e92ffe,3e77c567,c8c25640,d28d3713,f5cc2506,61a5e379,b0fe6895,bf8927b5,6e6491d7,226fbeb3,4158e8e2,937097ad,616ba542,67c95d6b,a082e575,d98bb172,41b4ad18,32b35d1e,69fa103c,33f69cea,f86ca75d,e179150a,7f738898,690ba117,5ff6db37,6b8c0591,444d7a1,8ef91e7,23842703,fb71ad8,29fe9aed,93477f91,78c4d20,50f910be,9e883fb1,7a6fe457,bbe21851,a6fc31f7,b4ba00ca,18d24d37,6742c347,6c4f2c4a,c6b73e35,78dd9b57,9ec7fb7d,a3549463,fac928b8,e5f5cc0b,e0dd2a71,75a3a4b5,75a8c667,736ca76c,ff8a47af,3958a344,a948a7c7,6d383a6c,d316c42c,57938a89,b4609ad6,ad54a808,a032f38d,c6ea758c,b9f66419,46f4e23f,8ce00d86,d62f1387,1c616b2d,5eeea63,ae452067,f81be059,6970d94b,30db8ea6,acaf40cf,273dd463,fe531d7d,9e071522
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------
initialized TensorflowDNNClassifier, version v0.1 ( 12006 )
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------
LOG: command: /var/lib/slurm-llnl/slurmd/job06629/slurm_script -c config/high_dropout.cfg -i /data/VHbb/2017/Zll/kinematicFit/Zll2017_ZllBDT_highpt_190627_V11-kinFitv6-Stitching.h5 -p 2lepHigh_V11_v6_St_v2_DeepCSVbins_cutMaxAtMedium --set='preprocess="np.concatenate((x[:6],np.atleast_1d(np.piecewise(x[6],[x[6]<0.1522,(x[6]>=0.1522)&(x[6]<0.4941),(x[6]>=0.4941)&(x[6]<0.8001),x[6]>=0.8001],[0,1,2,3])),np.atleast_1d(np.piecewise(x[7],[x[7]<0.1522,(x[7]>=0.1522)&(x[7]<0.4941),(x[7]>=0.4941)&(x[7]<0.8001),x[7]>=0.8001],[0,1,2,3])),x[8:]))";selection="(x[:,6]>=1.5)";plot-scores=True;inputPlotRange={"Jet_btagDeepB[hJidx[0]]":[-1,4], "Jet_btagDeepB[hJidx[1]]":[-1,4],}'
INFO: read inputs from disk, metadata is pesent:
INFO:  >   cut (V_mass > 75 && V_mass < 105 && (H_mass > 90 && H_mass < 150) && Jet_btagDeepB[hJidx[0]] > 0.1522 && Jet_btagDeepB[hJidx[1]] > 0.1522 && 1) && (isZee||isZmm) && (V_pt>150)
INFO:  >   cutName ZllBDT_highpt
INFO:  >   region ZllBDT_highpt
INFO:  >   samples {u'SIG_ALL': [u'ZH_Zll', u'ZH_Znunu', u'ggZH_Zll', u'ggZH_Znunu', u'WplusH', u'WminusH'], u'BKG_ALL': [u'ST_tW_antitop', u'ST_tW_top', u'ST_s-channel_4f', u'ST_t-channel_top_4f', u'ST_t-channel_antitop_4f', u'TT_2l2n', u'TT_h', u'TT_Sl', u'M4HT100to200_0b', u'M4HT100to200_1b', u'M4HT100to200_2b', u'M4HT200to400_0b', u'M4HT200to400_1b', u'M4HT200to400_2b', u'M4HT400to600_0b', u'M4HT400to600_1b', u'M4HT400to600_2b', u'M4HT600toInf_0b', u'M4HT600toInf_1b', u'M4HT600toInf_2b', u'HT0to100ZJets_0b', u'HT0to100ZJets_1b', u'HT0to100ZJets_2b', u'HT100to200ZJets_0b', u'HT100to200ZJets_1b', u'HT100to200ZJets_2b', u'HT200to400ZJets_0b', u'HT200to400ZJets_1b', u'HT200to400ZJets_2b', u'HT400to600ZJets_0b', u'HT400to600ZJets_1b', u'HT400to600ZJets_2b', u'HT600to800ZJets_0b', u'HT600to800ZJets_1b', u'HT600to800ZJets_2b', u'HT800to1200ZJets_0b', u'HT800to1200ZJets_1b', u'HT800to1200ZJets_2b', u'HT1200to2500ZJets_0b', u'HT1200to2500ZJets_1b', u'HT1200to2500ZJets_2b', u'HT2500toinfZJets_0b', u'HT2500toinfZJets_1b', u'HT2500toinfZJets_2b', u'ZJetsB_Zpt100to200_0b', u'ZJetsB_Zpt100to200_1b', u'ZJetsB_Zpt100to200_2b', u'ZJetsB_Zpt200toInf_0b', u'ZJetsB_Zpt200toInf_1b', u'ZJetsB_Zpt200toInf_2b', u'ZJetsGenB_Zpt100to200_0b', u'ZJetsGenB_Zpt100to200_1b', u'ZJetsGenB_Zpt100to200_2b', u'ZJetsGenB_Zpt200toInf_0b', u'ZJetsGenB_Zpt200toInf_1b', u'ZJetsGenB_Zpt200toInf_2b', u'WWnlo_0b', u'WWnlo_1b', u'WZnlo_0b', u'WZnlo_1b', u'ZZnlo_0b', u'ZZnlo_1b', u'WWnlo_2b', u'WZnlo_2b', u'ZZnlo_2b']}
INFO:  >   systematics []
INFO:  >   testCut ((event%2)==0||isData)
INFO:  >   trainCut !((event%2)==0||isData)
INFO:  >   variables kinFit_H_mass_fit H_mass kinFit_H_pt_fit H_pt kinFit_HVdPhi_fit abs(VHbb::deltaPhi(H_phi,V_phi)) Jet_btagDeepB[hJidx[0]] Jet_btagDeepB[hJidx[1]] kinFit_hJets_pt_0_fit Jet_PtReg[hJidx[0]] kinFit_hJets_pt_1_fit Jet_PtReg[hJidx[1]] kinFit_V_mass_fit V_mass Sum$(Jet_Pt>30&&abs(Jet_eta)<2.4&&Jet_puId==7&&Jet_jetId>0&&Jet_lepFilter>0&&Iteration$!=hJidx[0]&&Iteration$!=hJidx[1]) kinFit_V_pt_fit V_pt kinFit_jjVPtRatio_fit (H_pt/V_pt) abs(Jet_eta[hJidx[0]]-Jet_eta[hJidx[1]]) SA5 VHbb::deltaR(kinFit_H_eta_fit,kinFit_H_phi_fit,kinFit_V_eta_fit,kinFit_V_phi_fit) VHbb::deltaR(H_eta,H_phi,V_eta,V_phi) MET_Pt kinFit_H_mass_sigma_fit kinFit_n_recoil_jets_fit VHbb::deltaR(Jet_eta[hJidx[0]],Jet_phi[hJidx[0]],Jet_eta[hJidx[1]],Jet_phi[hJidx[1]])
INFO:  >   version 3
INFO:  >   weightF genWeight*puWeight*1.0*muonSF_Iso[0]*muonSF_Id[0]*electronSF_IdIso[0]*electronSF_trigger[0]*bTagWeightDeepCSV*EWKw[0]*weightLOtoNLO*FitCorr[0]
INFO:  >   weightSYS []
INFO: random state: (3, (2147483648L, 329901780L, 479245387L, 3181671559L, 984746386L, 4156890662L, 173623454L, 282696590L, 2940621963L, 2785470781L, 131754735L, 2160952229L, 1274477876L, 3156191196L, 2599292914L, 3345675944L, 1840786754L, 191634848L, 2429292514L, 1404572544L, 710185847L, 1874985118L, 602426841L, 1611530603L, 458291538L, 592133737L, 2514365078L, 2929017943L, 689013441L, 1717434454L, 1438331785L, 502794543L, 1931952094L, 722006966L, 860351729L, 580299455L, 155202556L, 3566801733L, 1035144257L, 2212840850L, 3157024053L, 1810323472L, 1714369965L, 1047145424L, 1634244735L, 2584811476L, 3958580943L, 1490114663L, 4292356283L, 3096985092L, 1120097160L, 2785333661L, 3583026226L, 4212404730L, 504742167L, 1221715095L, 2263890649L, 2093105539L, 643112341L, 3219157570L, 2869777662L, 824852291L, 106147082L, 3147499512L, 1873957197L, 3220912525L, 3790605772L, 3766875044L, 3597337032L, 137865609L, 608761822L, 1134874354L, 1370561574L, 1575561309L, 399578063L, 2303106664L, 3679063068L, 3755980359L, 3170277044L, 2441067524L, 1765233362L, 2561067962L, 1800866640L, 2715332003L, 4281662262L, 4157580918L, 1177833628L, 588773889L, 760423352L, 861260778L, 1698107383L, 3604752035L, 3666752142L, 3104821075L, 1016812700L, 2555715571L, 3239537560L, 2301450142L, 3478574173L, 850220796L, 1966701643L, 1309548309L, 827622797L, 683365652L, 390758723L, 2666275810L, 1622079950L, 185425945L, 3360742348L, 651053555L, 4122478077L, 3323163154L, 4171459139L, 2516018209L, 3614633417L, 502814658L, 398420469L, 1585713876L, 256932256L, 2119704102L, 517540306L, 1592458753L, 4214341710L, 691725708L, 3242781096L, 655399950L, 1166614444L, 4234872645L, 2168436514L, 2136510099L, 1416944163L, 374162776L, 316890879L, 2138937829L, 2827588837L, 980843327L, 3375744945L, 3460888944L, 2165762605L, 1646492815L, 1886985973L, 633375372L, 1416651646L, 2482708621L, 3548952041L, 2506217294L, 3703877195L, 1199729230L, 3837009389L, 4262919912L, 1492919506L, 1978275747L, 1787713265L, 3419175787L, 2083587186L, 2428888413L, 2191675012L, 2052861858L, 2799228876L, 4175772868L, 2438653497L, 39913345L, 369489091L, 4262518934L, 4243161899L, 2503902597L, 3085310224L, 2258842994L, 2621203859L, 4006709487L, 2044541531L, 3210473139L, 2642790426L, 1034420524L, 2415403552L, 405821876L, 1097817678L, 1807298642L, 1377987682L, 2017107501L, 403939846L, 3306117592L, 2453194658L, 3705949903L, 3543733394L, 3147652745L, 137840991L, 1293314407L, 2002552390L, 1676844487L, 3341260275L, 1808473018L, 2009696868L, 1457083862L, 4176510071L, 2052026356L, 2948247405L, 2434298538L, 3147521260L, 484959912L, 2449760220L, 3855113900L, 581851899L, 3263809279L, 2098666875L, 2824426997L, 1111737535L, 3967317658L, 1057088867L, 1855081386L, 4171900496L, 557555871L, 861171954L, 2706741382L, 1039612140L, 2470398062L, 3691176666L, 922994733L, 637558524L, 2002234329L, 1457760212L, 908546062L, 864080147L, 2602191831L, 719517629L, 3795080521L, 1310639728L, 2560304484L, 2781905464L, 3731621910L, 3205000166L, 225360650L, 1275062793L, 687738316L, 2925291568L, 2961145686L, 3519382423L, 3282291899L, 3625421696L, 2372816433L, 4140749032L, 3375423181L, 2707301637L, 3497163245L, 2111292148L, 3122820948L, 3138359279L, 3765825730L, 3647970845L, 3137119990L, 3566400325L, 3811781387L, 334797787L, 3374752902L, 2707787312L, 824212578L, 2099558601L, 2966309318L, 793841600L, 2385699511L, 1323531256L, 2870270311L, 4206865378L, 851313579L, 2067175210L, 2404722307L, 1197366248L, 4287337812L, 770044441L, 4190895392L, 3650998450L, 2926064641L, 831550103L, 2605209365L, 3717714557L, 1493749887L, 831880859L, 508180597L, 1035543571L, 3451624304L, 1440574729L, 3748010300L, 3010420413L, 2100932956L, 2338856177L, 1035236868L, 800972400L, 104027439L, 1367504684L, 816053051L, 330515699L, 3937097925L, 3367430374L, 3705475727L, 4246676000L, 946048572L, 1470917954L, 1993602254L, 1746531908L, 634691853L, 2523570686L, 2528444846L, 1775715027L, 1359129021L, 1275954070L, 3020877288L, 102708496L, 345581187L, 3329085705L, 471964339L, 1189765297L, 1201763975L, 8844148L, 2069815317L, 531400791L, 3606509079L, 2425762123L, 4289989036L, 3414032510L, 2571877893L, 1029268269L, 3381734250L, 460141640L, 405682267L, 3864139769L, 761548597L, 1338820683L, 1243574476L, 3677319914L, 3951905064L, 2787399728L, 2467549532L, 3430603595L, 2342727508L, 1066682184L, 2241844065L, 3977138465L, 3289998295L, 1949014566L, 3273973877L, 987581767L, 1129685556L, 1035080861L, 2572500034L, 4117169490L, 2266319863L, 1592503224L, 4201816805L, 1162354093L, 801154216L, 752782988L, 1092284774L, 2681302316L, 1750519924L, 15973060L, 119984902L, 2202042995L, 2077073688L, 816481132L, 4080045982L, 3299780416L, 1404227197L, 3170723414L, 2237441249L, 1846738176L, 1341732274L, 438228223L, 1251937477L, 128356142L, 4047941637L, 700867935L, 943213046L, 2452986765L, 3875371169L, 1910434463L, 3232916023L, 1224862236L, 3268479780L, 1644573705L, 1960411567L, 1617912504L, 3229786534L, 1204562957L, 2925228789L, 2172148362L, 669223413L, 966700746L, 1920214565L, 278935068L, 4052974197L, 555636774L, 374904468L, 3209711743L, 1093339456L, 3599380561L, 2419505106L, 3533179458L, 1434223520L, 2870493223L, 2718701778L, 560808074L, 934571555L, 911823152L, 2650724470L, 155095284L, 81640299L, 487441558L, 1271164802L, 612197793L, 1493719340L, 971716084L, 2876052787L, 2257191676L, 793440984L, 1862766000L, 3624069921L, 3995040341L, 2050718483L, 3366443008L, 688595246L, 1550388729L, 2498028816L, 2000984156L, 2970879116L, 3844992948L, 2876304204L, 3833158941L, 3443473243L, 1006709006L, 4185930012L, 2249482076L, 921349661L, 3332245310L, 4073397331L, 2540927789L, 216052113L, 2195120458L, 4159642830L, 4127867901L, 1942767946L, 2998083448L, 1314067472L, 2306344386L, 2236936521L, 3582428754L, 3236965534L, 3435304529L, 1470231099L, 1058584989L, 3392542671L, 3422879741L, 3259494501L, 785850859L, 2386825024L, 2112374583L, 2111839507L, 2867267074L, 1163048457L, 610365515L, 3547817577L, 1034173272L, 2147422427L, 3076827271L, 3491826893L, 376636703L, 1457456393L, 2881967917L, 3596677908L, 3160712109L, 3921879644L, 1178620022L, 1567305035L, 2184568040L, 493045346L, 2288373277L, 2795376629L, 549432137L, 3303953089L, 1875657556L, 3007860566L, 1785958633L, 3353118044L, 2857623049L, 3336772456L, 2022633536L, 3258761482L, 2303317278L, 820010436L, 2250655599L, 4178174907L, 1158915035L, 2381614365L, 3282590932L, 467541928L, 4271843044L, 2767484288L, 1575595071L, 1973791696L, 913177379L, 335931576L, 3023746381L, 1876434744L, 2650197041L, 1906023955L, 2720588031L, 1346478670L, 1788250206L, 3880317814L, 2546470593L, 2520603798L, 3150991989L, 973624606L, 2688737106L, 862401730L, 1872352241L, 861425832L, 2987104884L, 3835798936L, 3029012388L, 2426195572L, 3870567829L, 474275435L, 3735506725L, 1765734247L, 1836391641L, 2191797561L, 102335757L, 766674958L, 2870399909L, 3100900818L, 2344190956L, 2458327234L, 3140706574L, 3443463188L, 1716849231L, 532915599L, 3047654412L, 3203293160L, 2292485463L, 4149860559L, 988451823L, 3167490982L, 1912411484L, 1042434071L, 4280716349L, 2699441051L, 3900790533L, 4169510119L, 2648635293L, 1930498343L, 139743248L, 1671185777L, 3890624741L, 3391754412L, 1053177662L, 3409689008L, 3233546097L, 3219239662L, 2613830915L, 1514975914L, 3906064484L, 828788060L, 1071568960L, 3898301451L, 482966817L, 2246370641L, 4015408410L, 4049581518L, 4059827928L, 3513711183L, 2608392507L, 69431235L, 181455488L, 2347806467L, 4072326529L, 519945352L, 2059857414L, 3569646534L, 3949335195L, 1514363629L, 1402750505L, 868409031L, 1392858235L, 1729269579L, 1355851660L, 3217724447L, 885419900L, 2357262762L, 4184762048L, 3102297193L, 3174628782L, 1541785478L, 3470418604L, 3673411718L, 1452544372L, 2931909761L, 2341697249L, 167103121L, 4045798893L, 3310738130L, 272124387L, 3280487002L, 3393938255L, 3313864051L, 1347776125L, 1489858119L, 141075762L, 3634571316L, 967699562L, 2322241698L, 2905259347L, 3402016193L, 2734950755L, 2374545648L, 1670428939L, 801865580L, 2485423307L, 4075701879L, 305095838L, 1473661553L, 209075435L, 1057306862L, 1000505957L, 4172634834L, 624L), None)
INFO: preprocess test  X, mapfn= np.concatenate((x[:6],np.atleast_1d(np.piecewise(x[6],[x[6]<0.1522,(x[6]>=0.1522)&(x[6]<0.4941),(x[6]>=0.4941)&(x[6]<0.8001),x[6]>=0.8001],[0,1,2,3])),np.atleast_1d(np.piecewise(x[7],[x[7]<0.1522,(x[7]>=0.1522)&(x[7]<0.4941),(x[7]>=0.4941)&(x[7]<0.8001),x[7]>=0.8001],[0,1,2,3])),x[8:]))
INFO: preprocess train  X, mapfn= np.concatenate((x[:6],np.atleast_1d(np.piecewise(x[6],[x[6]<0.1522,(x[6]>=0.1522)&(x[6]<0.4941),(x[6]>=0.4941)&(x[6]<0.8001),x[6]>=0.8001],[0,1,2,3])),np.atleast_1d(np.piecewise(x[7],[x[7]<0.1522,(x[7]>=0.1522)&(x[7]<0.4941),(x[7]>=0.4941)&(x[7]<0.8001),x[7]>=0.8001],[0,1,2,3])),x[8:]))
INFO: selection for test is '(x[:,6]>=1.5)': 209964  -->  186647 88.89 %
INFO: selection for train is '(x[:,6]>=1.5)': 209235  -->  185927 88.86 %
nFeatures =  27
--------------------------------------------------------------------------------
statistics for dataset: train
--------------------------------------------------------------------------------
SIG_ALL (y= 0 ) : 146750  avg weight: 0.00036557489553877234
BKG_ALL (y= 1 ) : 39177  avg weight: 0.07246298096193966
--------------------------------------------------------------------------------
statistics for dataset: test
--------------------------------------------------------------------------------
SIG_ALL (y= 0 ) : 147529  avg weight: 0.0003649059955688602
BKG_ALL (y= 1 ) : 39118  avg weight: 0.0719286178349042
list of classes: (signals in [32mgreen[0m, backgrounds in [31mred[0m)
[32mclass 0 => SIG_ALL [0m is defined as a SIGNAL
[31mclass 1 => BKG_ALL [0m
weights:
train 0.0007160854 0.0011524784 0.00033389582 0.00071744004 0.00070742687 0.0008112496 0.00066039304 0.0009296102 0.0006432643 0.0007784817
test  0.0011021479 8.093302e-05 0.0002775393 0.0009652979 0.0001886169 0.00037404537 0.0003955271 0.0007240527 0.0006012493 0.00065871776
weights errors:
train 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
test  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
kinFit_H_mass_fit                                  train 122.64354 107.72562 112.657684 116.01292 118.450714 97.33056 102.831345 83.76432
kinFit_H_mass_fit                                  test  101.3274 118.1076 132.89279 138.50563 114.45057 115.92343 87.65719 113.11306
H_mass                                             train 135.48207 111.43505 122.922356 134.17542 112.62 123.07045 100.465775 134.70195
H_mass                                             test  101.80184 121.47412 141.86191 136.81494 134.66808 124.726654 111.03089 113.51701
kinFit_H_pt_fit                                    train 325.8613 171.06068 278.86642 124.23835 200.31783 162.89285 228.21086 334.81323
kinFit_H_pt_fit                                    test  277.18286 222.28981 65.26276 235.34213 336.56427 315.86093 131.07191 358.775
H_pt                                               train 323.1934 170.78357 296.07202 121.14706 177.25124 175.13643 216.35896 360.8937
H_pt                                               test  273.0791 221.10985 59.407196 225.95583 381.923 314.6815 168.25931 338.09525
kinFit_HVdPhi_fit                                  train 2.5500062 3.1059263 3.044465 3.3288653 3.1265848 3.2449715 3.121702 3.1238031
kinFit_HVdPhi_fit                                  test  3.1274498 2.9817007 4.306519 3.1839318 3.1546931 3.2490213 3.2638414 3.132788
abs(VHbb::deltaPhi(H_phi,V_phi))                   train 2.415794 3.0041611 2.9312637 2.8790722 3.0921555 3.0884833 3.0927324 3.0781398
abs(VHbb::deltaPhi(H_phi,V_phi))                   test  3.0845675 2.9970458 1.7408799 3.0999086 3.097613 2.9932735 3.0419755 3.10023
Jet_btagDeepB[hJidx[0]]                            train 3.0 3.0 3.0 3.0 3.0 3.0 3.0 3.0
Jet_btagDeepB[hJidx[0]]                            test  2.0 3.0 3.0 3.0 3.0 3.0 3.0 3.0
Jet_btagDeepB[hJidx[1]]                            train 3.0 1.0 3.0 3.0 2.0 1.0 3.0 1.0
Jet_btagDeepB[hJidx[1]]                            test  1.0 3.0 2.0 1.0 3.0 2.0 3.0 3.0
kinFit_hJets_pt_0_fit                              train 53.06204 82.845345 153.53477 15.819726 152.02292 133.46605 24.228035 45.3853
kinFit_hJets_pt_0_fit                              test  239.2029 174.96072 75.88459 197.78268 184.5943 146.4622 81.05175 106.57297
Jet_PtReg[hJidx[0]]                                train 70.030266 93.601746 154.73953 22.24407 132.61816 128.29628 21.846416 57.082394
Jet_PtReg[hJidx[0]]                                test  239.14447 175.8393 67.626396 190.61429 213.90353 144.73813 100.2595 95.56673
kinFit_hJets_pt_1_fit                              train 282.01947 119.253204 126.704216 115.85349 77.828354 31.088943 204.92287 290.85083
kinFit_hJets_pt_1_fit                              test  43.966244 74.18619 34.689255 70.54145 159.93814 169.80345 75.63481 267.53323
Jet_PtReg[hJidx[1]]                                train 263.04932 108.807304 142.59058 108.61699 72.93805 48.70867 195.32516 159.5614
Jet_PtReg[hJidx[1]]                                test  39.350853 71.506676 43.602543 67.31913 176.56958 170.3527 98.33647 257.17157
kinFit_V_mass_fit                                  train 90.69473 101.04974 89.105354 89.2541 89.007996 85.54864 87.61991 90.20606
kinFit_V_mass_fit                                  test  92.02158 92.343 89.24857 95.44636 91.72731 80.50804 90.79997 91.722244
V_mass                                             train 92.650894 102.45404 88.735725 89.12876 89.42429 82.82424 87.20986 84.04585
V_mass                                             test  93.79321 92.726944 89.166336 96.41487 91.02196 77.862175 90.62116 92.02361
Sum$(Jet_Pt>30&&abs(Jet_eta)<2.4&&Jet_puId==7&...  train 1.0 0.0 2.0 1.0 0.0 1.0 0.0 1.0
Sum$(Jet_Pt>30&&abs(Jet_eta)<2.4&&Jet_puId==7&...  test  0.0 1.0 2.0 0.0 0.0 0.0 0.0 0.0
kinFit_V_pt_fit                                    train 416.4233 171.63478 182.81805 212.83406 205.09294 195.3575 230.20146 331.4335
kinFit_V_pt_fit                                    test  277.42688 252.34023 155.93059 209.06706 331.7506 319.00067 175.82632 361.0517
V_pt                                               train 428.55362 172.63237 182.02151 212.90616 206.6852 189.73381 230.28674 305.14468
V_pt                                               test  287.24283 252.8826 156.0256 209.87387 328.31448 317.15436 175.49315 361.71234
kinFit_jjVPtRatio_fit                              train 0.7825241 0.9966551 1.5253768 0.5837334 0.9767173 0.83381927 0.9913528 1.0101973
kinFit_jjVPtRatio_fit                              test  0.9991204 0.88091314 0.4185372 1.1256777 1.0145099 0.9901576 0.7454624 0.99369425
(H_pt/V_pt)                                        train 0.75414926 0.98929054 1.626577 0.5690162 0.8575904 0.9230639 0.9395198 1.182697
(H_pt/V_pt)                                        test  0.95069075 0.8743577 0.38075286 1.0766268 1.1632841 0.99220294 0.95877993 0.9347075
abs(Jet_eta[hJidx[0]]-Jet_eta[hJidx[1]])           train 0.7551718 0.047973633 0.7641907 2.1020813 0.33325195 1.3597412 1.3293457 0.7967529
abs(Jet_eta[hJidx[0]]-Jet_eta[hJidx[1]])           test  0.79003906 0.3137207 1.704834 0.42611694 0.50439453 0.7128601 0.22546387 0.23327637
SA5                                                train 1.0 1.0 2.0 1.0 0.0 5.0 -1.0 1.0
SA5                                                test  0.0 4.0 4.0 2.0 -1.0 0.0 2.0 -1.0
VHbb::deltaR(kinFit_H_eta_fit,kinFit_H_phi_fit...  train 2.5546207 3.1076903 3.079145 2.9707851 3.1268272 3.097909 3.1247842 3.285546
VHbb::deltaR(kinFit_H_eta_fit,kinFit_H_phi_fit...  test  3.2350261 3.0046477 2.3283324 3.2084873 3.1340322 3.0341728 3.2278135 3.1510334
VHbb::deltaR(H_eta,H_phi,V_eta,V_phi)              train 2.4185953 3.0058632 2.9711628 2.8819652 3.0925536 3.1329713 3.0961308 3.2333384
VHbb::deltaR(H_eta,H_phi,V_eta,V_phi)              test  3.1980073 3.0194016 2.2835531 3.2094815 3.1034682 2.9932775 3.2453454 3.1182983
MET_Pt                                             train 75.57242 25.71167 31.941357 47.84475 78.40129 29.829681 24.827183 26.630056
MET_Pt                                             test  35.8507 57.02581 50.39648 31.44331 19.974499 69.95327 49.163204 73.0907
kinFit_H_mass_sigma_fit                            train 36.12665 14.739376 31.27036 8.939323 16.470669 20.250069 26.93385 68.29375
kinFit_H_mass_sigma_fit                            test  38.176517 18.799572 3.4745212 16.256433 44.666683 35.86714 13.215636 39.233337
kinFit_n_recoil_jets_fit                           train 1.0 0.0 2.0 1.0 0.0 1.0 0.0 0.0
kinFit_n_recoil_jets_fit                           test  0.0 2.0 2.0 1.0 0.0 1.0 1.0 0.0
VHbb::deltaR(Jet_eta[hJidx[0]],Jet_phi[hJidx[0...  train 0.9660941 1.1377306 0.7860779 2.3518863 1.1640192 1.3982579 1.3602552 0.92023295
VHbb::deltaR(Jet_eta[hJidx[0]],Jet_phi[hJidx[0...  test  0.97488123 1.0806819 2.700032 1.2381135 0.65717405 0.72016233 1.1425657 0.69351727
INFO: scaler params: <MyStandardScaler.StandardScaler object at 0x7f1db7afb610> [array([115.92315   , 120.64203   , 198.70395   , 193.87553   ,
         2.9816782 ,   2.7199163 ,   2.8435514 ,   2.0072556 ,
       108.44208   , 101.57127   , 112.59166   , 104.97436   ,
        90.52753   ,  90.64629   ,   0.832843  , 220.56786   ,
       221.16788   ,   0.9193468 ,   0.89654934,   0.73573506,
         2.3932834 ,   2.9593463 ,   2.9250383 ,  46.668667  ,
        19.828562  ,   1.2126372 ,   1.4412574 ], dtype=float32), array([25.5844    , 14.955225  , 89.684204  , 91.13842   ,  0.68877536,
        0.59882385,  0.3633728 ,  0.84364384, 67.433495  , 64.52513   ,
       77.93116   , 76.07403   ,  3.4704332 ,  4.0441537 ,  0.9773825 ,
       75.38576   , 75.9645    ,  0.32771835,  0.34717193,  0.5047285 ,
        2.0666082 ,  0.53820187,  0.541445  , 29.996367  , 16.137262  ,
        1.2522995 ,  0.59575343], dtype=float32)] [25.5844     14.955225   89.684204   91.13842     0.68877536  0.59882385
  0.3633728   0.84364384 67.433495   64.52513    77.93116    76.07403
  3.4704332   4.0441537   0.9773825  75.38576    75.9645      0.32771835
  0.34717193  0.5047285   2.0666082   0.53820187  0.541445   29.996367
 16.137262    1.2522995   0.59575343] [115.92315    120.64203    198.70395    193.87553      2.9816782
   2.7199163    2.8435514    2.0072556  108.44208    101.57127
 112.59166    104.97436     90.52753     90.64629      0.832843
 220.56786    221.16788      0.9193468    0.89654934   0.73573506
   2.3932834    2.9593463    2.9250383   46.668667    19.828562
   1.2126372    1.4412574 ]
[31mINFO: scaling is done inside tensorflow graph and StandardScaler() should not be used om top of it => scaler.dmp file will not be written![0m
number of expected events (test): {0: 2813.703672465783, 1: 53.834216620278376}
number of expected events (train): {0: 2838.88220514591, 1: 53.64811592031484}
balancing signal/background, reweight class SIG_ALL  from group 1 (signals) by 53.91671769727361
balancing signal/background, reweight class BKG_ALL  from group 0 (backgrounds) by 1.0188976195712065
shape train: (185927, 27)
shape test:  (186647, 27)
building tensorflow graph with parameters
 adam_epsilon                             1e-11
 adaptiveRate                             False
 additional_noise                         0.0
 bInitScale                               0.01
 balanceClasses                           False
 balanceSignalBackground                  True
 batchNormalization                       [1, 2, 3, 4, 5, 6, 7, 8]
 batchSize                                32
 batchSizeAtEpoch                         {0: 128, 80: 16384, 20: 512, 40: 1024, 120: 32768, 10: 256, 160: 65536, 60: 8192}
 batchSizeTest                            65536
 bin_opt_cumulative                       [0.8, 0.9, 1.0, 0.9, 0.8, 0.7, 0.55, 0.4, 0.25, 0.12, 0.06, 0.03, 0.02, 0.015, 0.01]
 crossValidation_splitSeed                123456
 dropoutDecay                             1.0
 ignoreLargeWeights                       False
 ignoreNegativeWeights                    False
 inputPlotRange                           {'Jet_btagDeepB[hJidx[1]]': [-1, 4], 'Jet_btagDeepB[hJidx[0]]': [-1, 4]}
 learning_rate_adam_start                 0.0005
 loss                                     'cross_entropy'
 massless_importance                      1.0
 massless_powers                          [1, 2]
 mvaScoreRescalingPercentileHigh          0.999
 mvaScoreRescalingPercentileLow           0.01
 nEpochs                                  200
 nNodes                                   [512, 256, 128, 64, 64, 64]
 nStepsPerEpoch                           -1
 pDropout                                 [0.2, 0.4, 0.5, 0.6, 0.7, 0.8]
 plot-scores                              True
 power                                    1.0
 preprocess                               'np.concatenate((x[:6],np.atleast_1d(np.piecewise(x[6],[x[6]<0.1522,(x[6]>=0.1522)&(x[6]<0.4941),(x[6]>=0.4941)&(x[6]<0.8001),x[6]>=0.8001],[0,1,2,3])),np.atleast_1d(np.piecewise(x[7],[x[7]<0.1522,(x[7]>=0.1522)&(x[7]<0.4941),(x[7]>=0.4941)&(x[7]<0.8001),x[7]>=0.8001],[0,1,2,3])),x[8:]))'
 rateGamma                                1.0
 removeFeature                            []
 reweight                                 None
 reweightTraining                         None
 saveCheckpointInterval                   50
 scaleInputsInsideGraph                   True
 selection                                '(x[:,6]>=1.5)'
 shuffle                                  True
 signif_loss_b_epsilon                    1e-08
 signif_loss_low_b_threshold              1.5
 signif_loss_low_b_threshold_width        1.5
 signif_loss_nbins                        15
 signif_loss_smoothness                   500.0
 signif_loss_sysApprox_constant           1.5
 signif_loss_sysApprox_linear             0.1
 signif_loss_sys_variance_offset          0.1
 signif_loss_xe_factor                    0.0
 skipConnections                          {8: [0, 2, 4, 6], 2: [0], 4: [0, 2], 6: [0, 2, 4]}
 statisticsInterval                       20
 systematics_scaling_factor               1.0
 systematics_weight_scaling_factor        1.0
 wInitScale                               0.01
 weight_sys_ntoys                         -1
initialize session...
initialized session!
add layers...
layer  1 :  [27, 512]
> activation with drop-out...
> batch normalization...
layer  2 :  [512, 256]
> activation with drop-out...
> batch normalization...
layer  3 :  [256, 128]
> activation with drop-out...
> batch normalization...
layer  4 :  [128, 64]
> activation with drop-out...
> batch normalization...
layer  5 :  [64, 64]
> activation with drop-out...
> batch normalization...
layer  6 :  [64, 64]
> activation with drop-out...
> batch normalization...
INFO: use ADAM optimizer!
graph built.
trainable variables: 242498
initialized.
conditions:
  loss(train,training)    training dataset, loss as seen by gradient descend optimizer
  loss(train,testing)     training dataset, loss with 'testing' conditions, e.g. is_training: False, no dropout etc
  loss(test)              test dataset
start training with batch size 32 
 epoch     loss(train,training) loss(train,testing) loss(test)
nSamples =  185927
set batch size to: 128
         1    0.01788    0.01496    0.01495 significance (train): 2.308 significance: 2.303 
         2    0.01564    0.01484    0.01483 
         3    0.01542    0.01462    0.01465 
         4    0.01527    0.01451    0.01453 
         5    0.01521    0.01446    0.01451 
         6    0.01517    0.01445    0.01450 
         7    0.01511    0.01439    0.01446 
         8    0.01509    0.01433    0.01442 
         9    0.01500    0.01432    0.01442 
        10    0.01495    0.01428    0.01440 
nSamples =  185927
set batch size to: 256
        11    0.01481    0.01417    0.01427 
        12    0.01477    0.01415    0.01430 
        13    0.01474    0.01413    0.01428 
        14    0.01474    0.01416    0.01432 
        15    0.01475    0.01413    0.01426 
        16    0.01472    0.01407    0.01425 
        17    0.01475    0.01404    0.01421 
        18    0.01470    0.01407    0.01423 
        19    0.01466    0.01400    0.01419 
        20    0.01464    0.01400    0.01417 
nSamples =  185927
set batch size to: 512
        21    0.01449    0.01395    0.01415 significance (train): 2.703 significance: 2.495 
        22    0.01451    0.01393    0.01415 
        23    0.01452    0.01389    0.01413 
        24    0.01450    0.01390    0.01414 
        25    0.01445    0.01388    0.01413 
        26    0.01449    0.01389    0.01415 
        27    0.01453    0.01386    0.01411 
        28    0.01449    0.01383    0.01408 
        29    0.01445    0.01385    0.01411 
        30    0.01452    0.01384    0.01411 
        31    0.01444    0.01386    0.01414 
        32    0.01448    0.01383    0.01413 
        33    0.01444    0.01382    0.01413 
        34    0.01441    0.01384    0.01416 
        35    0.01439    0.01380    0.01410 
        36    0.01436    0.01381    0.01414 
        37    0.01438    0.01381    0.01415 
        38    0.01436    0.01375    0.01408 
        39    0.01442    0.01381    0.01413 
        40    0.01438    0.01378    0.01410 
nSamples =  185927
set batch size to: 1024
        41    0.01435    0.01370    0.01406 significance (train): 2.710 significance: 2.500 
        42    0.01432    0.01371    0.01407 
        43    0.01428    0.01368    0.01406 
        44    0.01428    0.01368    0.01408 
        45    0.01423    0.01366    0.01407 
        46    0.01429    0.01368    0.01407 
        47    0.01432    0.01365    0.01406 
        48    0.01424    0.01362    0.01404 
        49    0.01428    0.01364    0.01404 
        50    0.01428    0.01365    0.01406 
        51    0.01425    0.01364    0.01407 
        52    0.01427    0.01361    0.01405 
        53    0.01426    0.01362    0.01406 
        54    0.01427    0.01360    0.01405 
        55    0.01422    0.01359    0.01403 
        56    0.01427    0.01358    0.01403 
        57    0.01423    0.01359    0.01405 
        58    0.01423    0.01359    0.01404 
        59    0.01421    0.01360    0.01406 
        60    0.01421    0.01357    0.01404 
nSamples =  185927
set batch size to: 8192
        61    0.01416    0.01354    0.01401 significance (train): 2.816 significance: 2.520 
        62    0.01414    0.01352    0.01400 
        63    0.01415    0.01351    0.01400 
        64    0.01412    0.01351    0.01400 
        65    0.01410    0.01350    0.01400 
        66    0.01409    0.01350    0.01400 
        67    0.01409    0.01349    0.01400 
        68    0.01409    0.01349    0.01401 
        69    0.01405    0.01348    0.01400 
        70    0.01407    0.01348    0.01401 
        71    0.01411    0.01348    0.01401 
        72    0.01411    0.01347    0.01401 
        73    0.01406    0.01347    0.01401 
        74    0.01407    0.01346    0.01401 
        75    0.01406    0.01346    0.01401 
        76    0.01409    0.01347    0.01401 
        77    0.01405    0.01346    0.01401 
        78    0.01404    0.01345    0.01401 
        79    0.01409    0.01345    0.01401 
        80    0.01404    0.01344    0.01400 
nSamples =  185927
set batch size to: 16384
        81    0.01409    0.01344    0.01400 significance (train): 2.810 significance: 2.517 
        82    0.01405    0.01344    0.01400 
        83    0.01406    0.01344    0.01400 
        84    0.01404    0.01343    0.01400 
        85    0.01409    0.01343    0.01401 
        86    0.01400    0.01343    0.01400 
        87    0.01407    0.01343    0.01400 
        88    0.01402    0.01343    0.01400 
        89    0.01405    0.01342    0.01400 
        90    0.01402    0.01343    0.01400 
        91    0.01405    0.01342    0.01400 
        92    0.01408    0.01342    0.01400 
        93    0.01403    0.01342    0.01401 
        94    0.01403    0.01342    0.01401 
        95    0.01408    0.01342    0.01400 
        96    0.01403    0.01342    0.01400 
        97    0.01403    0.01341    0.01400 
        98    0.01402    0.01341    0.01400 
        99    0.01404    0.01341    0.01400 
       100    0.01401    0.01341    0.01400 
       101    0.01405    0.01341    0.01400 significance (train): 2.824 significance: 2.511 
       102    0.01404    0.01341    0.01400 
       103    0.01406    0.01341    0.01400 
       104    0.01402    0.01340    0.01400 
       105    0.01401    0.01340    0.01400 
       106    0.01405    0.01340    0.01400 
       107    0.01407    0.01340    0.01400 
       108    0.01406    0.01340    0.01400 
       109    0.01404    0.01340    0.01401 
       110    0.01402    0.01340    0.01400 
       111    0.01404    0.01340    0.01400 
       112    0.01401    0.01340    0.01400 
       113    0.01403    0.01340    0.01401 
       114    0.01401    0.01339    0.01401 
       115    0.01401    0.01339    0.01400 
       116    0.01405    0.01339    0.01400 
       117    0.01396    0.01339    0.01400 
       118    0.01398    0.01339    0.01400 
       119    0.01399    0.01339    0.01400 
       120    0.01400    0.01339    0.01400 
nSamples =  185927
set batch size to: 32768
       121    0.01402    0.01339    0.01401 significance (train): 2.832 significance: 2.504 
       122    0.01410    0.01339    0.01400 
       123    0.01403    0.01339    0.01400 
       124    0.01407    0.01339    0.01400 
       125    0.01402    0.01338    0.01400 
       126    0.01402    0.01338    0.01400 
       127    0.01396    0.01338    0.01400 
       128    0.01400    0.01338    0.01400 
       129    0.01406    0.01338    0.01400 
       130    0.01398    0.01338    0.01400 
       131    0.01404    0.01338    0.01400 
       132    0.01403    0.01338    0.01400 
       133    0.01403    0.01338    0.01400 
       134    0.01401    0.01337    0.01400 
       135    0.01404    0.01337    0.01400 
       136    0.01403    0.01337    0.01400 
       137    0.01400    0.01338    0.01400 
       138    0.01401    0.01338    0.01400 
       139    0.01400    0.01338    0.01400 
       140    0.01400    0.01338    0.01400 
       141    0.01400    0.01337    0.01400 significance (train): 2.831 significance: 2.502 
       142    0.01406    0.01337    0.01400 
       143    0.01394    0.01337    0.01400 
       144    0.01397    0.01337    0.01400 
       145    0.01404    0.01337    0.01400 
       146    0.01402    0.01337    0.01400 
       147    0.01398    0.01337    0.01400 
       148    0.01403    0.01337    0.01400 
       149    0.01403    0.01337    0.01400 
       150    0.01403    0.01337    0.01401 
       151    0.01405    0.01337    0.01401 
       152    0.01399    0.01337    0.01401 
       153    0.01404    0.01336    0.01400 
       154    0.01407    0.01336    0.01400 
       155    0.01403    0.01336    0.01401 
       156    0.01402    0.01337    0.01401 
       157    0.01400    0.01336    0.01400 
       158    0.01396    0.01336    0.01400 
       159    0.01399    0.01336    0.01400 
       160    0.01399    0.01336    0.01400 
nSamples =  185927
set batch size to: 65536
       161    0.01398    0.01336    0.01400 significance (train): 2.836 significance: 2.508 
       162    0.01396    0.01336    0.01400 
       163    0.01406    0.01336    0.01401 
       164    0.01400    0.01336    0.01401 
       165    0.01398    0.01336    0.01401 
       166    0.01399    0.01336    0.01401 
       167    0.01398    0.01336    0.01401 
       168    0.01400    0.01336    0.01401 
       169    0.01397    0.01335    0.01401 
       170    0.01394    0.01335    0.01401 
       171    0.01402    0.01335    0.01400 
       172    0.01401    0.01335    0.01400 
       173    0.01401    0.01335    0.01400 
       174    0.01392    0.01335    0.01400 
       175    0.01402    0.01335    0.01400 
       176    0.01400    0.01335    0.01400 
       177    0.01401    0.01335    0.01400 
       178    0.01396    0.01335    0.01400 
       179    0.01390    0.01335    0.01400 
       180    0.01405    0.01335    0.01400 
       181    0.01403    0.01335    0.01400 significance (train): 2.841 significance: 2.501 
       182    0.01393    0.01335    0.01400 
       183    0.01409    0.01335    0.01400 
       184    0.01395    0.01335    0.01400 
       185    0.01401    0.01335    0.01400 
       186    0.01403    0.01335    0.01400 
       187    0.01406    0.01335    0.01400 
       188    0.01402    0.01335    0.01400 
       189    0.01401    0.01335    0.01400 
       190    0.01394    0.01335    0.01400 
       191    0.01394    0.01335    0.01400 
       192    0.01405    0.01335    0.01400 
       193    0.01401    0.01335    0.01400 
       194    0.01401    0.01335    0.01400 
       195    0.01393    0.01335    0.01400 
       196    0.01399    0.01335    0.01400 
       197    0.01395    0.01335    0.01400 
       198    0.01402    0.01335    0.01400 
       199    0.01392    0.01335    0.01400 
       200    0.01402    0.01335    0.01400 significance (train): 2.856 significance: 2.505 
FINAL RESULTS:        200   0.014024   0.014004 significance (train): 2.856 significance: 2.505 
TRAINING TIME: 0:04:59.409335 (299.4 seconds)
GRADIENT UPDATES: 33820
MIN TEST LOSS: 0.0139997708781
training done.
> results//2lepHigh_V11_v6_St_v2_DeepCSVbins_cutMaxAtMedium/Zll2017_ZllBDT_highpt_190627_V11-kinFitv6-Stitching.h5/512-256-128-64-64-64/0.20-0.40-0.50-0.60-0.70-0.80/5.000e-04/rnd_10/checkpoints/model.ckpt
saved checkpoint to [34m results//2lepHigh_V11_v6_St_v2_DeepCSVbins_cutMaxAtMedium/Zll2017_ZllBDT_highpt_190627_V11-kinFitv6-Stitching.h5/512-256-128-64-64-64/0.20-0.40-0.50-0.60-0.70-0.80/5.000e-04/rnd_10/checkpoints/model.ckpt [0m
LOSS(train, unmodified):  0.0133469632591
LOSS(test):               0.014004213967
---
S    B
---
 0.37 568.09
 0.77 359.31
 1.03 311.67
 1.33 266.50
 1.68 225.14
 2.05 207.16
 2.57 179.68
 2.90 169.78
 3.10 118.02
 2.80 83.32
 3.65 81.08
 4.94 81.71
 6.81 73.68
 9.80 62.47
10.03 26.10
---
significance: 2.505 
area under ROC: AUC_test =  86.8591008449
area under ROC: AUC_train =  88.2721439195
:: (185927,) (185927,) (185927,)
INFO: set range to: 1.1319071 593.8398
INFO: set range to: 90.00025 149.99971
INFO: set range to: 1.3194041 1909.8589
INFO: set range to: 0.12193481 1460.3706
INFO: set range to: 0.00010809631 6.2557387
INFO: set range to: 5.543232e-05 3.1415899
INFO: set range to: 0.008366257 1588.6918
INFO: set range to: 20.001316 1395.278
INFO: set range to: 0.00177088 1684.6715
INFO: set range to: 20.000278 1436.3312
INFO: set range to: 50.42034 109.51118
INFO: set range to: 75.00209 104.996346
INFO: set range to: 0.0 9.0
INFO: set range to: 106.82894 1753.4441
INFO: set range to: 150.00006 1591.287
INFO: set range to: 0.0043024076 8.992537
INFO: set range to: 0.0007145899 8.992537
INFO: set range to: 0.0 3.8076172
INFO: set range to: -1.0 19.0
INFO: set range to: 0.004794292 7.500883
INFO: set range to: 0.019376041 8.409098
INFO: set range to: 0.120362535 739.2409
INFO: set range to: -1.0 552.8983
INFO: set range to: -1.0 10.0
INFO: set range to: 0.39688307 4.6776905
BINS (from cumulative): [0.03542765 0.09786155 0.18657793 0.2836866  0.39303908 0.5069782
 0.638267   0.7737813  0.8642086  0.90758777 0.9374655  0.952262
 0.96415395 0.97598493]
-------------------------
with optimized binning:
 bins: 0.0000, 0.0354, 0.0979, 0.1866, 0.2837, 0.3930, 0.5070, 0.6383, 0.7738, 0.8642, 0.9076, 0.9375, 0.9523, 0.9642, 0.9760, 1.0000
-------------------------
---
S    B
---
 0.14 349.05
 0.56 393.30
 1.24 436.25
 1.97 391.78
 3.06 347.04
 4.53 301.69
 5.85 234.79
 7.73 167.26
 8.60 100.84
 5.92 46.68
 4.90 21.35
 2.70 10.47
 2.33  6.48
 2.16  4.40
 2.14  2.31
---
significance: 2.637 (for optimized binning)
significance: 2.632 ( 1% background uncertainty, for optimized binning)
significance: 2.532 ( 5% background uncertainty, for optimized binning)
significance: 2.356 (10% background uncertainty, for optimized binning)
significance: 2.191 (15% background uncertainty, for optimized binning)
significance: 2.044 (20% background uncertainty, for optimized binning)
[32mPLOTS: use n=S+B Asimov data in the plots![0m
set bin error:     0.0294210808269 10.2694584948 349.05102757
set bin error:     0.0271863191172 10.6925269375 393.305430259
set bin error:     0.0250362240943 10.9221037406 436.252036227
set bin error:     0.0247837314524 9.70983731126 391.782703502
set bin error:     0.0243785321369 8.46030369577 347.039093587
set bin error:     0.025259922828 7.62073882118 301.692878204
set bin error:     0.0293875069322 6.89970539674 234.783624642
set bin error:     0.0295134169908 4.93639048781 167.259199074
set bin error:     0.0376025032072 3.79185001603 100.840361482
set bin error:     0.0581786709599 2.7159085261 46.682202967
set bin error:     0.0744233187368 1.58924794663 21.3541665919
set bin error:     0.11504966185 1.20435591026 10.4681395051
set bin error:     0.162134854928 1.05087521914 6.48148863247
set bin error:     0.147840988024 0.65059434138 4.40063577819
set bin error:     0.184748157233 0.426894693283 2.31068444566
[32mPLOTS: use n=S+B Asimov data in the plots![0m
set bin error:     0.0294210808269 10.2694584948 349.05102757
set bin error:     0.0271863191172 10.6925269375 393.305430259
set bin error:     0.0250362240943 10.9221037406 436.252036227
set bin error:     0.0247837314524 9.70983731126 391.782703502
set bin error:     0.0243785321369 8.46030369577 347.039093587
set bin error:     0.025259922828 7.62073882118 301.692878204
set bin error:     0.0293875069322 6.89970539674 234.783624642
set bin error:     0.0295134169908 4.93639048781 167.259199074
set bin error:     0.0376025032072 3.79185001603 100.840361482
set bin error:     0.0581786709599 2.7159085261 46.682202967
set bin error:     0.0744233187368 1.58924794663 21.3541665919
set bin error:     0.11504966185 1.20435591026 10.4681395051
set bin error:     0.162134854928 1.05087521914 6.48148863247
set bin error:     0.147840988024 0.65059434138 4.40063577819
set bin error:     0.184748157233 0.426894693283 2.31068444566
[32mCOLOR:[0m <ROOT.TH1D object ("histo_e") at 0x559b849e7320> 824
[32mPLOTS: use n=S+B Asimov data in the plots![0m
set bin error:     0.0294210808269 10.2694584948 349.05102757
set bin error:     0.0271863191172 10.6925269375 393.305430259
set bin error:     0.0250362240943 10.9221037406 436.252036227
set bin error:     0.0247837314524 9.70983731126 391.782703502
set bin error:     0.0243785321369 8.46030369577 347.039093587
set bin error:     0.025259922828 7.62073882118 301.692878204
set bin error:     0.0293875069322 6.89970539674 234.783624642
set bin error:     0.0295134169908 4.93639048781 167.259199074
set bin error:     0.0376025032072 3.79185001603 100.840361482
set bin error:     0.0581786709599 2.7159085261 46.682202967
set bin error:     0.0744233187368 1.58924794663 21.3541665919
set bin error:     0.11504966185 1.20435591026 10.4681395051
set bin error:     0.162134854928 1.05087521914 6.48148863247
set bin error:     0.147840988024 0.65059434138 4.40063577819
set bin error:     0.184748157233 0.426894693283 2.31068444566
INFO: search optimal cut position for sensitivity
INFO: convert to histogram
