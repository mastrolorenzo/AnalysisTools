saving logfile to [34m results//2lepLow_V11_v6_St_v2_DeepCSVbins_cutMaxAtMedium/Zll2017_ZllBDT_lowpt_190627_V11-kinFitv6-Stitching.h5/512-256-128-64-64-64/0.20-0.40-0.50-0.60-0.70-0.80/5.000e-04/rnd_1/output.txt [0m
INFO: numpy random state =  MT19937 ,f32dd20f,fbc5743b,e4ad11ee,eb4663cc,73a581c6,2e2d1287,ad333d65,22bf15c2,4ed8ec1e,84664b6b,390fb077,bfde454c,6214c4ae,b4cecca1,5aa3480b,68a93018,92e720e5,3fe5b3be,4b5baf70,41c5e9f5,e1033164,7827793a,92e72b08,3c9d81ac,92047502,cad9d0ed,2a805f0,7d167689,c40c18d7,22dca987,ea3eed13,bd4a81e9,fc69d817,edce6b42,9fe66255,9a55ef7d,2723733f,2b3813f,f95b5a4d,c3d0674e,256e4df0,21a19c52,8f8ccd18,7b33224f,def83aa,6eb7ce82,6dcbc95f,374277a9,850996a0,a70a584c,cf551ce6,ea39f500,8a07f666,e9960b5b,fb14c4a8,981e7269,ac619800,51791065,84801253,ae44926a,54e4286,60f3fae,be2b68ef,e45ff50f,6171239f,460c5a1f,e013,c9e31862,ac08334e,894cfa27,ca814cc8,24d705e5,4d01de17,64e53d6c,a297b801,a7ffff48,6105ee0,3b693ece,54ade72,3d0ed538,44b0f99e,caaf9a60,d1af3c40,4fbd57b7,bab63b05,ab720587,32f2a10b,3ee15d51,aac66bb8,184f8914,653e1659,deb29e55,8b98c8e5,50d9daca,3d50f75b,16d83586,3aa202de,6e683bb8,55b8cb40,74dc965a,9c02503e,e7b9a9d1,20d519a0,dad1cb51,27c2a203,31a8bcfd,a5369444,af709518,b5d4448b,58398a5b,76931603,50063bd1,7d99cdb3,96c75420,3ab5d61f,f1f7e29e,bd3b3a40,bf199892,784580e8,b869f351,6184713a,1656e93e,818998cc,4486f790,acb197d1,51e492f4,f987d42a,2b03f149,b241ed80,d2c3976e,1a8bdc27,fdfbb03f,e94771e2,4905d365,b53b9c70,39cfa6d,382160a9,454294,57d37061,409bee5,731dfce1,1af0b157,d4bb4d08,21ba418f,810469c4,db4629eb,594e9b46,68296247,34494c7e,798d4b45,48c654f3,8ba46706,43409cee,83f86c0f,b22db236,858a6dba,c6344feb,a1a89c4b,28b46a31,92875f34,eea455df,2b86d5c1,a2d1155b,1dd4d3f0,3b85365a,3556da0e,c028841b,b4dd49b9,857e9ff5,42504ffa,f69164f9,9812e708,4ba6c025,10569a59,5a2f1136,f2503ce9,6d007fb0,62acec9b,73478852,bae67ea8,9860d30d,3f2d3fbd,7dc4edb8,96d73f0f,1d15c7a9,5cc8fb01,3d6a1e56,b34aee91,dd836a34,b0e23f78,4ebb82f3,a29f4e5f,ac07e85c,c887d28c,98dbb785,ce54fd76,b26c51f8,bedb138b,2b9e855a,1299f4d,1db94d1a,d56bb51f,7cbde6e1,c6948829,949dc3e9,44801c76,442e085,46e162d9,c7358ff0,fb110d29,afdc8be7,4db283fe,e152a580,d6b04b69,92005093,8d8af46c,8381e5eb,5bdbfb7a,7fce8b5d,c8f77218,e90f109b,bfb153e0,22bac043,ecb61730,23ab48d8,2d05683,31de0dc2,45dc46ff,146eebcf,6f8a298a,dd05d630,9a86a59b,2ffe7e18,b2258a6d,95460f60,960b1cc,3efa0cda,be04c98,d3358cec,bf98abad,6316130d,1a91dfec,1e2d7d59,a62ed67a,53e7a747,f8940be2,de37aff,cbe27d64,87a5fd05,e1e35560,21658960,535d8f7c,90e8507f,f7f5a159,a9db7ecf,4ed14905,25a5805e,4c07385c,2d32826f,7fb177fb,c9b1cce6,89970783,a306c7a4,fe0c77c0,f17aa217,b83bab3b,9aa91e00,50c8ccec,c537c0c2,e1b46a01,5cfca31b,97a35392,f264a5ff,b1b4b858,7d6ca961,86d3aacd,d5bc3955,4a67623d,5fc38948,a156f5bc,216f27cd,97c8b410,3990fc91,4184ede2,d874f437,3b4bc8b4,a2eb21b3,a1c9a5e4,ed065e7e,d94be205,66596fa4,b8952b43,fc02c196,7155abb2,54727261,de34fbe2,d4296146,a845ce23,ddf9436f,a1cbf4f5,f597cd74,9f78fd41,b9f7203c,ee6fec38,9eec565,7bf936c0,e4055b3e,7f3c9121,f95f04d2,c86680e3,5eb6773b,132eca11,24c491f9,da530ae6,e4f648eb,a91c0829,e624ef93,a852dca3,59fef043,89ce1caf,2e2198ec,26eb2f1b,dd9190a3,a7f23993,a84281b7,9d51a6a3,3a6f7ca9,a17a0186,70d3b0ec,5b429ff3,9c61c94,18c7e716,52b8a30a,efe49f64,2d076a22,4909cfbd,4be6d30d,686fb2d0,702c812c,d8e611f1,7657291f,b42c768c,bea3619,e2d34418,63abddba,2e638ce6,b0a1181c,46d82106,a2fbea1b,f2779183,65bea08b,59753cb8,f2c5a2ca,466d6bbe,bfc92462,e7b538c,14d84e5a,7d8fe00c,e9b89edd,ee3b9d7d,a522b095,b2f6fdd7,aa7f7e82,ae82fe35,5618ce67,2a1f3e94,449b8662,863b4cf7,2f4ecd3c,7fc243c0,afb7861d,e956843a,145c7909,c58fea03,63c44e0a,81f29c2e,8b46fa86,356d7e05,52f7fd91,6ceb0c9d,6706a1db,c6d7dda8,94611770,d348d982,e33ee475,c8feae8d,a56e4f2,115b627d,66e117a0,d7d47171,823cee26,a4c20a16,a249aeeb,dd2f3dfb,7af2798c,b73f19ec,2e5683e3,65b13b39,c4b28f47,9ca7baee,ac9e0091,6928c71b,b2d815eb,efd02a20,7a6839b7,65c3b8f0,c2da0ca,3cbd5b7f,a8a0764a,643b2f0c,4e2a0d9d,7fa0f67b,b2e37b28,cb51473a,8592c181,febc080f,d3579cdb,51cf2dc7,ad452178,e1a41fa8,4f6e872c,8a6eae1,92ca815,53855089,26375319,e662c2e8,44bf44b0,a0cfeedf,babb5f0f,e31f01aa,351f4283,3901a7b0,7ab4df68,6bc50ace,73b808ba,972594e2,d81c3f67,c6fa1e3,b77723ad,9b7e8cce,d39b6f82,3e470ec1,3e38cad2,ed6af4b9,90fdbd47,726c59b3,48978c44,a4e31ac3,7074bbc0,d2bf6ce5,528b5c96,94a11bf3,ba7d8fed,2f8b7639,41cd092a,967698e4,a1bcdae3,3277864e,78da34e1,6eb9f137,77eb661d,12481d60,f676345,624f42b5,7363cc4f,2461b033,be5d6125,57be2937,7342451e,27aa3091,a23628b1,2fa6c445,5f71f467,6ef0f43c,f8c4e4d0,5e72474e,5eb4364b,b61f990d,5e2f3a9c,693781fd,c5c94610,8d375d17,66e77a98,5ad753f7,d9beec12,24f63219,7b70077a,9d6e9378,e283cebd,6c9c9b31,7b61dd05,26eb4f57,cacc8fec,d0925846,8c3c0dab,6e4214b1,635c186b,37fac3b4,5ac41bff,43fa4724,2d75b26d,5cdb65ae,946ebea7,e49277bf,290cd68b,1e815baa,12e3a456,80d34a9e,54e665f4,a4da6b31,cb83024a,32e185ec,61d2df1d,1dce9df8,a589453d,a2bc8d52,3a4c97bc,a8a22459,cedf97e8,532de10c,c47978d9,2dfd1253,7a68a7e9,92639540,d5d9afec,9cba1fbb,91b17135,b4dea156,7157de08,c9951355,587723a9,600796ff,8c7bf22d,33f68303,ca8450b,4951f845,4839342a,39baddc3,2b4b3859,5c056048,ac4ea21b,5ca79ed5,36570c11,46fdeaee,589be9e3,325bef57,2fe596eb,ae673ecc,bdb4c191,64e0f922,cd123f57,56416c25,26d669f5,d2c2519b,5aca89e4,fce789bc,4d8ab821,81f2aaa0,99cbd9a7,4b0dacd2,4ace785,75fa30b9,17525d7c,877fbace,1d78e25f,2becab4f,8df01da3,348715c0,9c31b316,59efa266,611af1da,a16a007c,5cec3456,f6b6375b,afccb738,592c11b8,5571fa69,ad09a7bf,fa78daca,9789887c,62432b7a,b491a2b0,8d217126,e5f96bfb,729106b1,6fda47a4,82a9d66d,866a64fd,7f1253a,97499612,d1a37fab,e93db1f5,9b865e29,cc6c331b,e7d7716,97b92da1,4f6270f5,9bf51abd,24e1f68,7b62307d,87ce0a96,93fb7fe4,9b2de304,f124180d,36726fa7,67adef35,142daab,e6beab65,45526999,54b14845,5a7dc24c,96387873,fb486f27,2462e86e,4aa717b7,ab89d07f,4d322807,66381dcf,e1da68ed,d9c28a56,ffbca73a,e91d307a,3a8a1647,c3c39662
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------
initialized TensorflowDNNClassifier, version v0.1 ( 12006 )
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------
INFO: command: /var/lib/slurm-llnl/slurmd/job07225/slurm_script -c config/high_dropout.cfg -i /data/VHbb/2017/Zll/kinematicFit/Zll2017_ZllBDT_lowpt_190627_V11-kinFitv6-Stitching.h5 -p 2lepLow_V11_v6_St_v2_DeepCSVbins_cutMaxAtMedium --set='preprocess="np.concatenate((x[:6],np.atleast_1d(np.piecewise(x[6],[x[6]<0.1522,(x[6]>=0.1522)&(x[6]<0.4941),(x[6]>=0.4941)&(x[6]<0.8001),x[6]>=0.8001],[0,1,2,3])),np.atleast_1d(np.piecewise(x[7],[x[7]<0.1522,(x[7]>=0.1522)&(x[7]<0.4941),(x[7]>=0.4941)&(x[7]<0.8001),x[7]>=0.8001],[0,1,2,3])),x[8:]))";selection="(x[:,6]>=1.5)";plot-scores=True;inputPlotRange={"Jet_btagDeepB[hJidx[0]]":[-1,4], "Jet_btagDeepB[hJidx[1]]":[-1,4],}'
INFO: read inputs from disk, metadata is pesent:
INFO:  >   cut (V_mass > 75 && V_mass < 105 && (H_mass > 90 && H_mass < 150) && Jet_btagDeepB[hJidx[0]] > 0.1522 && Jet_btagDeepB[hJidx[1]] > 0.1522 && 1) && (isZee||isZmm)  && (V_pt>50&&V_pt<150)
INFO:  >   cutName ZllBDT_lowpt
INFO:  >   region ZllBDT_lowpt
INFO:  >   samples {u'SIG_ALL': [u'ZH_Zll', u'ZH_Znunu', u'ggZH_Zll', u'ggZH_Znunu', u'WplusH', u'WminusH'], u'BKG_ALL': [u'ST_tW_antitop', u'ST_tW_top', u'ST_s-channel_4f', u'ST_t-channel_top_4f', u'ST_t-channel_antitop_4f', u'TT_2l2n', u'TT_h', u'TT_Sl', u'M4HT100to200_0b', u'M4HT100to200_1b', u'M4HT100to200_2b', u'M4HT200to400_0b', u'M4HT200to400_1b', u'M4HT200to400_2b', u'M4HT400to600_0b', u'M4HT400to600_1b', u'M4HT400to600_2b', u'M4HT600toInf_0b', u'M4HT600toInf_1b', u'M4HT600toInf_2b', u'HT0to100ZJets_0b', u'HT0to100ZJets_1b', u'HT0to100ZJets_2b', u'HT100to200ZJets_0b', u'HT100to200ZJets_1b', u'HT100to200ZJets_2b', u'HT200to400ZJets_0b', u'HT200to400ZJets_1b', u'HT200to400ZJets_2b', u'HT400to600ZJets_0b', u'HT400to600ZJets_1b', u'HT400to600ZJets_2b', u'HT600to800ZJets_0b', u'HT600to800ZJets_1b', u'HT600to800ZJets_2b', u'HT800to1200ZJets_0b', u'HT800to1200ZJets_1b', u'HT800to1200ZJets_2b', u'HT1200to2500ZJets_0b', u'HT1200to2500ZJets_1b', u'HT1200to2500ZJets_2b', u'HT2500toinfZJets_0b', u'HT2500toinfZJets_1b', u'HT2500toinfZJets_2b', u'ZJetsB_Zpt100to200_0b', u'ZJetsB_Zpt100to200_1b', u'ZJetsB_Zpt100to200_2b', u'ZJetsB_Zpt200toInf_0b', u'ZJetsB_Zpt200toInf_1b', u'ZJetsB_Zpt200toInf_2b', u'ZJetsGenB_Zpt100to200_0b', u'ZJetsGenB_Zpt100to200_1b', u'ZJetsGenB_Zpt100to200_2b', u'ZJetsGenB_Zpt200toInf_0b', u'ZJetsGenB_Zpt200toInf_1b', u'ZJetsGenB_Zpt200toInf_2b', u'WWnlo_0b', u'WWnlo_1b', u'WZnlo_0b', u'WZnlo_1b', u'ZZnlo_0b', u'ZZnlo_1b', u'WWnlo_2b', u'WZnlo_2b', u'ZZnlo_2b']}
INFO:  >   systematics []
INFO:  >   testCut ((event%2)==0||isData)
INFO:  >   trainCut !((event%2)==0||isData)
INFO:  >   variables kinFit_H_mass_fit H_mass kinFit_H_pt_fit H_pt kinFit_HVdPhi_fit abs(VHbb::deltaPhi(H_phi,V_phi)) Jet_btagDeepB[hJidx[0]] Jet_btagDeepB[hJidx[1]] kinFit_hJets_pt_0_fit Jet_PtReg[hJidx[0]] kinFit_hJets_pt_1_fit Jet_PtReg[hJidx[1]] kinFit_V_mass_fit V_mass Sum$(Jet_Pt>30&&abs(Jet_eta)<2.4&&Jet_puId==7&&Jet_jetId>0&&Jet_lepFilter>0&&Iteration$!=hJidx[0]&&Iteration$!=hJidx[1]) kinFit_V_pt_fit V_pt kinFit_jjVPtRatio_fit (H_pt/V_pt) abs(Jet_eta[hJidx[0]]-Jet_eta[hJidx[1]]) SA5 VHbb::deltaR(kinFit_H_eta_fit,kinFit_H_phi_fit,kinFit_V_eta_fit,kinFit_V_phi_fit) VHbb::deltaR(H_eta,H_phi,V_eta,V_phi) MET_Pt kinFit_H_mass_sigma_fit kinFit_n_recoil_jets_fit VHbb::deltaR(Jet_eta[hJidx[0]],Jet_phi[hJidx[0]],Jet_eta[hJidx[1]],Jet_phi[hJidx[1]])
INFO:  >   version 3
INFO:  >   weightF genWeight*puWeight*1.0*muonSF_Iso[0]*muonSF_Id[0]*electronSF_IdIso[0]*electronSF_trigger[0]*bTagWeightDeepCSV*EWKw[0]*weightLOtoNLO*FitCorr[0]
INFO:  >   weightSYS []
INFO: random state: (3, (2147483648L, 2965411175L, 539265051L, 720742384L, 2776642422L, 2066137751L, 1673779003L, 3048322659L, 111057388L, 2293970575L, 552811103L, 4091857108L, 2763498934L, 786566888L, 3074843195L, 479624583L, 686873589L, 1454776651L, 1976578026L, 920651275L, 664787135L, 2432035063L, 4084940327L, 1669854086L, 1587961163L, 3021375070L, 3144343893L, 2578101182L, 1472324897L, 2563109707L, 1687907753L, 1131691295L, 2183189524L, 3250469709L, 2502424286L, 3371998207L, 2512523927L, 4266499911L, 3806313977L, 2149596449L, 813431717L, 3915251428L, 1559266215L, 860601820L, 3381623782L, 2936918500L, 2858605956L, 3248692574L, 905585685L, 1691328340L, 2393154606L, 3819423979L, 1795605178L, 3748047073L, 380782606L, 2460724262L, 3742755195L, 823687755L, 1564659602L, 1335469934L, 3391567269L, 3294027342L, 3609575007L, 3150898384L, 4021284256L, 1891682889L, 3708312279L, 331246217L, 4181047093L, 2308985294L, 3110716357L, 1823552019L, 3512134868L, 4015489614L, 1180930209L, 1536985027L, 1129195546L, 2877537792L, 3561597742L, 4181260017L, 3242054374L, 1448282220L, 234299626L, 1274069910L, 1786138897L, 2955333742L, 2684173708L, 3328171044L, 4043793805L, 3496441956L, 1830995581L, 3411575439L, 2293118201L, 4254502360L, 681937436L, 2384751849L, 223869362L, 2729876166L, 1369756647L, 786179446L, 260291144L, 3067888275L, 2388126037L, 2838823581L, 1615018777L, 3273717701L, 648840597L, 693681341L, 3345823297L, 2669857754L, 1836771943L, 1843639893L, 168199979L, 2765614852L, 1790854964L, 1859251597L, 609720564L, 2539743166L, 2807050029L, 1728430782L, 3760897507L, 3650345998L, 1323029514L, 935045257L, 2990505239L, 3679295902L, 3898947376L, 2099817632L, 3496313874L, 1370307987L, 1113620073L, 3276261149L, 244812199L, 3787558821L, 3169679986L, 277214817L, 4121473867L, 1937323388L, 1130044973L, 1439207124L, 2002399297L, 2449957502L, 3596188067L, 1714926726L, 936089215L, 2262840028L, 3994434282L, 3354555761L, 1903409800L, 998744594L, 2710792048L, 3120647256L, 253674829L, 1505237529L, 1214995977L, 1647168144L, 738042669L, 3567991082L, 2109435097L, 1870658032L, 2064314115L, 1371143384L, 2005059728L, 3804479300L, 2546451769L, 4236750094L, 1649294715L, 1321225904L, 4049382029L, 2721533725L, 2183621202L, 3362708120L, 2368145641L, 3963538801L, 963224645L, 4194729635L, 4118394413L, 3900211720L, 1155344194L, 298589540L, 1574932878L, 2239506358L, 2520771038L, 2862747092L, 3831973910L, 450992227L, 1291256426L, 3256770852L, 3876826744L, 4144667722L, 1677001197L, 3009907987L, 3342332083L, 3808022258L, 3658358321L, 3613859619L, 228376878L, 4159110279L, 2038324152L, 1197371153L, 1687955063L, 3511343454L, 3805526052L, 2511564109L, 87986729L, 3049179599L, 1350302053L, 2335844037L, 1163731980L, 1567872659L, 1465026246L, 448271971L, 3120296053L, 513458198L, 1668751597L, 3902631382L, 1651037492L, 1065973253L, 1409066373L, 2039127968L, 2113510381L, 4144668412L, 1798232859L, 2600848940L, 4079206471L, 179214849L, 4259800723L, 425740331L, 4048355642L, 1621046054L, 1592700916L, 321307247L, 1300801761L, 3594077033L, 1944627793L, 3385191303L, 2601873316L, 2319973503L, 763313989L, 3350153174L, 3239213898L, 354486859L, 1688905970L, 4216663838L, 1129012149L, 804452982L, 2352513174L, 2568470131L, 1406574441L, 3999180565L, 2992742589L, 2742562213L, 948824862L, 1298904059L, 2905250176L, 2939335113L, 1393764199L, 1421822446L, 2786897728L, 2760602428L, 4177904991L, 136093393L, 58663192L, 1433762868L, 2599495815L, 3898963871L, 2256258927L, 2856021543L, 3082483131L, 2488017541L, 399469882L, 1490466280L, 3030353816L, 969781595L, 1403726103L, 765655661L, 265332426L, 3592488983L, 1520247823L, 3157744464L, 3452683988L, 733845939L, 2879089572L, 1021075518L, 1966762760L, 1853844072L, 2378176363L, 1564261016L, 229557077L, 3872790867L, 667417402L, 188495251L, 2220740991L, 764619900L, 3576406562L, 3722926000L, 3593421467L, 788491241L, 3051150541L, 4078064692L, 1920455273L, 2878681381L, 2804702130L, 1180045440L, 3210090757L, 2802674535L, 2819375647L, 3972375341L, 3524244264L, 893955525L, 1100231104L, 555265801L, 1667668800L, 1327790623L, 2276603276L, 3367351748L, 3416329295L, 75078304L, 2038092355L, 277196989L, 2078191684L, 2852768647L, 623437717L, 753869507L, 950230825L, 2041143682L, 140756669L, 839581023L, 3597844034L, 124756454L, 2688875901L, 171374243L, 1880003021L, 1368074971L, 2563616979L, 3391870098L, 1847885546L, 191903477L, 759224634L, 1074031882L, 471764242L, 2295828287L, 3145156310L, 3005398904L, 1398519339L, 3764992241L, 2896155996L, 3160159731L, 2398069116L, 3621513446L, 2578064776L, 3288875811L, 793085220L, 1881394653L, 3470824913L, 3447992738L, 2298342137L, 2011642021L, 3808020415L, 2778055793L, 922144232L, 352967040L, 1292120260L, 1054403848L, 2925927777L, 1164549656L, 913437667L, 3140707491L, 948843163L, 1070089897L, 1664372041L, 1951168366L, 2422152006L, 344502231L, 669521940L, 1386508234L, 3281670102L, 3766189528L, 2511803500L, 4026843385L, 2577567217L, 3077706077L, 930203112L, 317240568L, 3725169398L, 3763804913L, 2155461633L, 1204011954L, 502146785L, 2812732860L, 1450104521L, 962003024L, 57456126L, 1730536760L, 3285939457L, 1947514767L, 1226597537L, 3101565118L, 2181620144L, 2975126026L, 1524171508L, 4277046228L, 1306147357L, 829651124L, 227393015L, 3495524632L, 1127420160L, 1297950147L, 3695653752L, 2223902192L, 2270198393L, 1524159351L, 643113397L, 1285454988L, 1583581154L, 471095611L, 3768419613L, 579004392L, 2823379731L, 2106226579L, 1424004018L, 1625558765L, 336596735L, 1710940939L, 4087362286L, 2107660368L, 4104798334L, 2369042464L, 3423494443L, 3571408712L, 809885135L, 1134859609L, 3675100212L, 1425572673L, 465918587L, 1700636879L, 3979331717L, 3856045968L, 2423238466L, 2489339164L, 1477429471L, 3525830788L, 1341118986L, 253915044L, 1672571981L, 1640334210L, 88525500L, 686348832L, 1740390952L, 2407860956L, 2957156291L, 3114378641L, 1082880027L, 2809762586L, 2563659319L, 1066921366L, 2597657024L, 4210417083L, 3924175797L, 617222593L, 3801070142L, 3399907266L, 2138685530L, 1393721549L, 1654746205L, 3623373915L, 4071682374L, 2569292851L, 171629685L, 1663949202L, 1123453851L, 4239516081L, 3375226951L, 17790276L, 743687995L, 452658596L, 3572786136L, 2390423178L, 3217089644L, 3568063407L, 998805081L, 674557714L, 1924146372L, 1242249287L, 1094080290L, 111006194L, 932394656L, 3344764980L, 3061964321L, 2478789700L, 3349197072L, 3828260571L, 2034552941L, 2865785906L, 3972092905L, 1339778645L, 1636498923L, 1394331833L, 4128109675L, 176669770L, 233020819L, 2552286910L, 509808182L, 2799337636L, 816222184L, 3467989782L, 936036254L, 1179189338L, 2433451586L, 1985955254L, 2079455434L, 3881768384L, 1651564955L, 2706183964L, 2497168797L, 139809410L, 187413360L, 3317479450L, 3759007244L, 2447982559L, 1745117208L, 642345635L, 864613727L, 139949310L, 978599361L, 3837111148L, 1844421139L, 1918789209L, 4279123916L, 1088246356L, 134890510L, 3250100238L, 2877693391L, 2505078564L, 4083172958L, 193847769L, 2326327765L, 3852807518L, 2783275342L, 579616888L, 601427476L, 2333006115L, 3227740185L, 1868214880L, 3653952216L, 3899391848L, 399956485L, 1714432415L, 2124156567L, 4106529928L, 1371437244L, 760052296L, 2606904059L, 900666268L, 2619576640L, 4254300521L, 3798639638L, 3908247719L, 1117622166L, 3401726801L, 367676601L, 646537191L, 1715566678L, 1658514468L, 2075266724L, 1857521305L, 2818717083L, 3929152671L, 2120515068L, 3826475568L, 2377027577L, 3410587322L, 681554910L, 635600821L, 952512256L, 84507355L, 3784831877L, 1048121665L, 2269747515L, 312605189L, 240239852L, 420212898L, 698058283L, 4195866491L, 1724844506L, 362395391L, 3449145310L, 569828316L, 771586969L, 888727315L, 383175830L, 423936908L, 545280618L, 709603642L, 2739178662L, 2236789809L, 3379929359L, 2978498845L, 2263203087L, 1588665574L, 1852176838L, 3711971710L, 783832395L, 3265836668L, 3020574439L, 1654790937L, 1901335663L, 140267822L, 454848417L, 1672128996L, 1180829481L, 2176510615L, 3975821008L, 1612732854L, 3624042844L, 489498470L, 953005195L, 3499102901L, 5590822L, 1110976858L, 3985643790L, 1579118364L, 2748971470L, 149451029L, 624L), None)
INFO: preprocess test  X, mapfn= np.concatenate((x[:6],np.atleast_1d(np.piecewise(x[6],[x[6]<0.1522,(x[6]>=0.1522)&(x[6]<0.4941),(x[6]>=0.4941)&(x[6]<0.8001),x[6]>=0.8001],[0,1,2,3])),np.atleast_1d(np.piecewise(x[7],[x[7]<0.1522,(x[7]>=0.1522)&(x[7]<0.4941),(x[7]>=0.4941)&(x[7]<0.8001),x[7]>=0.8001],[0,1,2,3])),x[8:]))
INFO: preprocess train  X, mapfn= np.concatenate((x[:6],np.atleast_1d(np.piecewise(x[6],[x[6]<0.1522,(x[6]>=0.1522)&(x[6]<0.4941),(x[6]>=0.4941)&(x[6]<0.8001),x[6]>=0.8001],[0,1,2,3])),np.atleast_1d(np.piecewise(x[7],[x[7]<0.1522,(x[7]>=0.1522)&(x[7]<0.4941),(x[7]>=0.4941)&(x[7]<0.8001),x[7]>=0.8001],[0,1,2,3])),x[8:]))
INFO: selection for test is '(x[:,6]>=1.5)': 432392  -->  383816 88.77 %
INFO: selection for train is '(x[:,6]>=1.5)': 432716  -->  384565 88.87 %
nFeatures =  27
--------------------------------------------------------------------------------
statistics for dataset: train
--------------------------------------------------------------------------------
SIG_ALL (y= 0 ) : 239812  avg weight: 0.00047359612331748834
BKG_ALL (y= 1 ) : 144753  avg weight: 0.18443535841508155
--------------------------------------------------------------------------------
statistics for dataset: test
--------------------------------------------------------------------------------
SIG_ALL (y= 0 ) : 238708  avg weight: 0.0004749831334354951
BKG_ALL (y= 1 ) : 145108  avg weight: 0.18221802644458868
--------------------------------------------------------------------------------
classes and labels
--------------------------------------------------------------------------------
list of classes: (signals in [32mgreen[0m, backgrounds in [31mred[0m)
[32m class 0 => SIG_ALL [0m is defined as a SIGNAL
[31m class 1 => BKG_ALL [0m
--------------------------------------------------------------------------------
weights and weight uncertainty examples
--------------------------------------------------------------------------------
weights:
train 0.000747736 0.00032260185 0.0006418578 0.0007424637 0.0012412944 0.0007568858 0.00049551233 0.0007870299 0.00090563064 0.00072062307
test  0.00064307475 0.0009911363 0.00055793877 0.0011546941 0.00081644015 0.0008283897 0.0005009505 0.00090630085 0.00056825724 0.00087846874
weights errors:
train 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
test  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
--------------------------------------------------------------------------------
input data
--------------------------------------------------------------------------------
feature                                            set   mean       std        examples
kinFit_H_mass_fit                                  train 1.08e+02   3.59e+01   85.69983 51.699482 105.71608 115.8991
kinFit_H_mass_fit                                  test  1.08e+02   3.56e+01   108.85095 129.76059 155.61629 107.33926
H_mass                                             train 1.17e+02   1.71e+01   111.18175 120.77479 98.024574 134.22299
H_mass                                             test  1.17e+02   1.71e+01   113.423256 136.90887 142.8039 111.76302
kinFit_H_pt_fit                                    train 8.24e+01   4.37e+01   145.14249 121.983536 84.85552 55.52349
kinFit_H_pt_fit                                    test  8.26e+01   4.38e+01   182.48859 89.04133 51.225674 82.74858
H_pt                                               train 8.44e+01   4.61e+01   153.06718 111.85144 53.695583 52.117123
H_pt                                               test  8.48e+01   4.59e+01   183.67542 97.55568 39.0415 85.15862
kinFit_HVdPhi_fit                                  train 2.91e+00   8.24e-01   3.1245425 3.0476284 3.154875 3.1884184
kinFit_HVdPhi_fit                                  test  2.90e+00   8.26e-01   1.046132 3.155889 3.2923064 3.1363297
abs(VHbb::deltaPhi(H_phi,V_phi))                   train 2.47e+00   7.26e-01   3.0621917 2.8090408 2.9727745 2.7114027
abs(VHbb::deltaPhi(H_phi,V_phi))                   test  2.47e+00   7.28e-01   0.96312475 3.0305626 2.870494 3.1308196
Jet_btagDeepB[hJidx[0]]                            train 2.69e+00   4.61e-01   3.0 3.0 3.0 3.0
Jet_btagDeepB[hJidx[0]]                            test  2.69e+00   4.62e-01   3.0 3.0 3.0 3.0
Jet_btagDeepB[hJidx[1]]                            train 1.64e+00   8.09e-01   3.0 3.0 1.0 3.0
Jet_btagDeepB[hJidx[1]]                            test  1.65e+00   8.05e-01   2.0 3.0 3.0 1.0
kinFit_hJets_pt_0_fit                              train 6.20e+01   3.61e+01   123.167595 5.55216 99.686844 88.690796
kinFit_hJets_pt_0_fit                              test  6.15e+01   3.60e+01   101.38209 98.69504 78.27328 34.880573
Jet_PtReg[hJidx[0]]                                train 6.15e+01   3.29e+01   90.95082 28.070026 70.313576 89.77448
Jet_PtReg[hJidx[0]]                                test  6.11e+01   3.25e+01   111.16625 63.063595 67.885605 36.22404
kinFit_hJets_pt_1_fit                              train 5.35e+01   3.62e+01   42.796158 125.63362 33.917164 34.51842
kinFit_hJets_pt_1_fit                              test  5.40e+01   3.61e+01   108.85538 58.66899 84.63819 100.41748
Jet_PtReg[hJidx[1]]                                train 5.27e+01   3.23e+01   50.71247 126.88313 37.473957 46.337837
Jet_PtReg[hJidx[1]]                                test  5.30e+01   3.24e+01   100.00976 55.08877 77.68222 102.83989
kinFit_V_mass_fit                                  train 9.02e+01   5.23e+00   90.95352 93.35977 90.75856 87.706314
kinFit_V_mass_fit                                  test  9.03e+01   5.24e+00   91.24626 89.643585 92.13723 93.00384
V_mass                                             train 9.03e+01   5.80e+00   91.16141 98.06045 90.68137 87.39481
V_mass                                             test  9.03e+01   5.81e+00   91.01591 89.79376 92.875305 93.1431
Sum$(Jet_Pt>30&&abs(Jet_eta)<2.4&&Jet_puId==7&...  train 5.20e-01   7.89e-01   1.0 0.0 0.0 0.0
Sum$(Jet_Pt>30&&abs(Jet_eta)<2.4&&Jet_puId==7&...  test  5.25e-01   7.90e-01   2.0 1.0 1.0 0.0
kinFit_V_pt_fit                                    train 8.14e+01   2.45e+01   142.92827 121.215355 95.21904 58.722565
kinFit_V_pt_fit                                    test  8.14e+01   2.46e+01   87.07064 87.54759 93.55103 110.66135
V_pt                                               train 8.15e+01   2.46e+01   143.56386 123.50405 95.13224 58.562378
V_pt                                               test  8.15e+01   2.46e+01   86.81569 88.0807 93.97755 110.71023
kinFit_jjVPtRatio_fit                              train 1.03e+00   5.22e-01   1.0154918 1.0063373 0.89116126 0.94552225
kinFit_jjVPtRatio_fit                              test  1.04e+00   5.25e-01   2.095868 1.017062 0.5475693 0.74776405
(H_pt/V_pt)                                        train 1.07e+00   5.80e-01   1.0661958 0.90564996 0.56443095 0.88994205
(H_pt/V_pt)                                        test  1.08e+00   5.80e-01   2.1156938 1.1075715 0.41543433 0.7692028
abs(Jet_eta[hJidx[0]]-Jet_eta[hJidx[1]])           train 1.32e+00   7.97e-01   0.49316406 0.6877136 0.3920288 0.64889526
abs(Jet_eta[hJidx[0]]-Jet_eta[hJidx[1]])           test  1.32e+00   8.00e-01   0.29248047 0.12762451 0.21252441 0.13452148
SA5                                                train 2.12e+00   1.90e+00   1.0 0.0 2.0 1.0
SA5                                                test  2.12e+00   1.90e+00   3.0 2.0 1.0 3.0
VHbb::deltaR(kinFit_H_eta_fit,kinFit_H_phi_fit...  train 3.01e+00   7.39e-01   3.145016 3.0658474 3.3687918 3.0999742
VHbb::deltaR(kinFit_H_eta_fit,kinFit_H_phi_fit...  test  3.00e+00   7.34e-01   1.0491105 3.2102804 3.5392694 3.7599533
VHbb::deltaR(H_eta,H_phi,V_eta,V_phi)              train 2.89e+00   7.83e-01   3.0855296 2.8339102 3.2868762 2.7129695
VHbb::deltaR(H_eta,H_phi,V_eta,V_phi)              test  2.88e+00   7.77e-01   0.96542543 3.1274571 3.511516 3.7551372
MET_Pt                                             train 4.97e+01   3.48e+01   14.063056 66.08536 52.39827 5.944703
MET_Pt                                             test  4.96e+01   3.52e+01   16.24543 24.827948 32.839954 17.223104
kinFit_H_mass_sigma_fit                            train 7.31e+00   8.57e+00   14.750453 21.255232 6.1841164 5.462744
kinFit_H_mass_sigma_fit                            test  7.31e+00   9.72e+00   16.061693 4.928521 1.6441895 6.1504803
kinFit_n_recoil_jets_fit                           train 8.08e-01   1.14e+00   0.0 0.0 0.0 1.0
kinFit_n_recoil_jets_fit                           test  8.01e-01   1.13e+00   3.0 0.0 1.0 3.0
VHbb::deltaR(Jet_eta[hJidx[0]],Jet_phi[hJidx[0...  train 2.38e+00   6.48e-01   1.058436 2.335949 2.3216677 2.7683895
VHbb::deltaR(Jet_eta[hJidx[0]],Jet_phi[hJidx[0...  test  2.37e+00   6.46e-01   1.0742731 2.08533 2.6237338 2.2464628
--------------------------------------------------------------------------------
input scaling
--------------------------------------------------------------------------------
[31mINFO: scaling is done inside tensorflow graph and StandardScaler() should not be used om top of it => scaler.dmp file will not be written![0m
number of expected events (test): {0: 26441.293381321375, 1: 113.38227381612016}
number of expected events (train): {0: 26697.571436658298, 1: 113.57403352501352}
balancing signal/background, reweight class SIG_ALL  from group 1 (signals) by 236.0675643722597
balancing signal/background, reweight class BKG_ALL  from group 0 (backgrounds) by 1.0042540960624255
shape train: (384565, 27)
shape test:  (383816, 27)
building tensorflow graph with parameters
 adam_epsilon                             1e-11
 adaptiveRate                             False
 additional_noise                         0.0
 bInitScale                               0.01
 balanceClasses                           False
 balanceSignalBackground                  True
 batchNormalization                       [1, 2, 3, 4, 5, 6, 7, 8]
 batchSize                                32
 batchSizeAtEpoch                         {0: 128, 80: 16384, 20: 512, 40: 1024, 120: 32768, 10: 256, 160: 65536, 60: 8192}
 batchSizeTest                            65536
 bin_opt_cumulative                       [0.8, 0.9, 1.0, 0.9, 0.8, 0.7, 0.55, 0.4, 0.25, 0.12, 0.06, 0.03, 0.02, 0.015, 0.01]
 crossValidation_splitSeed                123456
 dropoutDecay                             1.0
 ignoreLargeWeights                       False
 ignoreNegativeWeights                    False
 inputPlotRange                           {'Jet_btagDeepB[hJidx[1]]': [-1, 4], 'Jet_btagDeepB[hJidx[0]]': [-1, 4]}
 learningRate                             0.0005
 learning_rate_adam_start                 0.0005
 loss                                     'cross_entropy'
 massless_importance                      1.0
 massless_powers                          [1, 2]
 mvaScoreRescalingPercentileHigh          0.999
 mvaScoreRescalingPercentileLow           0.01
 nEpochs                                  200
 nNodes                                   [512, 256, 128, 64, 64, 64]
 nStepsPerEpoch                           -1
 pDropout                                 [0.2, 0.4, 0.5, 0.6, 0.7, 0.8]
 plot-scores                              True
 power                                    1.0
 preprocess                               'np.concatenate((x[:6],np.atleast_1d(np.piecewise(x[6],[x[6]<0.1522,(x[6]>=0.1522)&(x[6]<0.4941),(x[6]>=0.4941)&(x[6]<0.8001),x[6]>=0.8001],[0,1,2,3])),np.atleast_1d(np.piecewise(x[7],[x[7]<0.1522,(x[7]>=0.1522)&(x[7]<0.4941),(x[7]>=0.4941)&(x[7]<0.8001),x[7]>=0.8001],[0,1,2,3])),x[8:]))'
 rateGamma                                1.0
 removeFeature                            []
 reweight                                 None
 reweightTraining                         None
 saveCheckpointInterval                   50
 scaleInputsInsideGraph                   True
 selection                                '(x[:,6]>=1.5)'
 shuffle                                  True
 signif_loss_b_epsilon                    1e-08
 signif_loss_low_b_threshold              1.5
 signif_loss_low_b_threshold_width        1.5
 signif_loss_nbins                        15
 signif_loss_smoothness                   500.0
 signif_loss_sysApprox_constant           1.5
 signif_loss_sysApprox_linear             0.1
 signif_loss_sys_variance_offset          0.1
 signif_loss_xe_factor                    0.0
 skipConnections                          {8: [0, 2, 4, 6], 2: [0], 4: [0, 2], 6: [0, 2, 4]}
 statisticsInterval                       20
 systematics_scaling_factor               1.0
 systematics_weight_scaling_factor        1.0
 wInitScale                               0.01
 weight_sys_ntoys                         -1
initialize session...
initialized session!
add layers...
layer  1 :  [27, 512]
> activation with drop-out...
> batch normalization...
layer  2 :  [512, 256]
> activation with drop-out...
> batch normalization...
layer  3 :  [256, 128]
> activation with drop-out...
> batch normalization...
layer  4 :  [128, 64]
> activation with drop-out...
> batch normalization...
layer  5 :  [64, 64]
> activation with drop-out...
> batch normalization...
layer  6 :  [64, 64]
> activation with drop-out...
> batch normalization...
INFO: use ADAM optimizer!
graph built.
trainable variables: 242498
initialized.
conditions:
  loss(train,training)    training dataset, loss as seen by gradient descend optimizer
  loss(train,testing)     training dataset, loss with 'testing' conditions, e.g. is_training: False, no dropout etc
  loss(test)              test dataset
start training with batch size 32 
 epoch     loss(train,training) loss(train,testing) loss(test)
nSamples =  384565
set batch size to: 128
         1    0.07500    0.06769    0.06770 significance (train): 1.582 significance: 1.588 
         2    0.07023    0.06671    0.06676 
         3    0.06941    0.06652    0.06662 
         4    0.06909    0.06614    0.06620 
         5    0.06875    0.06531    0.06545 
         6    0.06838    0.06531    0.06544 
         7    0.06830    0.06581    0.06596 
         8    0.06818    0.06496    0.06512 
         9    0.06815    0.06468    0.06492 
        10    0.06784    0.06494    0.06521 
nSamples =  384565
set batch size to: 256
        11    0.06699    0.06433    0.06472 
        12    0.06707    0.06433    0.06462 
        13    0.06688    0.06416    0.06459 
        14    0.06680    0.06412    0.06453 
        15    0.06665    0.06413    0.06463 
        16    0.06668    0.06408    0.06458 
        17    0.06678    0.06384    0.06441 
        18    0.06661    0.06402    0.06455 
        19    0.06650    0.06413    0.06470 
        20    0.06651    0.06371    0.06429 
nSamples =  384565
set batch size to: 512
        21    0.06597    0.06342    0.06411 significance (train): 1.745 significance: 1.700 
        22    0.06592    0.06351    0.06414 
        23    0.06602    0.06354    0.06430 
        24    0.06595    0.06328    0.06405 
        25    0.06597    0.06320    0.06398 
        26    0.06588    0.06321    0.06401 
        27    0.06579    0.06331    0.06413 
        28    0.06584    0.06355    0.06441 
        29    0.06586    0.06314    0.06396 
        30    0.06586    0.06314    0.06398 
        31    0.06587    0.06320    0.06408 
        32    0.06570    0.06321    0.06409 
        33    0.06582    0.06305    0.06399 
        34    0.06569    0.06305    0.06392 
        35    0.06561    0.06309    0.06407 
        36    0.06559    0.06300    0.06393 
        37    0.06581    0.06294    0.06396 
        38    0.06571    0.06280    0.06384 
        39    0.06543    0.06283    0.06386 
        40    0.06565    0.06296    0.06399 
nSamples =  384565
set batch size to: 1024
        41    0.06528    0.06262    0.06372 significance (train): 1.782 significance: 1.730 
        42    0.06516    0.06264    0.06382 
        43    0.06520    0.06256    0.06374 
        44    0.06514    0.06259    0.06381 
        45    0.06518    0.06249    0.06374 
        46    0.06505    0.06255    0.06379 
        47    0.06515    0.06256    0.06383 
        48    0.06501    0.06242    0.06371 
        49    0.06505    0.06254    0.06379 
        50    0.06507    0.06252    0.06378 
        51    0.06502    0.06256    0.06384 
        52    0.06507    0.06241    0.06372 
        53    0.06509    0.06252    0.06378 
        54    0.06497    0.06239    0.06372 
        55    0.06507    0.06259    0.06386 
        56    0.06497    0.06238    0.06380 
        57    0.06513    0.06247    0.06378 
        58    0.06501    0.06242    0.06385 
        59    0.06494    0.06235    0.06373 
        60    0.06500    0.06255    0.06393 
nSamples =  384565
set batch size to: 8192
        61    0.06474    0.06217    0.06361 significance (train): 1.792 significance: 1.732 
        62    0.06461    0.06211    0.06359 
        63    0.06445    0.06207    0.06358 
        64    0.06435    0.06208    0.06360 
        65    0.06456    0.06204    0.06359 
        66    0.06450    0.06200    0.06357 
        67    0.06452    0.06200    0.06357 
        68    0.06434    0.06199    0.06357 
        69    0.06437    0.06199    0.06358 
        70    0.06437    0.06197    0.06357 
        71    0.06452    0.06196    0.06357 
        72    0.06450    0.06195    0.06357 
        73    0.06438    0.06193    0.06361 
        74    0.06441    0.06194    0.06359 
        75    0.06455    0.06196    0.06362 
        76    0.06431    0.06189    0.06356 
        77    0.06449    0.06190    0.06356 
        78    0.06440    0.06191    0.06362 
        79    0.06438    0.06188    0.06357 
        80    0.06452    0.06190    0.06358 
nSamples =  384565
set batch size to: 16384
        81    0.06420    0.06187    0.06357 significance (train): 1.798 significance: 1.721 
        82    0.06423    0.06185    0.06354 
        83    0.06435    0.06184    0.06355 
        84    0.06421    0.06184    0.06357 
        85    0.06427    0.06183    0.06355 
        86    0.06430    0.06183    0.06357 
        87    0.06431    0.06182    0.06356 
        88    0.06440    0.06183    0.06356 
        89    0.06432    0.06182    0.06357 
        90    0.06428    0.06180    0.06355 
        91    0.06435    0.06180    0.06356 
        92    0.06421    0.06180    0.06357 
        93    0.06432    0.06179    0.06356 
        94    0.06435    0.06180    0.06357 
        95    0.06435    0.06178    0.06356 
        96    0.06443    0.06178    0.06356 
        97    0.06435    0.06179    0.06355 
        98    0.06422    0.06177    0.06356 
        99    0.06415    0.06177    0.06357 
       100    0.06415    0.06176    0.06356 
       101    0.06423    0.06175    0.06356 significance (train): 1.808 significance: 1.726 
       102    0.06427    0.06175    0.06357 
       103    0.06430    0.06179    0.06362 
       104    0.06411    0.06174    0.06356 
       105    0.06421    0.06175    0.06355 
       106    0.06435    0.06173    0.06354 
       107    0.06418    0.06173    0.06357 
       108    0.06426    0.06173    0.06355 
       109    0.06408    0.06171    0.06357 
       110    0.06420    0.06172    0.06355 
       111    0.06429    0.06171    0.06355 
       112    0.06432    0.06171    0.06356 
       113    0.06419    0.06170    0.06354 
       114    0.06425    0.06170    0.06355 
       115    0.06434    0.06170    0.06357 
       116    0.06423    0.06173    0.06357 
       117    0.06423    0.06170    0.06356 
       118    0.06418    0.06168    0.06356 
       119    0.06410    0.06170    0.06354 
       120    0.06404    0.06166    0.06354 
nSamples =  384565
set batch size to: 32768
       121    0.06403    0.06167    0.06353 significance (train): 1.821 significance: 1.735 
       122    0.06421    0.06167    0.06354 
       123    0.06412    0.06166    0.06354 
       124    0.06425    0.06166    0.06354 
       125    0.06410    0.06166    0.06354 
       126    0.06406    0.06165    0.06354 
       127    0.06402    0.06164    0.06356 
       128    0.06401    0.06163    0.06354 
       129    0.06399    0.06163    0.06355 
       130    0.06422    0.06164    0.06355 
       131    0.06410    0.06164    0.06355 
       132    0.06416    0.06162    0.06355 
       133    0.06428    0.06163    0.06354 
       134    0.06429    0.06163    0.06354 
       135    0.06425    0.06162    0.06355 
       136    0.06420    0.06162    0.06354 
       137    0.06418    0.06162    0.06353 
       138    0.06411    0.06162    0.06354 
       139    0.06428    0.06161    0.06355 
       140    0.06425    0.06161    0.06354 
       141    0.06403    0.06161    0.06356 significance (train): 1.814 significance: 1.731 
       142    0.06397    0.06160    0.06356 
       143    0.06412    0.06160    0.06356 
       144    0.06410    0.06160    0.06357 
       145    0.06423    0.06159    0.06355 
       146    0.06403    0.06159    0.06355 
       147    0.06414    0.06160    0.06356 
       148    0.06405    0.06159    0.06356 
       149    0.06419    0.06159    0.06355 
       150    0.06396    0.06159    0.06357 
       151    0.06409    0.06158    0.06355 
       152    0.06420    0.06158    0.06354 
       153    0.06436    0.06159    0.06356 
       154    0.06408    0.06158    0.06355 
       155    0.06393    0.06156    0.06354 
       156    0.06407    0.06156    0.06355 
       157    0.06387    0.06157    0.06355 
       158    0.06407    0.06156    0.06355 
       159    0.06395    0.06155    0.06355 
       160    0.06399    0.06155    0.06354 
nSamples =  384565
set batch size to: 65536
       161    0.06398    0.06155    0.06356 significance (train): 1.812 significance: 1.720 
       162    0.06400    0.06155    0.06356 
       163    0.06433    0.06155    0.06356 
       164    0.06419    0.06155    0.06355 
       165    0.06410    0.06154    0.06354 
       166    0.06396    0.06154    0.06354 
       167    0.06420    0.06154    0.06354 
       168    0.06407    0.06154    0.06354 
       169    0.06401    0.06154    0.06355 
       170    0.06432    0.06155    0.06354 
       171    0.06401    0.06155    0.06355 
       172    0.06386    0.06154    0.06355 
       173    0.06419    0.06154    0.06354 
       174    0.06422    0.06153    0.06354 
       175    0.06419    0.06153    0.06355 
       176    0.06428    0.06154    0.06354 
       177    0.06398    0.06154    0.06354 
       178    0.06409    0.06154    0.06354 
       179    0.06406    0.06154    0.06354 
       180    0.06424    0.06154    0.06354 
       181    0.06419    0.06154    0.06355 significance (train): 1.822 significance: 1.733 
       182    0.06391    0.06154    0.06356 
       183    0.06426    0.06153    0.06356 
       184    0.06396    0.06153    0.06355 
       185    0.06401    0.06152    0.06354 
       186    0.06414    0.06152    0.06355 
       187    0.06407    0.06152    0.06356 
       188    0.06410    0.06153    0.06355 
       189    0.06423    0.06152    0.06354 
       190    0.06398    0.06152    0.06354 
       191    0.06397    0.06151    0.06355 
       192    0.06417    0.06151    0.06354 
       193    0.06392    0.06150    0.06354 
       194    0.06413    0.06151    0.06354 
       195    0.06402    0.06151    0.06355 
       196    0.06411    0.06151    0.06355 
       197    0.06395    0.06151    0.06354 
       198    0.06399    0.06150    0.06354 
       199    0.06386    0.06150    0.06355 
       200    0.06415    0.06150    0.06355 significance (train): 1.829 significance: 1.723 
FINAL RESULTS:        200   0.064155   0.063549 significance (train): 1.829 significance: 1.723 
TRAINING TIME: 0:10:50.203540 (650.2 seconds)
GRADIENT UPDATES: 70060
MIN TEST LOSS: 0.0635342929134
training done.
> results//2lepLow_V11_v6_St_v2_DeepCSVbins_cutMaxAtMedium/Zll2017_ZllBDT_lowpt_190627_V11-kinFitv6-Stitching.h5/512-256-128-64-64-64/0.20-0.40-0.50-0.60-0.70-0.80/5.000e-04/rnd_1/checkpoints/model.ckpt
saved checkpoint to [34m results//2lepLow_V11_v6_St_v2_DeepCSVbins_cutMaxAtMedium/Zll2017_ZllBDT_lowpt_190627_V11-kinFitv6-Stitching.h5/512-256-128-64-64-64/0.20-0.40-0.50-0.60-0.70-0.80/5.000e-04/rnd_1/checkpoints/model.ckpt [0m
LOSS(train, unmodified):  0.0615020368211
LOSS(test):               0.0635491387771
---
S    B
---
 0.75 6134.86
 1.47 3226.03
 2.05 2464.83
 2.59 2205.43
 3.13 1846.47
 3.89 1700.68
 4.88 1538.52
 6.67 1646.43
 7.41 1331.57
 7.27 1031.85
 9.50 955.26
12.49 920.05
16.49 785.77
20.20 507.55
14.59 145.99
---
significance: 1.723 
area under ROC: AUC_test =  86.3820173551
area under ROC: AUC_train =  87.5472270807
INFO: set range to: 0.32884827 596.39105
INFO: set range to: 90.00032 149.99956
INFO: set range to: 0.13183 1646.4913
INFO: set range to: 0.14538823 1638.5796
INFO: set range to: 0.00015607555 6.2391796
INFO: set range to: 5.0920993e-05 3.141592
INFO: set range to: 0.0035906518 1390.9807
INFO: set range to: 20.000076 1192.3236
INFO: set range to: 0.00032809365 1617.5891
INFO: set range to: 20.000298 1609.3499
INFO: set range to: 69.6143 112.76267
INFO: set range to: 75.00003 104.99973
INFO: set range to: 0.0 9.0
INFO: set range to: 32.60456 242.52531
INFO: set range to: 50.000095 149.9997
INFO: set range to: 0.0021407274 19.953035
INFO: set range to: 0.0012998589 18.629707
INFO: set range to: 0.0 3.8371582
INFO: set range to: -1.0 16.0
INFO: set range to: 0.007676397 8.0666895
INFO: set range to: 0.0071152686 7.9874964
INFO: set range to: 0.10550891 1035.141
INFO: set range to: -1.0 2928.0317
INFO: set range to: -1.0 11.0
INFO: set range to: 0.39838737 4.7884393
-------------------------
with optimized binning:
 method: SB
 target: 0.1220, 0.1373, 0.1526, 0.1373, 0.1220, 0.1068, 0.0839, 0.0610, 0.0381, 0.0183, 0.0092, 0.0046, 0.0031, 0.0023, 0.0015
 bins:   0.0000, 0.0232, 0.0803, 0.1751, 0.2866, 0.4099, 0.5285, 0.6479, 0.7602, 0.8354, 0.8830, 0.9136, 0.9318, 0.9487, 0.9694, 1.0000
-------------------------
---
S    B
---
 0.15 3240.27
 0.85 3645.50
 2.44 4046.00
 4.33 3646.34
 6.78 3234.38
10.28 2826.25
13.06 2214.23
16.42 1610.51
15.94 997.88
13.10 476.58
 9.37 233.73
 5.63 115.96
 5.11 76.02
 5.71 55.10
 4.25 22.55
---
significance: 1.792 (for optimized binning)
significance: 1.761 ( 1% background uncertainty, for optimized binning)
significance: 1.489 ( 5% background uncertainty, for optimized binning)
significance: 1.222 (10% background uncertainty, for optimized binning)
significance: 1.021 (15% background uncertainty, for optimized binning)
significance: 0.867 (20% background uncertainty, for optimized binning)
[32mPLOTS: use n=S+B Asimov data in the plots![0m
[32mPLOTS: use n=S+B Asimov data in the plots![0m
[32mPLOTS: use n=S+B Asimov data in the plots![0m
INFO: search optimal cut position for sensitivity
INFO: convert to histogram
